{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import *\n",
    "import tensorflow as tf\n",
    "from solo.getdata import *\n",
    "import tushare as ts\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、获取数据\n",
    "## 参数说明：\n",
    "- 'name'：名称\n",
    "- 'start'：开始时间\n",
    "- 'data_type'：数据处理方式，'normal'：原始数据, 'carg'：复合增长率, 'grow'：增长速度\n",
    "- 'ktype' ：时间周期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "起始时间：\t 2008-10-06\n",
      "结束时间：\t 2017-03-07\n",
      "数据个数：\t 1966\n",
      "\n",
      "      open  close   high    low    volume\n",
      "180  6.356  6.288  6.405  6.092  25489.71\n",
      "181  6.112  6.356  6.434  6.063  13945.27\n",
      "182  6.366  6.307  6.366  6.209  20080.54\n",
      "183  6.405  6.258  6.434  6.209  15268.85\n",
      "184  5.965  5.711  6.131  5.711  36454.38\n",
      "\n",
      "查看最新数据：\n",
      "\n",
      "                open      high     close       low    volume\n",
      "2008-10-07 -0.038389  0.004528  0.010814 -0.004760 -0.452906\n",
      "2008-10-08  0.041558 -0.010569 -0.007709  0.024080  0.439953\n",
      "2008-10-09  0.006126  0.010682 -0.007769  0.000000 -0.239620\n",
      "2008-10-10 -0.068696 -0.047094 -0.087408 -0.080206  1.387500\n",
      "2008-10-13 -0.104946 -0.030338  0.030818 -0.065137 -0.313828\n"
     ]
    }
   ],
   "source": [
    "new_data = data_recieve(name='000837',start = '2008-10-01',ktype='D',data_type = 'carg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# 二、建立特征\n",
    "## 参数说明：\n",
    "- 'data'：数据集\n",
    "- 'time'：多个时间周期\n",
    "- 'normal'：是否对数据进行归一化\n",
    "- 'pattern' ：是否加入k模式识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ml_datas = data_indicator(data=new_data,time=[5,10,20,30,60],normal=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataf = ts.get_k_data('399300',start = '2004-10-01',ktype='D')\n",
    "#new_data.to_csv('new.csv')\n",
    "#ml_datas.to_csv('ml_data.csv')\n",
    "#dataf.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 三、设定X和Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_labels = 10 #设定要分成多少个类\n",
    "\n",
    "ml_datas['Price'] = pd.qcut(ml_datas['target'],num_labels, precision=1) #将target进行10等分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X形状：\n",
      "(1610, 784)\n",
      "y形状：\n",
      "(1610, 10)\n"
     ]
    }
   ],
   "source": [
    "#y设定为符合增长率求整\n",
    "X_ori = ml_datas.drop(['target','Price'],axis=1).values\n",
    "y_ori = ml_datas['Price'].values\n",
    "\n",
    "y_ori = pd.get_dummies(y_ori)\n",
    "y_columns = y_ori.columns.values\n",
    "y_ori = y_ori.values\n",
    "\n",
    "\n",
    "print('X形状：')\n",
    "print(X_ori.shape)\n",
    "print('y形状：')\n",
    "print(y_ori.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Price分割：\n",
      "[-10, -3.5]\n",
      "(-3.5, -2.1]\n",
      "(-2.1, -1.1]\n",
      "(-1.1, -0.4]\n",
      "(-0.4, 0.1]\n",
      "(0.1, 0.6]\n",
      "(0.6, 1.2]\n",
      "(1.2, 2.1]\n",
      "(2.1, 3.4]\n",
      "(3.4, 10.1]\n"
     ]
    }
   ],
   "source": [
    "print('Price分割：')\n",
    "for i in y_columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 四、转换格式\n",
    "## 将原2维数组(N,D)转换成（N,D,H,W）的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1601, 28, 28, 10), (1601, 10))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_channels = 10 #多少天的数据（即多少个通道）\n",
    "\n",
    "X_1,y = data_trans(X_ori,y_ori,days=num_channels,depth=image_size)\n",
    "\n",
    "X = np.transpose(X_1,(0,3,2,1))\n",
    "\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、按8:1:1的比例分割Train、Test和Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_dataset_1, test_dataset, train_labels_1, test_labels = cross_validation.train_test_split(X, y, test_size=0.1, random_state=77)\n",
    "train_dataset, valid_dataset, train_labels, valid_labels = cross_validation.train_test_split(train_dataset_1, train_labels_1, test_size=0.1, random_state=77)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (1296, 28, 28, 10), (1296, 10))\n",
      "('Validation set', (144, 28, 28, 10), (144, 10))\n",
      "('Test set', (161, 28, 28, 10), (161, 10))\n"
     ]
    }
   ],
   "source": [
    "#转成float32，不转似乎不行\n",
    "\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "\n",
    "    labels = labels.astype(np.float32)\n",
    "    #labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "\n",
    "\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 六、Tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.488656\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 13.9%\n",
      "Minibatch loss at step 50: 3.413373\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 100: 4.074187\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 150: 3.217750\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 200: 3.041208\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 250: 2.956689\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 13.2%\n",
      "Minibatch loss at step 300: 2.783059\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 350: 2.784541\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 400: 2.347641\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 11.8%\n",
      "Minibatch loss at step 450: 2.537210\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.1%\n",
      "Minibatch loss at step 500: 2.747462\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 550: 2.533831\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 600: 2.569548\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 650: 2.547287\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 8.3%\n",
      "Minibatch loss at step 700: 2.515725\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 750: 2.422405\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 800: 2.360768\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 10.4%\n",
      "Minibatch loss at step 850: 2.502016\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 900: 2.495406\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 9.7%\n",
      "Minibatch loss at step 950: 2.475306\n",
      "Minibatch accuracy: 0.0%\n",
      "Validation accuracy: 9.0%\n",
      "Minibatch loss at step 1000: 2.490128\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 9.0%\n",
      "Test accuracy: 11.2%\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([patch_size*patch_size*depth*depth//25, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)    \n",
    "        hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME') \n",
    "        shape = hidden.get_shape().as_list()    \n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) \n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    l2_loss1 = tf.nn.l2_loss(layer1_weights)\n",
    "    l2_loss2 = tf.nn.l2_loss(layer2_weights)\n",
    "    l2_loss3 = tf.nn.l2_loss(layer3_weights)\n",
    "    l2_loss4 = tf.nn.l2_loss(layer4_weights)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + 0.00001 * (l2_loss1+l2_loss2+l2_loss3+l2_loss4)\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(5e-5).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "  \n",
    "    cc = np.array(valid_prediction.eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## 2、CrossValidation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test accuracy: 12.4%,\\t%', 1e-06, 1e-06)\n",
      "('Test accuracy: 12.4%,\\t%', 1e-06, 5e-06)\n",
      "('Test accuracy: 11.8%,\\t%', 1e-06, 1e-05)\n",
      "('Test accuracy: 13.7%,\\t%', 1e-06, 5e-05)\n",
      "('Test accuracy: 12.4%,\\t%', 1e-06, 0.0001)\n",
      "('Test accuracy: 9.3%,\\t%', 1e-06, 0.0005)\n",
      "('Test accuracy: 13.0%,\\t%', 1e-06, 0.001)\n",
      "('Test accuracy: 11.8%,\\t%', 1e-06, 0.005)\n",
      "('Test accuracy: 12.4%,\\t%', 1e-06, 0.01)\n",
      "('Test accuracy: 13.7%,\\t%', 1e-06, 0.05)\n",
      "('Test accuracy: 10.6%,\\t%', 1e-06, 0.1)\n",
      "('Test accuracy: 16.1%,\\t%', 1e-06, 0.5)\n",
      "('Test accuracy: 12.4%,\\t%', 5e-06, 1e-06)\n",
      "('Test accuracy: 9.9%,\\t%', 5e-06, 5e-06)\n",
      "('Test accuracy: 13.7%,\\t%', 5e-06, 1e-05)\n",
      "('Test accuracy: 9.9%,\\t%', 5e-06, 5e-05)\n",
      "('Test accuracy: 8.7%,\\t%', 5e-06, 0.0001)\n",
      "('Test accuracy: 7.5%,\\t%', 5e-06, 0.0005)\n",
      "('Test accuracy: 9.3%,\\t%', 5e-06, 0.001)\n",
      "('Test accuracy: 10.6%,\\t%', 5e-06, 0.005)\n",
      "('Test accuracy: 13.0%,\\t%', 5e-06, 0.01)\n",
      "('Test accuracy: 13.0%,\\t%', 5e-06, 0.05)\n",
      "('Test accuracy: 9.9%,\\t%', 5e-06, 0.1)\n",
      "('Test accuracy: 9.9%,\\t%', 5e-06, 0.5)\n",
      "('Test accuracy: 11.8%,\\t%', 1e-05, 1e-06)\n",
      "('Test accuracy: 12.4%,\\t%', 1e-05, 5e-06)\n",
      "('Test accuracy: 8.1%,\\t%', 1e-05, 1e-05)\n",
      "('Test accuracy: 10.6%,\\t%', 1e-05, 5e-05)\n",
      "('Test accuracy: 10.6%,\\t%', 1e-05, 0.0001)\n",
      "('Test accuracy: 10.6%,\\t%', 1e-05, 0.0005)\n",
      "('Test accuracy: 15.5%,\\t%', 1e-05, 0.001)\n",
      "('Test accuracy: 13.7%,\\t%', 1e-05, 0.005)\n",
      "('Test accuracy: 13.7%,\\t%', 1e-05, 0.01)\n",
      "('Test accuracy: 12.4%,\\t%', 1e-05, 0.05)\n",
      "('Test accuracy: 11.2%,\\t%', 1e-05, 0.1)\n",
      "('Test accuracy: 11.8%,\\t%', 1e-05, 0.5)\n",
      "('Test accuracy: 9.9%,\\t%', 5e-05, 1e-06)\n",
      "('Test accuracy: 10.6%,\\t%', 5e-05, 5e-06)\n",
      "('Test accuracy: 11.2%,\\t%', 5e-05, 1e-05)\n",
      "('Test accuracy: 12.4%,\\t%', 5e-05, 5e-05)\n",
      "('Test accuracy: 11.2%,\\t%', 5e-05, 0.0001)\n",
      "('Test accuracy: 13.7%,\\t%', 5e-05, 0.0005)\n",
      "('Test accuracy: 13.0%,\\t%', 5e-05, 0.001)\n",
      "('Test accuracy: 10.6%,\\t%', 5e-05, 0.005)\n",
      "('Test accuracy: 16.8%,\\t%', 5e-05, 0.01)\n",
      "('Test accuracy: 8.7%,\\t%', 5e-05, 0.05)\n",
      "('Test accuracy: 11.8%,\\t%', 5e-05, 0.1)\n",
      "('Test accuracy: 10.6%,\\t%', 5e-05, 0.5)\n",
      "('Test accuracy: 13.7%,\\t%', 0.0001, 1e-06)\n",
      "('Test accuracy: 14.9%,\\t%', 0.0001, 5e-06)\n",
      "('Test accuracy: 10.6%,\\t%', 0.0001, 1e-05)\n",
      "('Test accuracy: 13.7%,\\t%', 0.0001, 5e-05)\n",
      "('Test accuracy: 11.2%,\\t%', 0.0001, 0.0001)\n",
      "('Test accuracy: 7.5%,\\t%', 0.0001, 0.0005)\n",
      "('Test accuracy: 8.1%,\\t%', 0.0001, 0.001)\n",
      "('Test accuracy: 8.7%,\\t%', 0.0001, 0.005)\n",
      "('Test accuracy: 7.5%,\\t%', 0.0001, 0.01)\n",
      "('Test accuracy: 11.8%,\\t%', 0.0001, 0.05)\n",
      "('Test accuracy: 11.2%,\\t%', 0.0001, 0.1)\n",
      "('Test accuracy: 14.9%,\\t%', 0.0001, 0.5)\n",
      "('Test accuracy: 10.6%,\\t%', 0.0005, 1e-06)\n",
      "('Test accuracy: 16.8%,\\t%', 0.0005, 5e-06)\n",
      "('Test accuracy: 14.9%,\\t%', 0.0005, 1e-05)\n",
      "('Test accuracy: 16.1%,\\t%', 0.0005, 5e-05)\n",
      "('Test accuracy: 14.3%,\\t%', 0.0005, 0.0001)\n",
      "('Test accuracy: 9.9%,\\t%', 0.0005, 0.0005)\n",
      "('Test accuracy: 10.6%,\\t%', 0.0005, 0.001)\n",
      "('Test accuracy: 8.7%,\\t%', 0.0005, 0.005)\n",
      "('Test accuracy: 10.6%,\\t%', 0.0005, 0.01)\n",
      "('Test accuracy: 10.6%,\\t%', 0.0005, 0.05)\n",
      "('Test accuracy: 12.4%,\\t%', 0.0005, 0.1)\n",
      "('Test accuracy: 13.0%,\\t%', 0.0005, 0.5)\n",
      "('Test accuracy: 11.8%,\\t%', 0.001, 1e-06)\n",
      "('Test accuracy: 6.8%,\\t%', 0.001, 5e-06)\n",
      "('Test accuracy: 13.0%,\\t%', 0.001, 1e-05)\n",
      "('Test accuracy: 9.9%,\\t%', 0.001, 5e-05)\n",
      "('Test accuracy: 13.0%,\\t%', 0.001, 0.0001)\n",
      "('Test accuracy: 12.4%,\\t%', 0.001, 0.0005)\n",
      "('Test accuracy: 9.3%,\\t%', 0.001, 0.001)\n",
      "('Test accuracy: 11.8%,\\t%', 0.001, 0.005)\n",
      "('Test accuracy: 11.8%,\\t%', 0.001, 0.01)\n",
      "('Test accuracy: 11.2%,\\t%', 0.001, 0.05)\n",
      "('Test accuracy: 10.6%,\\t%', 0.001, 0.1)\n",
      "('Test accuracy: 14.3%,\\t%', 0.001, 0.5)\n",
      "('Test accuracy: 13.7%,\\t%', 0.005, 1e-06)\n",
      "('Test accuracy: 6.2%,\\t%', 0.005, 5e-06)\n",
      "('Test accuracy: 8.1%,\\t%', 0.005, 1e-05)\n",
      "('Test accuracy: 10.6%,\\t%', 0.005, 5e-05)\n",
      "('Test accuracy: 11.2%,\\t%', 0.005, 0.0001)\n",
      "('Test accuracy: 11.2%,\\t%', 0.005, 0.0005)\n",
      "('Test accuracy: 12.4%,\\t%', 0.005, 0.001)\n",
      "('Test accuracy: 12.4%,\\t%', 0.005, 0.005)\n",
      "('Test accuracy: 8.7%,\\t%', 0.005, 0.01)\n",
      "('Test accuracy: 8.1%,\\t%', 0.005, 0.05)\n",
      "('Test accuracy: 10.6%,\\t%', 0.005, 0.1)\n",
      "('Test accuracy: 11.2%,\\t%', 0.005, 0.5)\n",
      "('Test accuracy: 7.5%,\\t%', 0.01, 1e-06)\n",
      "('Test accuracy: 10.6%,\\t%', 0.01, 5e-06)\n",
      "('Test accuracy: 17.4%,\\t%', 0.01, 1e-05)\n",
      "('Test accuracy: 14.3%,\\t%', 0.01, 5e-05)\n",
      "('Test accuracy: 8.7%,\\t%', 0.01, 0.0001)\n",
      "('Test accuracy: 15.5%,\\t%', 0.01, 0.0005)\n",
      "('Test accuracy: 9.3%,\\t%', 0.01, 0.001)\n",
      "('Test accuracy: 8.1%,\\t%', 0.01, 0.005)\n",
      "('Test accuracy: 11.2%,\\t%', 0.01, 0.01)\n",
      "('Test accuracy: 11.2%,\\t%', 0.01, 0.05)\n",
      "('Test accuracy: 9.3%,\\t%', 0.01, 0.1)\n",
      "('Test accuracy: 8.1%,\\t%', 0.01, 0.5)\n",
      "('Test accuracy: 10.6%,\\t%', 0.05, 1e-06)\n",
      "('Test accuracy: 9.9%,\\t%', 0.05, 5e-06)\n",
      "('Test accuracy: 12.4%,\\t%', 0.05, 1e-05)\n",
      "('Test accuracy: 9.9%,\\t%', 0.05, 5e-05)\n",
      "('Test accuracy: 13.7%,\\t%', 0.05, 0.0001)\n",
      "('Test accuracy: 11.8%,\\t%', 0.05, 0.0005)\n",
      "('Test accuracy: 9.9%,\\t%', 0.05, 0.001)\n",
      "('Test accuracy: 14.9%,\\t%', 0.05, 0.005)\n",
      "('Test accuracy: 9.3%,\\t%', 0.05, 0.01)\n",
      "('Test accuracy: 14.3%,\\t%', 0.05, 0.05)\n",
      "('Test accuracy: 8.7%,\\t%', 0.05, 0.1)\n",
      "('Test accuracy: 7.5%,\\t%', 0.05, 0.5)\n",
      "('Test accuracy: 10.6%,\\t%', 0.1, 1e-06)\n",
      "('Test accuracy: 9.9%,\\t%', 0.1, 5e-06)\n",
      "('Test accuracy: 9.9%,\\t%', 0.1, 1e-05)\n",
      "('Test accuracy: 13.0%,\\t%', 0.1, 5e-05)\n",
      "('Test accuracy: 11.8%,\\t%', 0.1, 0.0001)\n",
      "('Test accuracy: 7.5%,\\t%', 0.1, 0.0005)\n",
      "('Test accuracy: 7.5%,\\t%', 0.1, 0.001)\n",
      "('Test accuracy: 13.0%,\\t%', 0.1, 0.005)\n",
      "('Test accuracy: 11.2%,\\t%', 0.1, 0.01)\n",
      "('Test accuracy: 10.6%,\\t%', 0.1, 0.05)\n",
      "('Test accuracy: 11.2%,\\t%', 0.1, 0.1)\n",
      "('Test accuracy: 10.6%,\\t%', 0.1, 0.5)\n",
      "('Test accuracy: 8.1%,\\t%', 0.5, 1e-06)\n",
      "('Test accuracy: 13.0%,\\t%', 0.5, 5e-06)\n",
      "('Test accuracy: 10.6%,\\t%', 0.5, 1e-05)\n",
      "('Test accuracy: 10.6%,\\t%', 0.5, 5e-05)\n",
      "('Test accuracy: 11.2%,\\t%', 0.5, 0.0001)\n",
      "('Test accuracy: 10.6%,\\t%', 0.5, 0.0005)\n",
      "('Test accuracy: 9.9%,\\t%', 0.5, 0.001)\n",
      "('Test accuracy: 11.2%,\\t%', 0.5, 0.005)\n",
      "('Test accuracy: 12.4%,\\t%', 0.5, 0.01)\n",
      "('Test accuracy: 6.8%,\\t%', 0.5, 0.05)\n",
      "('Test accuracy: 11.8%,\\t%', 0.5, 0.1)\n",
      "('Test accuracy: 7.5%,\\t%', 0.5, 0.5)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    \n",
    "    layer2_weights = tf.Variable(tf.truncated_normal([patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    \n",
    "    layer3_weights = tf.Variable(tf.truncated_normal([patch_size*patch_size*depth*depth//25, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal([num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)    \n",
    "        hidden = tf.nn.max_pool(hidden, [1,2,2,1], [1,2,2,1], padding='SAME') \n",
    "        shape = hidden.get_shape().as_list()    \n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases) \n",
    "\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    \n",
    "    l2_loss1 = tf.nn.l2_loss(layer1_weights)\n",
    "    l2_loss2 = tf.nn.l2_loss(layer2_weights)\n",
    "    l2_loss3 = tf.nn.l2_loss(layer3_weights)\n",
    "    l2_loss4 = tf.nn.l2_loss(layer4_weights)\n",
    "\n",
    "\n",
    "    aa = [1e-6,5e-6,1e-5,5e-5,1e-4,5e-4,1e-3,5e-3,1e-2,5e-2,1e-1,5e-1]\n",
    "    bb = [1e-6,5e-6,1e-5,5e-5,1e-4,5e-4,1e-3,5e-3,1e-2,5e-2,1e-1,5e-1]\n",
    "    for a in aa:\n",
    "        for b in bb:\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits)) + 0.00001 * (l2_loss1+l2_loss2+l2_loss3+l2_loss4)\n",
    "    \n",
    "            # Optimizer.\n",
    "            optimizer = tf.train.GradientDescentOptimizer(5e-5).minimize(loss)\n",
    "  \n",
    "            # Predictions for the training, validation, and test data.\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "            valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "            test_prediction = tf.nn.softmax(model(tf_test_dataset))\n",
    "            \n",
    "            num_steps = 20001\n",
    "            with tf.Session(graph=graph) as session:\n",
    "                tf.global_variables_initializer().run()\n",
    "            \n",
    "                for step in range(num_steps):\n",
    "                    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "                    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "                    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "                    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "                    _, l, predictions = session.run([optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "\n",
    "            \n",
    "                print('Test accuracy: %.1f%%,\\t%%' % accuracy(test_prediction.eval(), test_labels),a,b)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "('Test accuracy: 12.4%,\\t%', 1e-06, 1e-06)\n",
    "('Test accuracy: 12.4%,\\t%', 1e-06, 5e-06)\n",
    "('Test accuracy: 11.8%,\\t%', 1e-06, 1e-05)\n",
    "('Test accuracy: 13.7%,\\t%', 1e-06, 5e-05)\n",
    "('Test accuracy: 12.4%,\\t%', 1e-06, 0.0001)\n",
    "('Test accuracy: 9.3%,\\t%', 1e-06, 0.0005)\n",
    "('Test accuracy: 13.0%,\\t%', 1e-06, 0.001)\n",
    "('Test accuracy: 11.8%,\\t%', 1e-06, 0.005)\n",
    "('Test accuracy: 12.4%,\\t%', 1e-06, 0.01)\n",
    "('Test accuracy: 13.7%,\\t%', 1e-06, 0.05)\n",
    "('Test accuracy: 10.6%,\\t%', 1e-06, 0.1)\n",
    "('Test accuracy: 16.1%,\\t%', 1e-06, 0.5)\n",
    "('Test accuracy: 12.4%,\\t%', 5e-06, 1e-06)\n",
    "('Test accuracy: 9.9%,\\t%', 5e-06, 5e-06)\n",
    "('Test accuracy: 13.7%,\\t%', 5e-06, 1e-05)\n",
    "('Test accuracy: 9.9%,\\t%', 5e-06, 5e-05)\n",
    "('Test accuracy: 8.7%,\\t%', 5e-06, 0.0001)\n",
    "('Test accuracy: 7.5%,\\t%', 5e-06, 0.0005)\n",
    "('Test accuracy: 9.3%,\\t%', 5e-06, 0.001)\n",
    "('Test accuracy: 10.6%,\\t%', 5e-06, 0.005)\n",
    "('Test accuracy: 13.0%,\\t%', 5e-06, 0.01)\n",
    "('Test accuracy: 13.0%,\\t%', 5e-06, 0.05)\n",
    "('Test accuracy: 9.9%,\\t%', 5e-06, 0.1)\n",
    "('Test accuracy: 9.9%,\\t%', 5e-06, 0.5)\n",
    "('Test accuracy: 11.8%,\\t%', 1e-05, 1e-06)\n",
    "('Test accuracy: 12.4%,\\t%', 1e-05, 5e-06)\n",
    "('Test accuracy: 8.1%,\\t%', 1e-05, 1e-05)\n",
    "('Test accuracy: 10.6%,\\t%', 1e-05, 5e-05)\n",
    "('Test accuracy: 10.6%,\\t%', 1e-05, 0.0001)\n",
    "('Test accuracy: 10.6%,\\t%', 1e-05, 0.0005)\n",
    "('Test accuracy: 15.5%,\\t%', 1e-05, 0.001)\n",
    "('Test accuracy: 13.7%,\\t%', 1e-05, 0.005)\n",
    "('Test accuracy: 13.7%,\\t%', 1e-05, 0.01)\n",
    "('Test accuracy: 12.4%,\\t%', 1e-05, 0.05)\n",
    "('Test accuracy: 11.2%,\\t%', 1e-05, 0.1)\n",
    "('Test accuracy: 11.8%,\\t%', 1e-05, 0.5)\n",
    "('Test accuracy: 9.9%,\\t%', 5e-05, 1e-06)\n",
    "('Test accuracy: 10.6%,\\t%', 5e-05, 5e-06)\n",
    "('Test accuracy: 11.2%,\\t%', 5e-05, 1e-05)\n",
    "('Test accuracy: 12.4%,\\t%', 5e-05, 5e-05)\n",
    "('Test accuracy: 11.2%,\\t%', 5e-05, 0.0001)\n",
    "('Test accuracy: 13.7%,\\t%', 5e-05, 0.0005)\n",
    "('Test accuracy: 13.0%,\\t%', 5e-05, 0.001)\n",
    "('Test accuracy: 10.6%,\\t%', 5e-05, 0.005)\n",
    "('Test accuracy: 16.8%,\\t%', 5e-05, 0.01)\n",
    "('Test accuracy: 8.7%,\\t%', 5e-05, 0.05)\n",
    "('Test accuracy: 11.8%,\\t%', 5e-05, 0.1)\n",
    "('Test accuracy: 10.6%,\\t%', 5e-05, 0.5)\n",
    "('Test accuracy: 13.7%,\\t%', 0.0001, 1e-06)\n",
    "('Test accuracy: 14.9%,\\t%', 0.0001, 5e-06)\n",
    "('Test accuracy: 10.6%,\\t%', 0.0001, 1e-05)\n",
    "('Test accuracy: 13.7%,\\t%', 0.0001, 5e-05)\n",
    "('Test accuracy: 11.2%,\\t%', 0.0001, 0.0001)\n",
    "('Test accuracy: 7.5%,\\t%', 0.0001, 0.0005)\n",
    "('Test accuracy: 8.1%,\\t%', 0.0001, 0.001)\n",
    "('Test accuracy: 8.7%,\\t%', 0.0001, 0.005)\n",
    "('Test accuracy: 7.5%,\\t%', 0.0001, 0.01)\n",
    "('Test accuracy: 11.8%,\\t%', 0.0001, 0.05)\n",
    "('Test accuracy: 11.2%,\\t%', 0.0001, 0.1)\n",
    "('Test accuracy: 14.9%,\\t%', 0.0001, 0.5)\n",
    "('Test accuracy: 10.6%,\\t%', 0.0005, 1e-06)\n",
    "('Test accuracy: 16.8%,\\t%', 0.0005, 5e-06)\n",
    "('Test accuracy: 14.9%,\\t%', 0.0005, 1e-05)\n",
    "('Test accuracy: 16.1%,\\t%', 0.0005, 5e-05)\n",
    "('Test accuracy: 14.3%,\\t%', 0.0005, 0.0001)\n",
    "('Test accuracy: 9.9%,\\t%', 0.0005, 0.0005)\n",
    "('Test accuracy: 10.6%,\\t%', 0.0005, 0.001)\n",
    "('Test accuracy: 8.7%,\\t%', 0.0005, 0.005)\n",
    "('Test accuracy: 10.6%,\\t%', 0.0005, 0.01)\n",
    "('Test accuracy: 10.6%,\\t%', 0.0005, 0.05)\n",
    "('Test accuracy: 12.4%,\\t%', 0.0005, 0.1)\n",
    "('Test accuracy: 13.0%,\\t%', 0.0005, 0.5)\n",
    "('Test accuracy: 11.8%,\\t%', 0.001, 1e-06)\n",
    "('Test accuracy: 6.8%,\\t%', 0.001, 5e-06)\n",
    "('Test accuracy: 13.0%,\\t%', 0.001, 1e-05)\n",
    "('Test accuracy: 9.9%,\\t%', 0.001, 5e-05)\n",
    "('Test accuracy: 13.0%,\\t%', 0.001, 0.0001)\n",
    "('Test accuracy: 12.4%,\\t%', 0.001, 0.0005)\n",
    "('Test accuracy: 9.3%,\\t%', 0.001, 0.001)\n",
    "('Test accuracy: 11.8%,\\t%', 0.001, 0.005)\n",
    "('Test accuracy: 11.8%,\\t%', 0.001, 0.01)\n",
    "('Test accuracy: 11.2%,\\t%', 0.001, 0.05)\n",
    "('Test accuracy: 10.6%,\\t%', 0.001, 0.1)\n",
    "('Test accuracy: 14.3%,\\t%', 0.001, 0.5)\n",
    "('Test accuracy: 13.7%,\\t%', 0.005, 1e-06)\n",
    "('Test accuracy: 6.2%,\\t%', 0.005, 5e-06)\n",
    "('Test accuracy: 8.1%,\\t%', 0.005, 1e-05)\n",
    "('Test accuracy: 10.6%,\\t%', 0.005, 5e-05)\n",
    "('Test accuracy: 11.2%,\\t%', 0.005, 0.0001)\n",
    "('Test accuracy: 11.2%,\\t%', 0.005, 0.0005)\n",
    "('Test accuracy: 12.4%,\\t%', 0.005, 0.001)\n",
    "('Test accuracy: 12.4%,\\t%', 0.005, 0.005)\n",
    "('Test accuracy: 8.7%,\\t%', 0.005, 0.01)\n",
    "('Test accuracy: 8.1%,\\t%', 0.005, 0.05)\n",
    "('Test accuracy: 10.6%,\\t%', 0.005, 0.1)\n",
    "('Test accuracy: 11.2%,\\t%', 0.005, 0.5)\n",
    "('Test accuracy: 7.5%,\\t%', 0.01, 1e-06)\n",
    "('Test accuracy: 10.6%,\\t%', 0.01, 5e-06)\n",
    "('Test accuracy: 17.4%,\\t%', 0.01, 1e-05)\n",
    "('Test accuracy: 14.3%,\\t%', 0.01, 5e-05)\n",
    "('Test accuracy: 8.7%,\\t%', 0.01, 0.0001)\n",
    "('Test accuracy: 15.5%,\\t%', 0.01, 0.0005)\n",
    "('Test accuracy: 9.3%,\\t%', 0.01, 0.001)\n",
    "('Test accuracy: 8.1%,\\t%', 0.01, 0.005)\n",
    "('Test accuracy: 11.2%,\\t%', 0.01, 0.01)\n",
    "('Test accuracy: 11.2%,\\t%', 0.01, 0.05)\n",
    "('Test accuracy: 9.3%,\\t%', 0.01, 0.1)\n",
    "('Test accuracy: 8.1%,\\t%', 0.01, 0.5)\n",
    "('Test accuracy: 10.6%,\\t%', 0.05, 1e-06)\n",
    "('Test accuracy: 9.9%,\\t%', 0.05, 5e-06)\n",
    "('Test accuracy: 12.4%,\\t%', 0.05, 1e-05)\n",
    "('Test accuracy: 9.9%,\\t%', 0.05, 5e-05)\n",
    "('Test accuracy: 13.7%,\\t%', 0.05, 0.0001)\n",
    "('Test accuracy: 11.8%,\\t%', 0.05, 0.0005)\n",
    "('Test accuracy: 9.9%,\\t%', 0.05, 0.001)\n",
    "('Test accuracy: 14.9%,\\t%', 0.05, 0.005)\n",
    "('Test accuracy: 9.3%,\\t%', 0.05, 0.01)\n",
    "('Test accuracy: 14.3%,\\t%', 0.05, 0.05)\n",
    "('Test accuracy: 8.7%,\\t%', 0.05, 0.1)\n",
    "('Test accuracy: 7.5%,\\t%', 0.05, 0.5)\n",
    "('Test accuracy: 10.6%,\\t%', 0.1, 1e-06)\n",
    "('Test accuracy: 9.9%,\\t%', 0.1, 5e-06)\n",
    "('Test accuracy: 9.9%,\\t%', 0.1, 1e-05)\n",
    "('Test accuracy: 13.0%,\\t%', 0.1, 5e-05)\n",
    "('Test accuracy: 11.8%,\\t%', 0.1, 0.0001)\n",
    "('Test accuracy: 7.5%,\\t%', 0.1, 0.0005)\n",
    "('Test accuracy: 7.5%,\\t%', 0.1, 0.001)\n",
    "('Test accuracy: 13.0%,\\t%', 0.1, 0.005)\n",
    "('Test accuracy: 11.2%,\\t%', 0.1, 0.01)\n",
    "('Test accuracy: 10.6%,\\t%', 0.1, 0.05)\n",
    "('Test accuracy: 11.2%,\\t%', 0.1, 0.1)\n",
    "('Test accuracy: 10.6%,\\t%', 0.1, 0.5)\n",
    "('Test accuracy: 8.1%,\\t%', 0.5, 1e-06)\n",
    "('Test accuracy: 13.0%,\\t%', 0.5, 5e-06)\n",
    "('Test accuracy: 10.6%,\\t%', 0.5, 1e-05)\n",
    "('Test accuracy: 10.6%,\\t%', 0.5, 5e-05)\n",
    "('Test accuracy: 11.2%,\\t%', 0.5, 0.0001)\n",
    "('Test accuracy: 10.6%,\\t%', 0.5, 0.0005)\n",
    "('Test accuracy: 9.9%,\\t%', 0.5, 0.001)\n",
    "('Test accuracy: 11.2%,\\t%', 0.5, 0.005)\n",
    "('Test accuracy: 12.4%,\\t%', 0.5, 0.01)\n",
    "('Test accuracy: 6.8%,\\t%', 0.5, 0.05)\n",
    "('Test accuracy: 11.8%,\\t%', 0.5, 0.1)\n",
    "('Test accuracy: 7.5%,\\t%', 0.5, 0.5)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
