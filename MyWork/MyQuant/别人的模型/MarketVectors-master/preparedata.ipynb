{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 id=\"tocheading\">Table of Contents</h1>\n",
    "<div id=\"toc\"></div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I recently published a post about applying [deep learning to the stock market](https://medium.com/@TalPerry/deep-learning-the-stock-market-df853d139e02#.e33ao345g). This Notebook is an initial sketch of the implementation. People approach the market to make money, and I am no exception, so it may come as some suprise that I am giving this away. In fact, I don't think this implementation does make money. But it does illustrate a variety of concepts that are important for data science, deep learning and the application of both to finance. I learnt almost everything I know from tutorials like this one on the internet, and I hope someone will learn a thing or two from this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's in this thing?\n",
    "\n",
    "This notebook goes through the full the process of implementing the ideas I layed out in the post. That is it covers\n",
    "* Preparing the data\n",
    "* Building a baseline\n",
    "* Implementing a DL model\n",
    "\n",
    "## A (more or less) standard data science work flow\n",
    "\n",
    "### Defining the problem you want to solve\n",
    "I think all three of those stages are important, and in many respects they are ordered by both importance and the order in which I approach them. My first step in this process was writing that post (4.5K words) explaining in relatively simple terms what I want to do and why. \n",
    "\n",
    "### Collecting data\n",
    "The next stage was collecting data, this actually took me a while and I ended up going with a freely availble dataset you can download [here](https://quantquote.com/historical-stock-data). \n",
    "\n",
    "### Reconciling the problem and the data\n",
    "Once I had an idea of what problem I wanted to solve and I had some data I went about getting the data into a shape  I could work with. \n",
    "\n",
    "### Making a baseline\n",
    "Next I built a baseline model. A baseline doesn't need to be good, but it needs to be something you can compare your actual model to, so that you can know if all the fancy complicated things you did are any better than something simple. This is also a good point to choose how to measure your models performance.\n",
    "\n",
    "### Testing the idea\n",
    "I ended up bulding to baselines. One is a simple logistic regression and the other is a 3 layer feed forward network, or if you like big words, a multi layer perceptron. I made the first because I wanted a baseline and the second because I wanted to see if the additional complexity and computation cost of an RNN added benefit over a simpler network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepaparation\n",
    "Maybe its different for the more fortunate but in my experience 80% of the work in data science is data engineering. If I had to break that down even more it would be 20% \"science\", 20% engineering and 60% finding the right, clean data. \n",
    "For the project at hand I wanted a lot of data for many stocks. My \"production\" version uses slightly different data but for the POC using 10 years of the S&P was enough to prover my point.\n",
    "\n",
    "In case you missed it, the S&P500 is an index of 500 stocks that is updated once every 3 months. For POC purposes using daily data is sufficient and I ended up finding a free data set [here](https://quantquote.com/historical-stock-data). You can download it and point the *datapath* variable to point at it and runs this notebook yourself\n",
    "This notebook explores the data, joins all the stocks and creates targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preliminaries\n",
    "So first we need to import some libraries and define some helper functions\n",
    "* **get_ticker** applies a regex to the filename we are looking at and extracts the ticker from it\n",
    "* **ret calculates** to log return between two points. Returns should be observed in log space because\n",
    "    * Returns are log-normally distributed so log returns are follow a normal distribution\n",
    "    * You can some log returns instead of taking products which makes life easier\n",
    "* **zscore** maps a pandas series to it's zscore. In other words, it makes it have mean 0 and variance 1\n",
    "    * It's good to have variablised normalized like this as it makes all of the dimensions of your data behave the same\n",
    "    * It doesn't make sense to apply a z-score to distributions that don't (more or less) folllow the normal distribution. So check your variables first and if they don't follow a normal distribution transform them so that they do are use other scaling methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datapath = './daily'\n",
    "filepath = os.path.join(datapath,os.listdir('./daily')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./daily/table_a.csv', 'a')\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "ticker_regex = re.compile('.+_(?P<ticker>.+)\\.csv')\n",
    "get_ticker =lambda x :ticker_regex.match(x).groupdict()['ticker']\n",
    "print(filepath,get_ticker(filepath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ret = lambda x,y: log(y/x) #Log return \n",
    "zscore = lambda x:(x -x.mean())/x.std() # zscore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First peak at the data\n",
    "I blah blahed alot without actually looking at the data. lets load it. I use the pandas library to rad a single CSV. Since the data had no column headers I specified them. I don't know what the first column is so I labeled it UNK.\n",
    "Notice that the index, which is the real first column in the original data, is a date but in a string pandas didn't understand. We're going to parse it to a datetime object later so that pandas preserves the right order. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "D = pd.read_csv(filepath,header=None,names=['UNK','o','h','l','c','v']) #Load the dataframe with headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>UNK</th>\n",
       "      <th>o</th>\n",
       "      <th>h</th>\n",
       "      <th>l</th>\n",
       "      <th>c</th>\n",
       "      <th>v</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19991118</th>\n",
       "      <td>0</td>\n",
       "      <td>42.2076</td>\n",
       "      <td>46.3820</td>\n",
       "      <td>37.4581</td>\n",
       "      <td>39.1928</td>\n",
       "      <td>4.398181e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991119</th>\n",
       "      <td>0</td>\n",
       "      <td>39.8329</td>\n",
       "      <td>39.8885</td>\n",
       "      <td>36.9293</td>\n",
       "      <td>37.6251</td>\n",
       "      <td>1.139020e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991122</th>\n",
       "      <td>0</td>\n",
       "      <td>38.3208</td>\n",
       "      <td>40.0091</td>\n",
       "      <td>37.1613</td>\n",
       "      <td>39.9442</td>\n",
       "      <td>4.654716e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991123</th>\n",
       "      <td>0</td>\n",
       "      <td>39.4247</td>\n",
       "      <td>40.4729</td>\n",
       "      <td>37.3375</td>\n",
       "      <td>37.5138</td>\n",
       "      <td>4.268903e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19991124</th>\n",
       "      <td>0</td>\n",
       "      <td>37.2262</td>\n",
       "      <td>38.9052</td>\n",
       "      <td>37.1056</td>\n",
       "      <td>38.0889</td>\n",
       "      <td>3.602367e+06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          UNK        o        h        l        c             v\n",
       "19991118    0  42.2076  46.3820  37.4581  39.1928  4.398181e+07\n",
       "19991119    0  39.8329  39.8885  36.9293  37.6251  1.139020e+07\n",
       "19991122    0  38.3208  40.0091  37.1613  39.9442  4.654716e+06\n",
       "19991123    0  39.4247  40.4729  37.3375  37.5138  4.268903e+06\n",
       "19991124    0  37.2262  38.9052  37.1056  38.0889  3.602367e+06"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D.head() #Lets peack at it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Relevant Data\n",
    "We don't care about prices, just about their fluctations. So we'll give the log return of the various prices, and take a zscore so everything is nice. \n",
    "Also, extract the ticker from the file name and add it to the df as a column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_inputs(filepath):\n",
    "    D = pd.read_csv(filepath,header=None,names=['UNK','o','h','l','c','v']) #Load the dataframe with headers\n",
    "    D.index = pd.to_datetime(D.index,format='%Y%m%d') # Set the indix to a datetime\n",
    "    Res = pd.DataFrame()\n",
    "    ticker = get_ticker(filepath)\n",
    "\n",
    "    Res['c_2_o'] = zscore(ret(D.o,D.c))\n",
    "    Res['h_2_o'] = zscore(ret(D.o,D.h))\n",
    "    Res['l_2_o'] = zscore(ret(D.o,D.l))\n",
    "    Res['c_2_h'] = zscore(ret(D.h,D.c))\n",
    "    Res['h_2_l'] = zscore(ret(D.h,D.l))\n",
    "    Res['c1_c0'] = ret(D.c,D.c.shift(-1)).fillna(0) #Tommorows return \n",
    "    Res['vol'] = zscore(D.v)\n",
    "    Res['ticker'] = ticker\n",
    "    return Res\n",
    "Res = make_inputs(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_2_o</th>\n",
       "      <th>h_2_o</th>\n",
       "      <th>l_2_o</th>\n",
       "      <th>c_2_h</th>\n",
       "      <th>h_2_l</th>\n",
       "      <th>c1_c0</th>\n",
       "      <th>vol</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999-11-18</th>\n",
       "      <td>-2.875906</td>\n",
       "      <td>3.942736</td>\n",
       "      <td>-5.176517</td>\n",
       "      <td>-8.313468</td>\n",
       "      <td>-7.250098</td>\n",
       "      <td>-0.040822</td>\n",
       "      <td>22.437430</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-19</th>\n",
       "      <td>-2.218402</td>\n",
       "      <td>-0.819624</td>\n",
       "      <td>-2.967516</td>\n",
       "      <td>-2.283408</td>\n",
       "      <td>-1.726542</td>\n",
       "      <td>0.059812</td>\n",
       "      <td>4.609137</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-22</th>\n",
       "      <td>1.572687</td>\n",
       "      <td>1.318667</td>\n",
       "      <td>-0.694121</td>\n",
       "      <td>0.829875</td>\n",
       "      <td>-1.595383</td>\n",
       "      <td>-0.062775</td>\n",
       "      <td>0.924687</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-23</th>\n",
       "      <td>-1.936005</td>\n",
       "      <td>0.453801</td>\n",
       "      <td>-1.890881</td>\n",
       "      <td>-3.243109</td>\n",
       "      <td>-1.870150</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.713638</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-24</th>\n",
       "      <td>0.857645</td>\n",
       "      <td>1.369972</td>\n",
       "      <td>0.695310</td>\n",
       "      <td>-0.243544</td>\n",
       "      <td>-0.524722</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.349030</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               c_2_o     h_2_o     l_2_o     c_2_h     h_2_l     c1_c0  \\\n",
       "1999-11-18 -2.875906  3.942736 -5.176517 -8.313468 -7.250098 -0.040822   \n",
       "1999-11-19 -2.218402 -0.819624 -2.967516 -2.283408 -1.726542  0.059812   \n",
       "1999-11-22  1.572687  1.318667 -0.694121  0.829875 -1.595383 -0.062775   \n",
       "1999-11-23 -1.936005  0.453801 -1.890881 -3.243109 -1.870150  0.015214   \n",
       "1999-11-24  0.857645  1.369972  0.695310 -0.243544 -0.524722  0.003161   \n",
       "\n",
       "                  vol ticker  \n",
       "1999-11-18  22.437430      a  \n",
       "1999-11-19   4.609137      a  \n",
       "1999-11-22   0.924687      a  \n",
       "1999-11-23   0.713638      a  \n",
       "1999-11-24   0.349030      a  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res.head() # Lets look at what we got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_2_o</th>\n",
       "      <th>h_2_o</th>\n",
       "      <th>l_2_o</th>\n",
       "      <th>c_2_h</th>\n",
       "      <th>h_2_l</th>\n",
       "      <th>c1_c0</th>\n",
       "      <th>vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>c_2_o</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.713152</td>\n",
       "      <td>0.699395</td>\n",
       "      <td>0.661722</td>\n",
       "      <td>-0.003292</td>\n",
       "      <td>0.046037</td>\n",
       "      <td>-0.048971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_2_o</th>\n",
       "      <td>0.713152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.207554</td>\n",
       "      <td>-0.053673</td>\n",
       "      <td>-0.622913</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.172745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l_2_o</th>\n",
       "      <td>0.699395</td>\n",
       "      <td>0.207554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.774273</td>\n",
       "      <td>0.635967</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>-0.218147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_2_h</th>\n",
       "      <td>0.661722</td>\n",
       "      <td>-0.053673</td>\n",
       "      <td>0.774273</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.661533</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>-0.254513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h_2_l</th>\n",
       "      <td>-0.003292</td>\n",
       "      <td>-0.622913</td>\n",
       "      <td>0.635967</td>\n",
       "      <td>0.661533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.036401</td>\n",
       "      <td>-0.310731</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c1_c0</th>\n",
       "      <td>0.046037</td>\n",
       "      <td>0.018455</td>\n",
       "      <td>0.063724</td>\n",
       "      <td>0.045840</td>\n",
       "      <td>0.036401</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.003882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vol</th>\n",
       "      <td>-0.048971</td>\n",
       "      <td>0.172745</td>\n",
       "      <td>-0.218147</td>\n",
       "      <td>-0.254513</td>\n",
       "      <td>-0.310731</td>\n",
       "      <td>0.003882</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          c_2_o     h_2_o     l_2_o     c_2_h     h_2_l     c1_c0       vol\n",
       "c_2_o  1.000000  0.713152  0.699395  0.661722 -0.003292  0.046037 -0.048971\n",
       "h_2_o  0.713152  1.000000  0.207554 -0.053673 -0.622913  0.018455  0.172745\n",
       "l_2_o  0.699395  0.207554  1.000000  0.774273  0.635967  0.063724 -0.218147\n",
       "c_2_h  0.661722 -0.053673  0.774273  1.000000  0.661533  0.045840 -0.254513\n",
       "h_2_l -0.003292 -0.622913  0.635967  0.661533  1.000000  0.036401 -0.310731\n",
       "c1_c0  0.046037  0.018455  0.063724  0.045840  0.036401  1.000000  0.003882\n",
       "vol   -0.048971  0.172745 -0.218147 -0.254513 -0.310731  0.003882  1.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Res.corr() #Quick check to see we didn't mess it up. All values should be different, otherwise we repeated a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating the full data set\n",
    "I'll iterate over each file, run the above and concat to a final df. Then we'll pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Final = pd.DataFrame()\n",
    "for f in os.listdir(datapath):\n",
    "    filepath = os.path.join(datapath,f)\n",
    "    if filepath.endswith('.csv'):\n",
    "        Res = make_inputs(filepath)\n",
    "        Final = Final.append(Res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c_2_o</th>\n",
       "      <th>h_2_o</th>\n",
       "      <th>l_2_o</th>\n",
       "      <th>c_2_h</th>\n",
       "      <th>h_2_l</th>\n",
       "      <th>c1_c0</th>\n",
       "      <th>vol</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1999-11-18</th>\n",
       "      <td>-2.875906</td>\n",
       "      <td>3.942736</td>\n",
       "      <td>-5.176517</td>\n",
       "      <td>-8.313468</td>\n",
       "      <td>-7.250098</td>\n",
       "      <td>-0.040822</td>\n",
       "      <td>22.437430</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-19</th>\n",
       "      <td>-2.218402</td>\n",
       "      <td>-0.819624</td>\n",
       "      <td>-2.967516</td>\n",
       "      <td>-2.283408</td>\n",
       "      <td>-1.726542</td>\n",
       "      <td>0.059812</td>\n",
       "      <td>4.609137</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-22</th>\n",
       "      <td>1.572687</td>\n",
       "      <td>1.318667</td>\n",
       "      <td>-0.694121</td>\n",
       "      <td>0.829875</td>\n",
       "      <td>-1.595383</td>\n",
       "      <td>-0.062775</td>\n",
       "      <td>0.924687</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-23</th>\n",
       "      <td>-1.936005</td>\n",
       "      <td>0.453801</td>\n",
       "      <td>-1.890881</td>\n",
       "      <td>-3.243109</td>\n",
       "      <td>-1.870150</td>\n",
       "      <td>0.015214</td>\n",
       "      <td>0.713638</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1999-11-24</th>\n",
       "      <td>0.857645</td>\n",
       "      <td>1.369972</td>\n",
       "      <td>0.695310</td>\n",
       "      <td>-0.243544</td>\n",
       "      <td>-0.524722</td>\n",
       "      <td>0.003161</td>\n",
       "      <td>0.349030</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               c_2_o     h_2_o     l_2_o     c_2_h     h_2_l     c1_c0  \\\n",
       "1999-11-18 -2.875906  3.942736 -5.176517 -8.313468 -7.250098 -0.040822   \n",
       "1999-11-19 -2.218402 -0.819624 -2.967516 -2.283408 -1.726542  0.059812   \n",
       "1999-11-22  1.572687  1.318667 -0.694121  0.829875 -1.595383 -0.062775   \n",
       "1999-11-23 -1.936005  0.453801 -1.890881 -3.243109 -1.870150  0.015214   \n",
       "1999-11-24  0.857645  1.369972  0.695310 -0.243544 -0.524722  0.003161   \n",
       "\n",
       "                  vol ticker  \n",
       "1999-11-18  22.437430      a  \n",
       "1999-11-19   4.609137      a  \n",
       "1999-11-22   0.924687      a  \n",
       "1999-11-23   0.713638      a  \n",
       "1999-11-24   0.349030      a  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pivot_columns = Final.columns[:-1]\n",
    "P = Final.pivot_table(index=Final.index,columns='ticker',values=pivot_columns) # Make a pivot table from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">c_2_o</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">vol</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ticker</th>\n",
       "      <th>a</th>\n",
       "      <th>aa</th>\n",
       "      <th>aapl</th>\n",
       "      <th>abbv</th>\n",
       "      <th>abc</th>\n",
       "      <th>abt</th>\n",
       "      <th>ace</th>\n",
       "      <th>acn</th>\n",
       "      <th>act</th>\n",
       "      <th>adbe</th>\n",
       "      <th>...</th>\n",
       "      <th>xl</th>\n",
       "      <th>xlnx</th>\n",
       "      <th>xom</th>\n",
       "      <th>xray</th>\n",
       "      <th>xrx</th>\n",
       "      <th>xyl</th>\n",
       "      <th>yhoo</th>\n",
       "      <th>yum</th>\n",
       "      <th>zion</th>\n",
       "      <th>zmh</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.017649</td>\n",
       "      <td>6.991394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.047250</td>\n",
       "      <td>1.845516</td>\n",
       "      <td>-0.601609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.911471</td>\n",
       "      <td>-0.375461</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.676802</td>\n",
       "      <td>1.391497</td>\n",
       "      <td>-1.045626</td>\n",
       "      <td>-1.147952</td>\n",
       "      <td>-0.817355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.368713</td>\n",
       "      <td>-0.678763</td>\n",
       "      <td>-0.724482</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.373488</td>\n",
       "      <td>-1.224062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.160945</td>\n",
       "      <td>0.031659</td>\n",
       "      <td>-0.234577</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.090086</td>\n",
       "      <td>0.791331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.621025</td>\n",
       "      <td>1.019607</td>\n",
       "      <td>-0.615370</td>\n",
       "      <td>-0.556684</td>\n",
       "      <td>-0.757660</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064841</td>\n",
       "      <td>0.980283</td>\n",
       "      <td>-0.735275</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.718569</td>\n",
       "      <td>6.857266</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.102853</td>\n",
       "      <td>-0.473164</td>\n",
       "      <td>0.452262</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.239961</td>\n",
       "      <td>1.451409</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.634169</td>\n",
       "      <td>0.020195</td>\n",
       "      <td>-0.527923</td>\n",
       "      <td>-1.131583</td>\n",
       "      <td>-0.629732</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.150071</td>\n",
       "      <td>1.697887</td>\n",
       "      <td>-0.683202</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.980669</td>\n",
       "      <td>-3.007557</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.553079</td>\n",
       "      <td>0.746779</td>\n",
       "      <td>0.137372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.878292</td>\n",
       "      <td>-0.534781</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616242</td>\n",
       "      <td>1.023705</td>\n",
       "      <td>-0.134130</td>\n",
       "      <td>-0.329006</td>\n",
       "      <td>-0.504232</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.463651</td>\n",
       "      <td>0.515268</td>\n",
       "      <td>-0.635858</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.828950</td>\n",
       "      <td>1.388888</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.488170</td>\n",
       "      <td>-0.221620</td>\n",
       "      <td>-0.484264</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.300492</td>\n",
       "      <td>0.538778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.655003</td>\n",
       "      <td>0.645285</td>\n",
       "      <td>-0.907375</td>\n",
       "      <td>-1.066107</td>\n",
       "      <td>-0.560549</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.157777</td>\n",
       "      <td>0.235907</td>\n",
       "      <td>-0.605092</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           c_2_o                                                             \\\n",
       "ticker         a        aa      aapl abbv       abc       abt       ace acn   \n",
       "1998-01-02   NaN -0.017649  6.991394  NaN  0.047250  1.845516 -0.601609 NaN   \n",
       "1998-01-05   NaN  0.373488 -1.224062  NaN -0.160945  0.031659 -0.234577 NaN   \n",
       "1998-01-06   NaN -0.718569  6.857266  NaN -1.102853 -0.473164  0.452262 NaN   \n",
       "1998-01-07   NaN  0.980669 -3.007557  NaN  0.553079  0.746779  0.137372 NaN   \n",
       "1998-01-08   NaN -1.828950  1.388888  NaN  0.488170 -0.221620 -0.484264 NaN   \n",
       "\n",
       "                               ...       vol                                \\\n",
       "ticker           act      adbe ...        xl      xlnx       xom      xray   \n",
       "1998-01-02 -0.911471 -0.375461 ... -0.676802  1.391497 -1.045626 -1.147952   \n",
       "1998-01-05  2.090086  0.791331 ... -0.621025  1.019607 -0.615370 -0.556684   \n",
       "1998-01-06 -1.239961  1.451409 ... -0.634169  0.020195 -0.527923 -1.131583   \n",
       "1998-01-07 -0.878292 -0.534781 ... -0.616242  1.023705 -0.134130 -0.329006   \n",
       "1998-01-08  1.300492  0.538778 ... -0.655003  0.645285 -0.907375 -1.066107   \n",
       "\n",
       "                                                            \n",
       "ticker           xrx xyl      yhoo       yum      zion zmh  \n",
       "1998-01-02 -0.817355 NaN -0.368713 -0.678763 -0.724482 NaN  \n",
       "1998-01-05 -0.757660 NaN  0.064841  0.980283 -0.735275 NaN  \n",
       "1998-01-06 -0.629732 NaN  0.150071  1.697887 -0.683202 NaN  \n",
       "1998-01-07 -0.504232 NaN -0.463651  0.515268 -0.635858 NaN  \n",
       "1998-01-08 -0.560549 NaN -0.157777  0.235907 -0.605092 NaN  \n",
       "\n",
       "[5 rows x 3500 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flattening the pivot\n",
    "source http://stackoverflow.com/questions/14507794/python-pandas-how-to-flatten-a-hierarchical-index-in-columns\n",
    "At the end of this P is a flattened dataframe of all the entries for each stock, one day per row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mi = P.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_ind = pd.Index(e[1] +'_' + e[0] for e in mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:2: FutureWarning: sort(....) is deprecated, use sort_index(.....)\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "P.columns = new_ind\n",
    "P = P.sort(axis=1) # Sort by columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a_c1_c0</th>\n",
       "      <th>a_c_2_h</th>\n",
       "      <th>a_c_2_o</th>\n",
       "      <th>a_h_2_l</th>\n",
       "      <th>a_h_2_o</th>\n",
       "      <th>a_l_2_o</th>\n",
       "      <th>a_vol</th>\n",
       "      <th>aa_c1_c0</th>\n",
       "      <th>aa_c_2_h</th>\n",
       "      <th>aa_c_2_o</th>\n",
       "      <th>...</th>\n",
       "      <th>zion_h_2_o</th>\n",
       "      <th>zion_l_2_o</th>\n",
       "      <th>zion_vol</th>\n",
       "      <th>zmh_c1_c0</th>\n",
       "      <th>zmh_c_2_h</th>\n",
       "      <th>zmh_c_2_o</th>\n",
       "      <th>zmh_h_2_l</th>\n",
       "      <th>zmh_h_2_o</th>\n",
       "      <th>zmh_l_2_o</th>\n",
       "      <th>zmh_vol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1998-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.017390</td>\n",
       "      <td>0.670259</td>\n",
       "      <td>-0.017649</td>\n",
       "      <td>...</td>\n",
       "      <td>0.246401</td>\n",
       "      <td>0.710284</td>\n",
       "      <td>-0.724482</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-05</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.022601</td>\n",
       "      <td>0.462520</td>\n",
       "      <td>0.373488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.127875</td>\n",
       "      <td>0.710284</td>\n",
       "      <td>-0.735275</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-06</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001693</td>\n",
       "      <td>-0.088471</td>\n",
       "      <td>-0.718569</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677287</td>\n",
       "      <td>-1.264514</td>\n",
       "      <td>-0.683202</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-07</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.042290</td>\n",
       "      <td>1.002099</td>\n",
       "      <td>0.980669</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.677287</td>\n",
       "      <td>-1.175835</td>\n",
       "      <td>-0.635858</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1998-01-08</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.024094</td>\n",
       "      <td>-2.074267</td>\n",
       "      <td>-1.828950</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.202411</td>\n",
       "      <td>-2.278200</td>\n",
       "      <td>-0.605092</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 3500 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            a_c1_c0  a_c_2_h  a_c_2_o  a_h_2_l  a_h_2_o  a_l_2_o  a_vol  \\\n",
       "1998-01-02      NaN      NaN      NaN      NaN      NaN      NaN    NaN   \n",
       "1998-01-05      NaN      NaN      NaN      NaN      NaN      NaN    NaN   \n",
       "1998-01-06      NaN      NaN      NaN      NaN      NaN      NaN    NaN   \n",
       "1998-01-07      NaN      NaN      NaN      NaN      NaN      NaN    NaN   \n",
       "1998-01-08      NaN      NaN      NaN      NaN      NaN      NaN    NaN   \n",
       "\n",
       "            aa_c1_c0  aa_c_2_h  aa_c_2_o   ...     zion_h_2_o  zion_l_2_o  \\\n",
       "1998-01-02  0.017390  0.670259 -0.017649   ...       0.246401    0.710284   \n",
       "1998-01-05 -0.022601  0.462520  0.373488   ...       0.127875    0.710284   \n",
       "1998-01-06  0.001693 -0.088471 -0.718569   ...      -0.677287   -1.264514   \n",
       "1998-01-07 -0.042290  1.002099  0.980669   ...      -0.677287   -1.175835   \n",
       "1998-01-08 -0.024094 -2.074267 -1.828950   ...      -0.202411   -2.278200   \n",
       "\n",
       "            zion_vol  zmh_c1_c0  zmh_c_2_h  zmh_c_2_o  zmh_h_2_l  zmh_h_2_o  \\\n",
       "1998-01-02 -0.724482        NaN        NaN        NaN        NaN        NaN   \n",
       "1998-01-05 -0.735275        NaN        NaN        NaN        NaN        NaN   \n",
       "1998-01-06 -0.683202        NaN        NaN        NaN        NaN        NaN   \n",
       "1998-01-07 -0.635858        NaN        NaN        NaN        NaN        NaN   \n",
       "1998-01-08 -0.605092        NaN        NaN        NaN        NaN        NaN   \n",
       "\n",
       "            zmh_l_2_o  zmh_vol  \n",
       "1998-01-02        NaN      NaN  \n",
       "1998-01-05        NaN      NaN  \n",
       "1998-01-06        NaN      NaN  \n",
       "1998-01-07        NaN      NaN  \n",
       "1998-01-08        NaN      NaN  \n",
       "\n",
       "[5 rows x 3500 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clean_and_flat = P.dropna(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_cols = list(filter(lambda x: 'c1_c0' in x, clean_and_flat.columns.values))\n",
    "input_cols  = list(filter(lambda x: 'c1_c0' not in x, clean_and_flat.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "InputDF = clean_and_flat[input_cols][:3900]\n",
    "TargetDF = clean_and_flat[target_cols][:3900]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corrs = TargetDF.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Targets\n",
    "We now have an our inputs and targets, kind of. \n",
    "InputsDF has all the inputs we want to predict. Targets DF has the return of each stock each day. \n",
    "For starters, lets give a simpler target to predict than the reuturn of each stock, since we don't have much data. \n",
    "\n",
    "\n",
    "We're going to label the targets as either up (1) down (-1) or flat (0) days.\n",
    "The top chart shows what would happen if we bought 1 dollar of ewach stock each day\n",
    "The bottom chart shows what would happen if we longed the whole basket on (1) days, shorted it on down days (-1) and ignored it on  (0) days. \n",
    "You can see that this is a valuable target to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_stocks = len(TargetDF.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TotalReturn = ((1-exp(TargetDF)).sum(1))/num_stocks # If i put one dollar in each stock at the close, this is how much I'd get back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def labeler(x):\n",
    "    if x>0.0029:\n",
    "        return 1\n",
    "    if x<-0.00462:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labeled = pd.DataFrame()\n",
    "Labeled['return'] = TotalReturn\n",
    "Labeled['class'] = TotalReturn.apply(labeler,1)\n",
    "Labeled['multi_class'] = pd.qcut(TotalReturn,11,labels=range(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-0.00301, 0.00119], (0.00741, 0.0999], (0.00119, 0.00741], [-0.108, -0.00877], (-0.00877, -0.00301]]\n",
       "Categories (5, object): [[-0.108, -0.00877] < (-0.00877, -0.00301] < (-0.00301, 0.00119] < (0.00119, 0.00741] < (0.00741, 0.0999]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(TotalReturn,5).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def labeler_multi(x):\n",
    "    if x>0.0029:\n",
    "        return 1\n",
    "    if x<-0.00462:\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 1    1301\n",
       " 0    1301\n",
       "-1    1298\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Labeled['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labeled['act_return'] = Labeled['class'] * Labeled['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x115515dd0>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x116a65a50>], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAADyCAYAAABd/T4iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1155154d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Labeled[['return','act_return']].cumsum().plot(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "logreg = linear_model.LogisticRegression(C=1e5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_size=600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "res = logreg.fit(InputDF[:-test_size],Labeled['multi_class'][:-test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.20      0.04      0.06        52\n",
      "          1       0.08      0.06      0.07        50\n",
      "          2       0.09      0.17      0.11        60\n",
      "          3       0.12      0.11      0.12        62\n",
      "          4       0.10      0.06      0.07        50\n",
      "          5       0.06      0.12      0.08        51\n",
      "          6       0.15      0.23      0.18        60\n",
      "          7       0.05      0.04      0.04        53\n",
      "          8       0.06      0.06      0.06        54\n",
      "          9       0.11      0.07      0.09        56\n",
      "         10       0.00      0.00      0.00        52\n",
      "\n",
      "avg / total       0.09      0.09      0.08       600\n",
      "\n",
      "[[ 2  3 11  7  1  6  7  3  3  4  5]\n",
      " [ 1  3  8  8  7  7  6  2  3  2  3]\n",
      " [ 1  3 10  5  2 13  5 11  5  3  2]\n",
      " [ 1  5  9  7  3 15  6  2  8  4  2]\n",
      " [ 1  3  6  2  3  9 10  5  7  4  0]\n",
      " [ 0  4 12  3  2  6  6  4  5  6  3]\n",
      " [ 1  2 15  3  1 12 14  1  4  3  4]\n",
      " [ 0  4  8  4  3  8 11  2 10  3  0]\n",
      " [ 0  2 16  6  2  9 12  3  3  0  1]\n",
      " [ 0  7 11  5  2  7 10  4  4  4  2]\n",
      " [ 3  3 11  6  4  9  6  4  2  4  0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "print(classification_report(Labeled['multi_class'][-test_size:],res.predict(InputDF[-test_size:])))\n",
    "print(confusion_matrix(Labeled['multi_class'][-test_size:],res.predict(InputDF[-test_size:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[89 36 78]\n",
      " [72 35 93]\n",
      " [91 30 76]]\n"
     ]
    }
   ],
   "source": [
    "Labeled['predicted_action'] = list(map(lambda x: -1 if x <5 else 0 if x==5 else 1,res.predict(InputDF)))\n",
    "print(confusion_matrix(Labeled['class'][-test_size:],Labeled['predicted_action'][-test_size:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labeled['pred_return'] = Labeled['predicted_action'] * Labeled['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1232c2d90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD6CAYAAACh4jDWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0ywRlTITqjMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1232c2250>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Res = Labeled[-test_size:][['return','act_return','pred_return']].cumsum()\n",
    "Res[0] =0\n",
    "Res.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training a basic feed forward network\n",
    "Here I'll use the tensorflow contrib.learn to quickly train a feed forward network. More of a benchmark than something I plan on using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from  tensorflow.contrib.learn.python.learn.estimators.dnn  import DNNClassifier\n",
    "from tensorflow.contrib.layers import real_valued_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labeled['tf_class'] = Labeled['multi_class']\n",
    "num_features = len(InputDF.columns)\n",
    "dropout=0.2\n",
    "hidden_1_size = 1000\n",
    "hidden_2_size = 250\n",
    "num_classes = Labeled.tf_class.nunique()\n",
    "NUM_EPOCHS=100\n",
    "BATCH_SIZE=50\n",
    "lr=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = (InputDF[:-test_size].values,Labeled.tf_class[:-test_size].values)\n",
    "val = (InputDF[-test_size:].values,Labeled.tf_class[-test_size:].values)\n",
    "NUM_TRAIN_BATCHES = int(len(train[0])/BATCH_SIZE)\n",
    "NUM_VAL_BATCHES = int(len(val[1])/BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3900"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(InputDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        self.input_data = tf.placeholder(dtype=tf.float32,shape=[None,num_features])\n",
    "        self.target_data = tf.placeholder(dtype=tf.int32,shape=[None])\n",
    "        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "        with tf.variable_scope(\"ff\"):\n",
    "            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\n",
    "            \n",
    "            layer_1 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=hidden_1_size,\n",
    "                inputs=droped_input,\n",
    "            )\n",
    "            layer_2 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=hidden_2_size,\n",
    "                inputs=layer_1,\n",
    "            )\n",
    "            self.logits = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=num_classes,\n",
    "                activation_fn =None,\n",
    "                inputs=layer_2,\n",
    "            )\n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            \n",
    "            self.losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.target_data, logits=self.logits)\n",
    "            mask = (1-tf.sign(1-self.target_data)) #Don't give credit for flat days\n",
    "            mask = tf.cast(mask,tf.float32)            \n",
    "            self.loss = tf.reduce_sum(self.losses)\n",
    "        \n",
    "        with tf.name_scope(\"train\"):\n",
    "          opt = tf.train.AdamOptimizer(lr)\n",
    "          gvs = opt.compute_gradients(self.loss)\n",
    "          self.train_op = opt.apply_gradients(gvs, global_step=global_step)\n",
    "        \n",
    "        with tf.name_scope(\"predictions\"):\n",
    "            self.probs = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(self.probs, 1)\n",
    "            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\n",
    "            self.accuracy = tf.reduce_mean(correct_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "logits and labels must be same size: logits_size=[50,11] labels_size=[1,50]\n\t [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op u'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-74-b8216b954a56>\", line 2, in <module>\n    model = Model()\n  File \"<ipython-input-73-11e82e52d518>\", line 25, in __init__\n    self.losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.target_data, logits=self.logits)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[50,11] labels_size=[1,50]\n\t [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/Reshape, loss/Reshape_1)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-b8216b954a56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                     ]\n\u001b[0;32m---> 29\u001b[0;31m                     \u001b[0;34m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                 )\n\u001b[1;32m     31\u001b[0m                 \u001b[0mepoch_loss\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: logits and labels must be same size: logits_size=[50,11] labels_size=[1,50]\n\t [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/Reshape, loss/Reshape_1)]]\n\nCaused by op u'loss/SoftmaxCrossEntropyWithLogits', defined at:\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-74-b8216b954a56>\", line 2, in <module>\n    model = Model()\n  File \"<ipython-input-73-11e82e52d518>\", line 25, in __init__\n    self.losses = tf.nn.softmax_cross_entropy_with_logits(labels=self.target_data, logits=self.logits)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/nn_ops.py\", line 1617, in softmax_cross_entropy_with_logits\n    precise_logits, labels, name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\", line 2265, in _softmax_cross_entropy_with_logits\n    features=features, labels=labels, name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2395, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1264, in __init__\n    self._traceback = _extract_stack()\n\nInvalidArgumentError (see above for traceback): logits and labels must be same size: logits_size=[50,11] labels_size=[1,50]\n\t [[Node: loss/SoftmaxCrossEntropyWithLogits = SoftmaxCrossEntropyWithLogits[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](loss/Reshape, loss/Reshape_1)]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = Model()\n",
    "    input_ = train[0]\n",
    "    target = train[1]\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run([init])\n",
    "        epoch_loss =0\n",
    "        for e in range(NUM_EPOCHS):\n",
    "            if epoch_loss >0 and epoch_loss <1:\n",
    "                break\n",
    "            epoch_loss =0\n",
    "            for batch in range(0,NUM_TRAIN_BATCHES):\n",
    "                \n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:input_[start:end],\n",
    "                    model.target_data:target[start:end],\n",
    "                    model.dropout_prob:0.9\n",
    "                            }\n",
    "                \n",
    "                _,loss,acc = sess.run(\n",
    "                    [\n",
    "                        model.train_op,\n",
    "                        model.loss,\n",
    "                        model.accuracy,\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                epoch_loss+=loss\n",
    "            print('step - {0} loss - {1} acc - {2}'.format((1+batch+NUM_TRAIN_BATCHES*e),epoch_loss,acc))\n",
    "                \n",
    "        \n",
    "        print('done training')\n",
    "        final_preds =np.array([])\n",
    "        final_probs =None\n",
    "        for batch in range(0,NUM_VAL_BATCHES):\n",
    "            \n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:val[0][start:end],\n",
    "                    model.target_data:val[1][start:end],\n",
    "                    model.dropout_prob:1\n",
    "                            }\n",
    "                \n",
    "                acc,preds,probs = sess.run(\n",
    "                    [\n",
    "                        model.accuracy,\n",
    "                        model.predictions,\n",
    "                        model.probs\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                print(acc)\n",
    "                final_preds = np.concatenate((final_preds,preds),axis=0)\n",
    "                if final_probs is None:\n",
    "                    final_probs = probs\n",
    "                else:\n",
    "                    final_probs = np.concatenate((final_probs,probs),axis=0)\n",
    "        prediction_conf = final_probs[np.argmax(final_probs,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result = Labeled[-test_size:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'final_preds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-3138d7459813>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nn_pred'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_preds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mod_nn_prod'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinal_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nn_ret'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmod_nn_prod\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mResult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'return'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'final_preds' is not defined"
     ]
    }
   ],
   "source": [
    "Result['nn_pred'] = final_preds\n",
    "Result['mod_nn_prod'] = list(map(lambda x: -1 if x <5 else 0 if x==5 else 1,final_preds))\n",
    "Result['nn_ret'] = Result.mod_nn_prod*Result['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Res = Result[-test_size:][['return','act_return','pred_return','nn_ret']].cumsum()\n",
    "Res = (1+Result[-test_size:][['return','act_return','nn_ret','pred_return']]).cumprod()\n",
    "Res[0] =0\n",
    "Res.plot(secondary_y='act_return')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(Result['class'],Result['mod_nn_prod']))\n",
    "print(classification_report(Result['class'],Result['mod_nn_prod']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cm = pd.DataFrame(confusion_matrix(Result['multi_class'],Result['nn_pred']))\n",
    "#sns.heatmap(cm.div(cm.sum(1)))\n",
    "Result[Result.multi_class==6]['return'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(classification_report(Result['multi_class'],Result['nn_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result.hist(by='multi_class',column='return',sharex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Event - RNN\n",
    "In this section we'll make an rnn model that learns to take the past into account as well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining an rnn Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.layers.python.layers.initializers import xavier_initializer\n",
    "RNN_HIDDEN_SIZE=100\n",
    "FIRST_LAYER_SIZE=1000\n",
    "SECOND_LAYER_SIZE=250\n",
    "NUM_LAYERS=2\n",
    "BATCH_SIZE=50\n",
    "NUM_EPOCHS=200\n",
    "lr=0.0003\n",
    "NUM_TRAIN_BATCHES = int(len(train[0])/BATCH_SIZE)\n",
    "NUM_VAL_BATCHES = int(len(val[1])/BATCH_SIZE)\n",
    "ATTN_LENGTH=30\n",
    "beta=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class RNNModel():\n",
    "    def __init__(self):\n",
    "        global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        self.input_data = tf.placeholder(dtype=tf.float32,shape=[BATCH_SIZE,num_features])\n",
    "        self.target_data = tf.placeholder(dtype=tf.int32,shape=[BATCH_SIZE])\n",
    "        self.dropout_prob = tf.placeholder(dtype=tf.float32,shape=[])\n",
    "        \n",
    "        def makeGRUCells():\n",
    "            base_cell = tf.nn.rnn_cell.GRUCell(num_units=RNN_HIDDEN_SIZE,) \n",
    "            layered_cell = tf.nn.rnn_cell.MultiRNNCell([base_cell] * NUM_LAYERS,state_is_tuple=False) \n",
    "            attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=layered_cell,attn_length=ATTN_LENGTH,state_is_tuple=False)\n",
    "            return attn_cell\n",
    "        \n",
    "        self.gru_cell = makeGRUCells()\n",
    "        self.zero_state = self.gru_cell.zero_state(1, tf.float32)\n",
    "        \n",
    "        self.start_state = tf.placeholder(dtype=tf.float32,shape=[1,self.gru_cell.state_size])\n",
    "        \n",
    "        \n",
    "\n",
    "        with tf.variable_scope(\"ff\",initializer=xavier_initializer(uniform=False)):\n",
    "            droped_input = tf.nn.dropout(self.input_data,keep_prob=self.dropout_prob)\n",
    "            \n",
    "            layer_1 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=FIRST_LAYER_SIZE,\n",
    "                inputs=droped_input,\n",
    "                \n",
    "            )\n",
    "            layer_2 = tf.contrib.layers.fully_connected(\n",
    "                num_outputs=RNN_HIDDEN_SIZE,\n",
    "                inputs=layer_1,\n",
    "                \n",
    "            )\n",
    "            \n",
    "        \n",
    "        split_inputs = tf.reshape(droped_input,shape=[1,BATCH_SIZE,num_features],name=\"reshape_l1\") # Each item in the batch is a time step, iterate through them\n",
    "        split_inputs = tf.unpack(split_inputs,axis=1,name=\"unpack_l1\")\n",
    "        states =[]\n",
    "        outputs =[]\n",
    "        with tf.variable_scope(\"rnn\",initializer=xavier_initializer(uniform=False)) as scope:\n",
    "            state = self.start_state\n",
    "            for i, inp in enumerate(split_inputs):\n",
    "                if i >0:\n",
    "                    scope.reuse_variables()\n",
    "                \n",
    "                output, state = self.gru_cell(inp, state)\n",
    "                states.append(state)\n",
    "                outputs.append(output)\n",
    "        self.end_state = states[-1]\n",
    "        outputs = tf.pack(outputs,axis=1) # Pack them back into a single tensor\n",
    "        outputs = tf.reshape(outputs,shape=[BATCH_SIZE,RNN_HIDDEN_SIZE])\n",
    "        self.logits = tf.contrib.layers.fully_connected(\n",
    "            num_outputs=num_classes,\n",
    "            inputs=outputs,\n",
    "            activation_fn=None\n",
    "        )\n",
    "\n",
    "            \n",
    "        with tf.variable_scope(\"loss\"):\n",
    "            self.penalties =    tf.reduce_sum([beta*tf.nn.l2_loss(var) for var in tf.trainable_variables()])\n",
    "\n",
    "            \n",
    "            self.losses = tf.nn.sparse_softmax_cross_entropy_with_logits(self.logits,self.target_data)\n",
    "            self.loss = tf.reduce_sum(self.losses + beta*self.penalties)\n",
    "        \n",
    "        with tf.name_scope(\"train_step\"):\n",
    "          opt = tf.train.AdamOptimizer(lr)\n",
    "          gvs = opt.compute_gradients(self.loss)\n",
    "          self.train_op = opt.apply_gradients(gvs, global_step=global_step)\n",
    "        \n",
    "        with tf.name_scope(\"predictions\"):\n",
    "            probs = tf.nn.softmax(self.logits)\n",
    "            self.predictions = tf.argmax(probs, 1)\n",
    "            correct_pred = tf.cast(tf.equal(self.predictions, tf.cast(self.target_data,tf.int64)),tf.float64)\n",
    "            self.accuracy = tf.reduce_mean(correct_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'module' object has no attribute 'rnn_cell'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-77-528856338431>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRNNModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-e322aca5289e>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mattn_cell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmakeGRUCells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-76-e322aca5289e>\u001b[0m in \u001b[0;36mmakeGRUCells\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mmakeGRUCells\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mbase_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGRUCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_units\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRNN_HIDDEN_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mlayered_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn_cell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbase_cell\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_LAYERS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mattn_cell\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAttentionCellWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayered_cell\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mATTN_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'module' object has no attribute 'rnn_cell'"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    model = RNNModel()\n",
    "    input_ = train[0]\n",
    "    target = train[1]\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.initialize_all_variables()\n",
    "        sess.run([init])\n",
    "        loss = 2000\n",
    "        \n",
    "        for e in range(NUM_EPOCHS):\n",
    "            state = sess.run(model.zero_state)\n",
    "            epoch_loss =0\n",
    "            for batch in range(0,NUM_TRAIN_BATCHES):\n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:input_[start:end],\n",
    "                    model.target_data:target[start:end],\n",
    "                    model.dropout_prob:0.5,\n",
    "                    model.start_state:state\n",
    "                            }\n",
    "                _,loss,acc,state = sess.run(\n",
    "                    [\n",
    "                        model.train_op,\n",
    "                        model.loss,\n",
    "                        model.accuracy,\n",
    "                        model.end_state\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                epoch_loss+=loss\n",
    "                \n",
    "            print('step - {0} loss - {1} acc - {2}'.format((e),epoch_loss,acc))\n",
    "        final_preds =np.array([])\n",
    "        for batch in range(0,NUM_VAL_BATCHES):\n",
    "                start = batch*BATCH_SIZE\n",
    "                end = start + BATCH_SIZE \n",
    "                feed = {\n",
    "                    model.input_data:val[0][start:end],\n",
    "                    model.target_data:val[1][start:end],\n",
    "                    model.dropout_prob:1,\n",
    "                    model.start_state:state\n",
    "                            }\n",
    "                acc,preds,state = sess.run(\n",
    "                    [\n",
    "                        model.accuracy,\n",
    "                        model.predictions,\n",
    "                        model.end_state\n",
    "                    ]\n",
    "                    ,feed_dict=feed\n",
    "                )\n",
    "                print(acc)\n",
    "                assert len(preds) == BATCH_SIZE\n",
    "                final_preds = np.concatenate((final_preds,preds),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## RNN Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result['rnn_pred'] = final_preds\n",
    "Result['mod_rnn_prod'] = list(map(lambda x: -1 if x <5 else 0 if x==5 else 1,final_preds))\n",
    "Result['rnn_ret'] = Result.mod_rnn_prod*Result['return']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(Result['multi_class'],Result['rnn_pred']))\n",
    "print(classification_report(Result['class'],Result['mod_rnn_prod']))\n",
    "print(confusion_matrix(Result['class'],Result['mod_rnn_prod']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "(96/(96+82) + 94/(77+94))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Res = (Result[-test_size:][['return','nn_ret','rnn_ret','pred_return']]).cumsum()\n",
    "Res[0] =0\n",
    "Res.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Res.columns =['Market Baseline','Simple Neural Newtwork','My Algo','Logistic Regression (simple ML)','Do Nothing(0)']\n",
    "Res.plot(figsize=(20,10),title=\"Performance of MarketVectors algo over 27 months compared with baselines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Res.columns\n",
    "Res.columns =['baseline','logistic_regression','feed_forward_net','rnn_net','do_nothing']\n",
    "Res.plot(figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.rnn_cell import BasicLSTMCell, GRUCell, MultiRNNCell, DropoutWrapper\n",
    "cell = tf.nn.rnn_cell.GRUCell(num_units=RNN_HIDDEN_SIZE)\n",
    "cell = MultiRNNCell(cells=[cell]*NUM_LAYERS,state_is_tuple=True)\n",
    "attn_cell =tf.contrib.rnn.AttentionCellWrapper(cell=cell,attn_length=ATTN_LENGTH,state_is_tuple=True)\n",
    "print(attn_cell.zero_state(batch_size=1,dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.start_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = ([1,2,3,4],())\n",
    "y = sum([1,2,3],())\n",
    "type(())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Labeled.hist(column='return',by='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result['class'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "g = sns.FacetGrid(Result, row=\"class\", col=\"rnn_pred\", margin_titles=True)\n",
    "g.map(sns.distplot, \"return\",);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result.hist(by=['class','nn_pred'],column='return',sharex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result['zreturn'] = zscore(Result['return'])\n",
    "Result['day'] = Result.index.dayofweek\n",
    "sns.lmplot(data=Result,y='zreturn',x='nn_prediction_conf',hue='day',col='class',row='nn_pred',fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Result.index.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Res.rnn_ret.mean()/Res.rnn_ret.std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
