{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、读取训练集和测试集，确定训练（预测）目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File train_modified_Gaoshou.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-b65bb366e088>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train_modified_Gaoshou.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#train = pd.read_csv('train_modified.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#test = pd.read_csv('test_modified.csv')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmean_squared_error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File train_modified_Gaoshou.csv does not exist"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_modified_Gaoshou.csv')\n",
    "#train = pd.read_csv('train_modified.csv')\n",
    "#test = pd.read_csv('test_modified.csv')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target='SalePrice'\n",
    "IDcol = 'ID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df_train_target = train['SalePrice']\n",
    "#df_train_data = train.drop(['SalePrice'],axis = 1)\n",
    "#print 'df_train_data shape is ', df_train_data.shape\n",
    "#print 'df_train_target shape is ', df_train_target.shape\n",
    "#print 'df_train_target shape is ', test.shape\n",
    "\n",
    "\n",
    "#df_train_target = train['SalePrice'].values\n",
    "df_train_target = train['SalePrice']\n",
    "df_train_data = train.drop(['SalePrice'],axis = 1).values\n",
    "#df_train_target[\"SalePrice\"] = np.log(df_train_target[\"SalePrice\"])\n",
    "\n",
    "\n",
    "\n",
    "print 'df_train_data shape is ', df_train_data.shape\n",
    "print 'df_train_target shape is ', df_train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 二、切分训练集，初步尝试各种回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 总得切分一下数据咯（训练集和测试集）\n",
    "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "# 各种模型来一圈\n",
    "\n",
    "print \"Ridge\"    \n",
    "z0=list()\n",
    "for train, test in cv:    \n",
    "    svc0 = linear_model.Ridge().fit(df_train_data[train], df_train_target[train])\n",
    "    \n",
    "    print(\"Ridge score on training set: \", rmse(svc0.predict(df_train_data[test]), df_train_target[test]))\n",
    "    z0.append(rmse(svc0.predict(df_train_data[test]), df_train_target[test]))\n",
    "    \n",
    "print \"Lasso\"  \n",
    "z1=list()\n",
    "for train, test in cv:    \n",
    "    svc1 = Lasso(alpha=0.00099, max_iter=50000).fit(df_train_data[train], df_train_target[train])\n",
    "    \n",
    "    print(\"Lasso score on training set: \", rmse(svc1.predict(df_train_data[test]), df_train_target[test]))\n",
    "    z1.append(rmse(svc1.predict(df_train_data[test]), df_train_target[test]))\n",
    "\n",
    "    \n",
    "    \n",
    "print \"GradientBoostingRegressor\"\n",
    "for train, test in cv:\n",
    "    \n",
    "    svc2 = GradientBoostingRegressor().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"GradientBoostingRegressor score on training set: \", rmse(svc2.predict(df_train_data[test]), df_train_target[test]))\n",
    "\n",
    "print \"ExtraTreesRegressor\"\n",
    "for train, test in cv:\n",
    "    \n",
    "    svc3 = ExtraTreesRegressor().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"ExtraTreesRegressor score on training set: \", rmse(svc3.predict(df_train_data[test]), df_train_target[test]))\n",
    "        \n",
    "    \n",
    "print \"随机森林回归/Random Forest(n_estimators = 100)\"    \n",
    "for train, test in cv:    \n",
    "    svc4 = RandomForestRegressor(n_estimators = 100).fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"RF score on training set: \", rmse(svc4.predict(df_train_data[test]), df_train_target[test]))\n",
    "\n",
    "print \"XGBRegressor\"    \n",
    "for train, test in cv:  \n",
    "    z5=list()\n",
    "    svc5 =  XGBRegressor(colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                 \n",
    "#                 reg_alpha=0.9,\n",
    "#                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42,\n",
    "                 silent=1).fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"XGBoost score on training set: \", rmse(svc5.predict(df_train_data[test]), df_train_target[test]))\n",
    "    z5.append(rmse(svc5.predict(df_train_data[test]), df_train_target[test]))\n",
    "    \n",
    "print \"随机森林回归/Random Forest(n_estimators = 100)\"    \n",
    "for train, test in cv:    \n",
    "    svc6 = BaggingRegressor().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"RF score on training set: \", rmse(svc6.predict(df_train_data[test]), df_train_target[test]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s015 = rmse((svc0.predict(df_train_data)+svc1.predict(df_train_data)+svc5.predict(df_train_data))/3, df_train_target)\n",
    "s01 = rmse((svc0.predict(df_train_data)+svc1.predict(df_train_data))/2, df_train_target)\n",
    "s05 = rmse((svc0.predict(df_train_data)+svc5.predict(df_train_data))/2, df_train_target)\n",
    "s15 = rmse((svc1.predict(df_train_data)+svc5.predict(df_train_data))/2, df_train_target)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print \"s015:%s,s01:%s,s15:%s,s05:%s, \" %(s015,s01,s15,s05)                                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将多个模型的结果融合，再跑一边xgboost\n",
    "svc0 = linear_model.Ridge().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc1 = Lasso(alpha=0.00099, max_iter=50000).fit(df_train_data, df_train_target)\n",
    "\n",
    "svc2 = GradientBoostingRegressor().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc3 = ExtraTreesRegressor().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc4 = RandomForestRegressor(n_estimators = 100).fit(df_train_data, df_train_target)\n",
    "\n",
    "svc5 =  XGBRegressor(max_depth=3, \n",
    "                    learning_rate=0.1, \n",
    "                    n_estimators=100, \n",
    "                    silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    subsample=1, \n",
    "                    colsample_bytree=1, \n",
    "#                    colsample_bylevel=1, \n",
    "#                    reg_alpha=0, \n",
    "#                    reg_lambda=1, \n",
    "#                    scale_pos_weight=1, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None).fit(df_train_data, df_train_target)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trai = pd.read_csv('train_modified_Gaoshou.csv')\n",
    "df1 = trai.drop(['SalePrice'],axis = 1)\n",
    "#df1 = pd.read_csv('test_modified.csv')\n",
    "\n",
    "df1_train = pd.DataFrame({\n",
    "                    'svc0':svc0.predict(df1),\n",
    "#                    'svc1':svc1.predict(df1),\n",
    "#                    'svc2':svc2.predict(df1),\n",
    "#                    'svc3':svc3.predict(df1),\n",
    "#                    'svc4':svc4.predict(df1),\n",
    "                    'svc5':svc5.predict(df1),\n",
    "                   })\n",
    "df1_train = df1_train.values\n",
    "\n",
    "svc_last =  BaggingRegressor().fit(df1_train[train], df_train_target[train])\n",
    "\n",
    "cv1 = cross_validation.ShuffleSplit(len(df_train_target), n_iter=10, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "for train, test in cv1:    \n",
    "    svc_last1 =  svc_last\n",
    "    print rmse(svc_last1.predict(df1_train[test]), df_train_target[test])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、将各个具有较好默认表现的回归模型，进行更为细致的grid调参和CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = df_train_data\n",
    "y = df_train_target\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "tuned_parameters = [{\n",
    "        'alpha':[1.0,2.0,3.0,4.0,5.0,6.0], \n",
    "       'tol':[1.0,2.0,3.0,4.0,5.0], \n",
    "#        'max_iter':[3,5,7],\n",
    "\n",
    "#        'n_estimators':[100,200,500,1000,2000], \n",
    "#        'subsample':[1,2,5], \n",
    "#        'colsample_bytree':[1,2,5], \n",
    "                    }]   \n",
    "    \n",
    "scores = ['r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO: 导入 'GridSearchCV' 和 'make_scorer'\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(df_train_data, df_train_target, test_size=0.3, random_state=0)\n",
    "\n",
    "# TODO：创建你希望调整的参数列表\n",
    "parameters = {'alpha':[1.0,2.0,3.0,4.0,5.0,6.0], \n",
    "       'tol':[1.0,2.0,3.0,4.0,5.0], }\n",
    "\n",
    "# TODO：初始化分类器\n",
    "clf = linear_model.Ridge()\n",
    "\n",
    "# TODO：用'make_scorer'创建一个f1评分函数\n",
    "f1_scorer = make_scorer(mean_squared_error)\n",
    "\n",
    "# TODO：在分类器上使用f1_scorer作为评分函数运行网格搜索\n",
    "grid_obj = GridSearchCV(clf,param_grid=parameters,cv=None,scoring=f1_scorer)\n",
    "\n",
    "# TODO: Fit the grid search object to the training data and find the optimal parameters\n",
    "# TODO：用训练集训练grid search object来寻找最佳参数\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "# Get the estimator\n",
    "# 得到预测的结果\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "# 输出经过调参之后的训练集和测试集的F1值\n",
    "print clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、通过学习曲线，验证模型是否过拟合或者欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "\n",
    "cv = cross_validation.ShuffleSplit(df_train_data.shape[0], n_iter=10,test_size=0.2, random_state=0)\n",
    "#estimator = linear_model.Ridge( copy_X=True, fit_intercept=False, max_iter=None,\n",
    "#      normalize=False, random_state=0, solver='auto',alpha=2.0,tol=1.0)\n",
    "                                  \n",
    "#plot_learning_curve(estimator, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "estimator0 = linear_model.Ridge()\n",
    "estimator1 = Lasso(alpha=0.00099, max_iter=50000)\n",
    "estimator2 = GradientBoostingRegressor()\n",
    "estimator3 = ExtraTreesRegressor()\n",
    "estimator4 = RandomForestRegressor()\n",
    "estimator5 = XGBRegressor()\n",
    "\n",
    "\n",
    "title = \"Learning Curves (Ridge)\"\n",
    "plot_learning_curve(estimator0, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
    "title = \"Learning Curves (Lasso)\"\n",
    "plot_learning_curve(estimator1, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "\n",
    "title = \"Learning Curves (XGBRegressor)\"\n",
    "plot_learning_curve(estimator5, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()  \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、用得到的参数，训练测试集，得到结果并保存"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df2 = pd.read_csv('test_modified.csv')\n",
    "\n",
    "\n",
    "df2_train = pd.DataFrame({\n",
    "                    'svc0':svc0.predict(df2),\n",
    "                    'svc1':svc1.predict(df2),\n",
    "                    'svc2':svc2.predict(df2),\n",
    "                    'svc3':svc3.predict(df2),\n",
    "                    'svc4':svc4.predict(df2),\n",
    "#                    'SalePrice':df_train_target,\n",
    "                   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "df2_train = df2_train.values\n",
    "\n",
    "result_last_var = svc_last.predict(df2_train)\n",
    "\n",
    "\n",
    "result_last = pd.DataFrame({'Id':df2['Id'],'SalePrice':result_last_var})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('test_modified_Gaoshou.csv')\n",
    "\n",
    "pre0 = svc0.predict(df2)\n",
    "pre1 = svc1.predict(df2)\n",
    "pre5 = svc5.predict(df2)\n",
    "\n",
    "predicts = (pre1+pre5)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#加第二层\n",
    "df1_train = pd.DataFrame({\n",
    "                    'svc0':svc0.predict(df2),\n",
    "#                    'svc1':svc1.predict(df2),\n",
    "#                    'svc2':svc2.predict(df1),\n",
    "#                    'svc3':svc3.predict(df1),\n",
    "#                    'svc4':svc4.predict(df1),\n",
    "                    'svc5':svc5.predict(df2),\n",
    "                   })\n",
    "#df1_train = df1_train.values\n",
    "\n",
    "predicts = svc_last.predict(df1_train)\n",
    "#predicts = np.exp(predict1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "predicts_end = np.exp(predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = pd.read_csv('test_modified.csv')\n",
    "result_last = pd.DataFrame({'Id':df3['Id'],'SalePrice':predicts_end})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_last.to_csv('12-11.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_last.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "result_last.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "aaaaaaa= pd.read_csv('output.csv')\n",
    "aaaaaaa.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.hist(result_last['SalePrice'], bins=12, normed=True, color=\"#FF0000\", alpha=.9)\n",
    "plt.hist(aaaaaaa['SalePrice'], bins=12, normed=True, color=\"#C1F320\", alpha=.9)\n",
    "plt.hist(np.exp(df_train_target), bins=12, normed=True,  alpha=.9)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#ensemble函数\n",
    "class Ensemble(object):\n",
    "    def __init__(self, n_folds, stacker, base_models):\n",
    "        self.n_folds = n_folds\n",
    "        self.stacker = stacker\n",
    "        self.base_models = base_models\n",
    "\n",
    "    def fit_predict(self, X, y, T):\n",
    "        X = np.array(X)\n",
    "        y = np.array(y)\n",
    "        T = np.array(T)\n",
    "\n",
    "        folds = list(KFold(len(y), n_folds=self.n_folds, shuffle=True, random_state=2016))\n",
    "\n",
    "        S_train = np.zeros((X.shape[0], len(self.base_models)))\n",
    "        S_test = np.zeros((T.shape[0], len(self.base_models)))\n",
    "\n",
    "        for i, clf in enumerate(self.base_models):\n",
    "            S_test_i = np.zeros((T.shape[0], len(folds)))\n",
    "\n",
    "            for j, (train_idx, test_idx) in enumerate(folds):\n",
    "                X_train = X[train_idx]\n",
    "                y_train = y[train_idx]\n",
    "                X_holdout = X[test_idx]\n",
    "                # y_holdout = y[test_idx]\n",
    "                clf.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_holdout)[:]\n",
    "                S_train[test_idx, i] = y_pred\n",
    "                S_test_i[:, j] = clf.predict(T)[:]\n",
    "\n",
    "            S_test[:, i] = S_test_i.mean(1)\n",
    "\n",
    "        self.stacker.fit(S_train, y)\n",
    "        y_pred = self.stacker.predict(S_test)[:]\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
