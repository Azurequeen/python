{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from sklearn import cross_validation, metrics\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn import linear_model\n",
    "from sklearn import cross_validation\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.learning_curve import learning_curve\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import explained_variance_score\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.pylab import rcParams\n",
    "rcParams['figure.figsize'] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、读取训练集和测试集，确定训练（预测）目标"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_modified.csv')\n",
    "#test = pd.read_csv('test_modified.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_train_data shape is  (1281, 340)\n",
      "df_train_target shape is  (1281,)\n"
     ]
    }
   ],
   "source": [
    "#df_train_target = train['SalePrice']\n",
    "#df_train_data = train.drop(['SalePrice'],axis = 1)\n",
    "#print 'df_train_data shape is ', df_train_data.shape\n",
    "#print 'df_train_target shape is ', df_train_target.shape\n",
    "#print 'df_train_target shape is ', test.shape\n",
    "\n",
    "\n",
    "df_train_target = train['SalePrice'].values\n",
    "df_train_data = train.drop(['SalePrice'],axis = 1).values\n",
    "print 'df_train_data shape is ', df_train_data.shape\n",
    "print 'df_train_target shape is ', df_train_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "target='SalePrice'\n",
    "IDcol = 'ID'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 二、切分训练集，初步尝试各种回归模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "岭回归\n",
      "train score: 0.944, test score: 0.902\n",
      "\n",
      "train score: 0.943, test score: 0.900\n",
      "\n",
      "train score: 0.942, test score: 0.904\n",
      "\n",
      "GradientBoostingRegressor\n",
      "train score: 0.963, test score: 0.869\n",
      "\n",
      "train score: 0.962, test score: 0.902\n",
      "\n",
      "train score: 0.964, test score: 0.905\n",
      "\n",
      "ExtraTreesRegressor\n",
      "train score: 1.000, test score: 0.795\n",
      "\n",
      "train score: 1.000, test score: 0.860\n",
      "\n",
      "train score: 1.000, test score: 0.779\n",
      "\n",
      "随机森林回归/Random Forest(n_estimators = 100)\n",
      "train score: 0.982, test score: 0.841\n",
      "\n",
      "train score: 0.979, test score: 0.868\n",
      "\n",
      "train score: 0.981, test score: 0.898\n",
      "\n",
      "XGBRegressor\n",
      "train score: 0.961, test score: 0.866\n",
      "\n",
      "train score: 0.956, test score: 0.905\n",
      "\n",
      "train score: 0.960, test score: 0.901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 总得切分一下数据咯（训练集和测试集）\n",
    "cv = cross_validation.ShuffleSplit(len(df_train_data), n_iter=3, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "# 各种模型来一圈\n",
    "\n",
    "print \"岭回归\"    \n",
    "for train, test in cv:    \n",
    "    svc = linear_model.Ridge().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
    "    \n",
    "print \"GradientBoostingRegressor\"\n",
    "for train, test in cv:\n",
    "    \n",
    "    svc = GradientBoostingRegressor().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
    "\n",
    "print \"ExtraTreesRegressor\"\n",
    "for train, test in cv:\n",
    "    \n",
    "    svc = ExtraTreesRegressor().fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
    "        \n",
    "    \n",
    "print \"随机森林回归/Random Forest(n_estimators = 100)\"    \n",
    "for train, test in cv:    \n",
    "    svc = RandomForestRegressor(n_estimators = 100).fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))\n",
    "\n",
    "print \"XGBRegressor\"    \n",
    "for train, test in cv:    \n",
    "    svc =  XGBRegressor(max_depth=3, \n",
    "                    learning_rate=0.1, \n",
    "                    n_estimators=100, \n",
    "                    silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    subsample=1, \n",
    "                    colsample_bytree=1, \n",
    "#                    colsample_bylevel=1, \n",
    "#                    reg_alpha=0, \n",
    "#                    reg_lambda=1, \n",
    "#                    scale_pos_weight=1, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None).fit(df_train_data[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc.score(df_train_data[train], df_train_target[train]), svc.score(df_train_data[test], df_train_target[test])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#将多个模型的结果融合，再跑一边xgboost\n",
    "svc0 = linear_model.Ridge().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc1 = GradientBoostingRegressor().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc2 = ExtraTreesRegressor().fit(df_train_data, df_train_target)\n",
    "\n",
    "svc3 = RandomForestRegressor(n_estimators = 100).fit(df_train_data, df_train_target)\n",
    "\n",
    "svc4 =  XGBRegressor(max_depth=3, \n",
    "                    learning_rate=0.1, \n",
    "                    n_estimators=100, \n",
    "                    silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    subsample=1, \n",
    "                    colsample_bytree=1, \n",
    "#                    colsample_bylevel=1, \n",
    "#                    reg_alpha=0, \n",
    "#                    reg_lambda=1, \n",
    "#                    scale_pos_weight=1, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None).fit(df_train_data, df_train_target)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trai = pd.read_csv('train_modified.csv')\n",
    "df1 = trai.drop(['SalePrice'],axis = 1)\n",
    "#df1 = pd.read_csv('test_modified.csv')\n",
    "\n",
    "df1_train = pd.DataFrame({\n",
    "                    'svc0':svc0.predict(df1),\n",
    "                    'svc1':svc1.predict(df1),\n",
    "                    'svc2':svc2.predict(df1),\n",
    "                    'svc3':svc3.predict(df1),\n",
    "                    'svc4':svc4.predict(df1),\n",
    "#                    'SalePrice':df_train_target,\n",
    "                   })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "indices are out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-792fb5e613a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcv1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0msvc_last\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvc_last\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf1_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_train_target\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n\u001b[1;32m     25\u001b[0m         svc_last.score(df1_train[train], df_train_target[train]), svc_last.score(df1_train[test], df_train_target[test])))\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1989\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1990\u001b[0m             \u001b[0;31m# either boolean or fancy integer index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1991\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1992\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1993\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2034\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2035\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2036\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2037\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2038\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, convert, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m         new_data = self._data.take(indices,\n\u001b[1;32m   1632\u001b[0m                                    \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1633\u001b[0;31m                                    convert=True, verify=True)\n\u001b[0m\u001b[1;32m   1634\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   3700\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3702\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_convert_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3704\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/pandas/core/indexing.pyc\u001b[0m in \u001b[0;36mmaybe_convert_indices\u001b[0;34m(indices, n)\u001b[0m\n\u001b[1;32m   1854\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindices\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1856\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"indices are out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1857\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: indices are out-of-bounds"
     ]
    }
   ],
   "source": [
    "cv1 = cross_validation.ShuffleSplit(len(df_train_target), n_iter=3, test_size=0.2,\n",
    "    random_state=0)\n",
    "\n",
    "svc_last =  XGBRegressor(max_depth=3, \n",
    "                    learning_rate=0.1, \n",
    "                    n_estimators=100, \n",
    "                    silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    subsample=1, \n",
    "                    colsample_bytree=1, \n",
    "#                    colsample_bylevel=1, \n",
    "#                    reg_alpha=0, \n",
    "#                    reg_lambda=1, \n",
    "#                    scale_pos_weight=1, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None)\n",
    "\n",
    "for train, test in cv1:    \n",
    "    svc_last = svc_last.fit(df1_train[train], df_train_target[train])\n",
    "    print(\"train score: {0:.3f}, test score: {1:.3f}\\n\".format(\n",
    "        svc_last.score(df1_train[train], df_train_target[train]), svc_last.score(df1_train[test], df_train_target[test])))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 三、将各个具有较好默认表现的回归模型，进行更为细致的grid调参和CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X = df_train_data\n",
    "y = df_train_target\n",
    "\n",
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(\n",
    "    X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "tuned_parameters = [{\n",
    "        'max_depth':[4,6], \n",
    "#        'learning_rate':[0.05,0.1,0.2,0.4], \n",
    "#        'n_estimators':[100,200,500,1000,2000], \n",
    "#        'subsample':[1,2,5], \n",
    "#        'colsample_bytree':[1,2,5], \n",
    "                    }]   \n",
    "    \n",
    "scores = ['r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "for score in scores:\n",
    "    \n",
    "    print score\n",
    "    \n",
    "    clf = GridSearchCV(XGBRegressor( silent=False, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, \n",
    "                    learning_rate = 0.1,\n",
    "#                    max_depth = 4,\n",
    "                    \n",
    "#                    n_estimators = 200 ,              \n",
    "                                  \n",
    "                                    \n",
    "#                   gamma=0, \n",
    "#                    min_child_weight=1, \n",
    "#                    max_delta_step=0, \n",
    "#                    base_score=0.5, \n",
    "#                    seed=0, \n",
    "#                    missing=None\n",
    "                                   ), tuned_parameters, cv=5, scoring=score)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"别！喝！咖！啡！了！最佳参数找到了亲！！：\")\n",
    "    print \"\"\n",
    "    #best_estimator_ returns the best estimator chosen by the search\n",
    "    print(clf.best_estimator_)\n",
    "    print \"\"\n",
    "    print(\"得分分别是:\")\n",
    "    print \"\"\n",
    "    #grid_scores_的返回值:\n",
    "    #    * a dict of parameter settings\n",
    "    #    * the mean score over the cross-validation folds \n",
    "    #    * the list of scores for each fold\n",
    "    for params, mean_score, scores in clf.grid_scores_:\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean_score, scores.std() / 2, params))\n",
    "        print \"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 四、通过学习曲线，验证模型是否过拟合或者欠拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "title = \"Learning Curves (XGBRegressor)\"\n",
    "cv = cross_validation.ShuffleSplit(df_train_data.shape[0], n_iter=10,test_size=0.2, random_state=0)\n",
    "estimator = XGBRegressor( silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, \n",
    "                    \n",
    "                    max_depth = 4,\n",
    "                    learning_rate = 0.1,\n",
    "                    n_estimators = 200 ,              \n",
    "                                  \n",
    "                                    \n",
    "                   gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None)\n",
    "plot_learning_curve(estimator, title, X, y, (0.0, 1.01), cv=cv, n_jobs=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、用得到的参数，训练测试集，得到结果并保存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('test_modified.csv')\n",
    "svc = XGBRegressor(silent=True, \n",
    "                    objective='reg:linear', \n",
    "                    nthread=-1, \n",
    "                    \n",
    "                    max_depth = 4,\n",
    "                    learning_rate = 0.1,\n",
    "                    n_estimators = 200 ,              \n",
    "                                  \n",
    "                                    \n",
    "                   gamma=0, \n",
    "                    min_child_weight=1, \n",
    "                    max_delta_step=0, \n",
    "                    base_score=0.5, \n",
    "                    seed=0, \n",
    "                    missing=None)\n",
    "\n",
    "svc = svc.fit(df_train_data,df_train_target)\n",
    "tt = svc.predict(test)\n",
    "ttt = pd.DataFrame({'Id':test['Id'],'SalePrice':tt})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ttt.to_csv('12-8.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttt.describe()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
