{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/sklearn/learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd #数据分析\n",
    "import numpy as np #科学计算\n",
    "\n",
    "import visuals as rs\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import cross_validation\n",
    "from IPython.display import display # 使得我们可以对DataFrame使用display()函数\n",
    "from sklearn import cluster\n",
    "from sklearn import decomposition\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、了解目标数据集和学习目标，合并train和test便于清洗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1460, 81), (1459, 80))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#查看数据集大小\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "train.shape,test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#确定模型要学习的目标，\n",
    "target='SalePrice'\n",
    "IDcol = 'Id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2919, 82)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#添加一列，合成一个总的data，便于清洗\n",
    "train['source']= 'train'\n",
    "test['source'] = 'test'\n",
    "df=pd.concat([train, test],ignore_index=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#建立一个空DF，逐项处理特征后填入\n",
    "all_df = pd.DataFrame(index = df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、处理缺失值<br><br>\n",
    "- 1、查看缺失值及缺失值的分布情况\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#通过neighborhood与LotFrontage，新建特征，代替LotFrontage缺失项（此特征重要，且缺失较多，明显故意的嘛）\n",
    "\n",
    "lot_frontage_by_neighborhood = train[\"LotFrontage\"].groupby(train[\"Neighborhood\"])\n",
    "\n",
    "all_df[\"LotFrontage\"] = df[\"LotFrontage\"]   \n",
    "for key, group in lot_frontage_by_neighborhood:\n",
    "    idx = (df[\"Neighborhood\"] == key) & (df[\"LotFrontage\"].isnull())\n",
    "    all_df.loc[idx, \"LotFrontage\"] = group.median() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#根据说明txt，补齐部分特征的有意义的Nan\n",
    "\n",
    "\n",
    "all_df[\"LotArea\"] = df[\"LotArea\"]\n",
    "\n",
    "all_df[\"MasVnrArea\"] = df[\"MasVnrArea\"]\n",
    "all_df[\"MasVnrArea\"].fillna(0, inplace=True)\n",
    "   \n",
    "all_df[\"BsmtFinSF1\"] = df[\"BsmtFinSF1\"]\n",
    "all_df[\"BsmtFinSF1\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"BsmtFinSF2\"] = df[\"BsmtFinSF2\"]\n",
    "all_df[\"BsmtFinSF2\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"BsmtUnfSF\"] = df[\"BsmtUnfSF\"]\n",
    "all_df[\"BsmtUnfSF\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"TotalBsmtSF\"] = df[\"TotalBsmtSF\"]\n",
    "all_df[\"TotalBsmtSF\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"1stFlrSF\"] = df[\"1stFlrSF\"]\n",
    "all_df[\"2ndFlrSF\"] = df[\"2ndFlrSF\"]\n",
    "all_df[\"GrLivArea\"] = df[\"GrLivArea\"]\n",
    "\n",
    "all_df[\"GarageArea\"] = df[\"GarageArea\"]\n",
    "all_df[\"GarageArea\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"WoodDeckSF\"] = df[\"WoodDeckSF\"]\n",
    "all_df[\"OpenPorchSF\"] = df[\"OpenPorchSF\"]\n",
    "all_df[\"EnclosedPorch\"] = df[\"EnclosedPorch\"]\n",
    "all_df[\"3SsnPorch\"] = df[\"3SsnPorch\"]\n",
    "all_df[\"ScreenPorch\"] = df[\"ScreenPorch\"]\n",
    "\n",
    "all_df[\"BsmtFullBath\"] = df[\"BsmtFullBath\"]\n",
    "all_df[\"BsmtFullBath\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"BsmtHalfBath\"] = df[\"BsmtHalfBath\"]\n",
    "all_df[\"BsmtHalfBath\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"FullBath\"] = df[\"FullBath\"] \n",
    "all_df[\"HalfBath\"] = df[\"HalfBath\"] \n",
    "all_df[\"BedroomAbvGr\"] = df[\"BedroomAbvGr\"] \n",
    "all_df[\"KitchenAbvGr\"] = df[\"KitchenAbvGr\"] \n",
    "all_df[\"TotRmsAbvGrd\"] = df[\"TotRmsAbvGrd\"] \n",
    "all_df[\"Fireplaces\"] = df[\"Fireplaces\"] \n",
    "\n",
    "all_df[\"GarageCars\"] = df[\"GarageCars\"]\n",
    "all_df[\"GarageCars\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"CentralAir\"] = (df[\"CentralAir\"] == \"Y\") * 1.0\n",
    "   \n",
    "all_df[\"OverallQual\"] = df[\"OverallQual\"]\n",
    "all_df[\"OverallCond\"] = df[\"OverallCond\"]\n",
    "\n",
    "all_df[\"YearBuilt\"] = df[\"YearBuilt\"]\n",
    "all_df[\"YearRemodAdd\"] = df[\"YearRemodAdd\"]\n",
    "\n",
    "all_df[\"GarageYrBlt\"] = df[\"GarageYrBlt\"]\n",
    "all_df[\"GarageYrBlt\"].fillna(0.0, inplace=True)\n",
    "\n",
    "all_df[\"MoSold\"] = df[\"MoSold\"]\n",
    "all_df[\"YrSold\"] = df[\"YrSold\"]\n",
    "    \n",
    "all_df[\"LowQualFinSF\"] = df[\"LowQualFinSF\"]\n",
    "all_df[\"MiscVal\"] = df[\"MiscVal\"]\n",
    "\n",
    "\n",
    "\n",
    "all_df[\"PoolArea\"] = df[\"PoolArea\"]\n",
    "all_df[\"PoolArea\"].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#对于有说明的关于质量等的字符型特征，替换成int的数字\n",
    "qual_dict = {None: 0, \"Po\": 1, \"Fa\": 2, \"TA\": 3, \"Gd\": 4, \"Ex\": 5}\n",
    "all_df[\"PoolQC\"] = df[\"PoolQC\"].map(qual_dict).astype(int)\n",
    "all_df[\"ExterQual\"] = df[\"ExterQual\"].map(qual_dict).astype(int)\n",
    "all_df[\"ExterCond\"] = df[\"ExterCond\"].map(qual_dict).astype(int)\n",
    "all_df[\"BsmtQual\"] = df[\"BsmtQual\"].map(qual_dict).astype(int)\n",
    "all_df[\"BsmtCond\"] = df[\"BsmtCond\"].map(qual_dict).astype(int)\n",
    "all_df[\"HeatingQC\"] = df[\"HeatingQC\"].map(qual_dict).astype(int)\n",
    "all_df[\"KitchenQual\"] = df[\"KitchenQual\"].map(qual_dict).astype(int)\n",
    "all_df[\"FireplaceQu\"] = df[\"FireplaceQu\"].map(qual_dict).astype(int)\n",
    "all_df[\"GarageQual\"] = df[\"GarageQual\"].map(qual_dict).astype(int)\n",
    "all_df[\"GarageCond\"] = df[\"GarageCond\"].map(qual_dict).astype(int)\n",
    "\n",
    "all_df[\"BsmtExposure\"] = df[\"BsmtExposure\"].map(\n",
    "{None: 0, \"No\": 1, \"Mn\": 2, \"Av\": 3, \"Gd\": 4}).astype(int)\n",
    "\n",
    "bsmt_fin_dict = {None: 0, \"Unf\": 1, \"LwQ\": 2, \"Rec\": 3, \"BLQ\": 4, \"ALQ\": 5, \"GLQ\": 6}\n",
    "all_df[\"BsmtFinType1\"] = df[\"BsmtFinType1\"].map(bsmt_fin_dict).astype(int)\n",
    "all_df[\"BsmtFinType2\"] = df[\"BsmtFinType2\"].map(bsmt_fin_dict).astype(int)\n",
    "\n",
    "all_df[\"Functional\"] = df[\"Functional\"].map(\n",
    "{None: 0, \"Sal\": 1, \"Sev\": 2, \"Maj2\": 3, \"Maj1\": 4, \n",
    " \"Mod\": 5, \"Min2\": 6, \"Min1\": 7, \"Typ\": 8}).astype(int)\n",
    "\n",
    "all_df[\"GarageFinish\"] = df[\"GarageFinish\"].map(\n",
    "{None: 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3}).astype(int)\n",
    "\n",
    "all_df[\"Fence\"] = df[\"Fence\"].map(\n",
    "{None: 0, \"MnWw\": 1, \"GdWo\": 2, \"MnPrv\": 3, \"GdPrv\": 4}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#对于标签进行自动编码\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "\n",
    "def factorize(df, factor_df, column, fill_na=None):\n",
    "    factor_df[column] = df[column]\n",
    "    if fill_na is not None:\n",
    "        factor_df[column].fillna(fill_na, inplace=True)\n",
    "    le.fit(factor_df[column].unique())\n",
    "    factor_df[column] = le.transform(factor_df[column])\n",
    "    return factor_df\n",
    "\n",
    "all_df = factorize(df, all_df, \"MSSubClass\")\n",
    "all_df = factorize(df, all_df, \"MSZoning\", \"RL\")\n",
    "all_df = factorize(df, all_df, \"LotConfig\")\n",
    "all_df = factorize(df, all_df, \"Neighborhood\")\n",
    "all_df = factorize(df, all_df, \"Condition1\")\n",
    "all_df = factorize(df, all_df, \"BldgType\")\n",
    "all_df = factorize(df, all_df, \"HouseStyle\")\n",
    "all_df = factorize(df, all_df, \"RoofStyle\")\n",
    "all_df = factorize(df, all_df, \"Exterior1st\", \"Other\")\n",
    "all_df = factorize(df, all_df, \"Exterior2nd\", \"Other\")\n",
    "all_df = factorize(df, all_df, \"MasVnrType\", \"None\")\n",
    "all_df = factorize(df, all_df, \"Foundation\")\n",
    "all_df = factorize(df, all_df, \"SaleType\", \"Oth\")\n",
    "all_df = factorize(df, all_df, \"SaleCondition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#对于极少出现的特征值，单独给予1个特征表示\n",
    "\n",
    "\n",
    "\n",
    "# IR2 and IR3 don't appear that often, so just make a distinction\n",
    "# between regular and irregular.\n",
    "all_df[\"IsRegularLotShape\"] = (df[\"LotShape\"] == \"Reg\") * 1\n",
    "\n",
    "# Most properties are level; bin the other possibilities together\n",
    "# as \"not level\".\n",
    "all_df[\"IsLandLevel\"] = (df[\"LandContour\"] == \"Lvl\") * 1\n",
    "\n",
    "# Most land slopes are gentle; treat the others as \"not gentle\".\n",
    "all_df[\"IsLandSlopeGentle\"] = (df[\"LandSlope\"] == \"Gtl\") * 1\n",
    "\n",
    "# Most properties use standard circuit breakers.\n",
    "all_df[\"IsElectricalSBrkr\"] = (df[\"Electrical\"] == \"SBrkr\") * 1\n",
    "\n",
    "# About 2/3rd have an attached garage.\n",
    "all_df[\"IsGarageDetached\"] = (df[\"GarageType\"] == \"Detchd\") * 1\n",
    "\n",
    "# Most have a paved drive. Treat dirt/gravel and partial pavement\n",
    "# as \"not paved\".\n",
    "all_df[\"IsPavedDrive\"] = (df[\"PavedDrive\"] == \"Y\") * 1\n",
    "\n",
    "# The only interesting \"misc. feature\" is the presence of a shed.\n",
    "all_df[\"HasShed\"] = (df[\"MiscFeature\"] == \"Shed\") * 1.  \n",
    "\n",
    "# If YearRemodAdd != YearBuilt, then a remodeling took place at some point.\n",
    "all_df[\"Remodeled\"] = (all_df[\"YearRemodAdd\"] != all_df[\"YearBuilt\"]) * 1\n",
    "\n",
    "# Did a remodeling happen in the year the house was sold?\n",
    "all_df[\"RecentRemodel\"] = (all_df[\"YearRemodAdd\"] == all_df[\"YrSold\"]) * 1\n",
    "\n",
    "# Was this house sold in the year it was built?\n",
    "all_df[\"VeryNewHouse\"] = (all_df[\"YearBuilt\"] == all_df[\"YrSold\"]) * 1\n",
    "\n",
    "all_df[\"Has2ndFloor\"] = (all_df[\"2ndFlrSF\"] == 0) * 1\n",
    "all_df[\"HasMasVnr\"] = (all_df[\"MasVnrArea\"] == 0) * 1\n",
    "all_df[\"HasWoodDeck\"] = (all_df[\"WoodDeckSF\"] == 0) * 1\n",
    "all_df[\"HasOpenPorch\"] = (all_df[\"OpenPorchSF\"] == 0) * 1\n",
    "all_df[\"HasEnclosedPorch\"] = (all_df[\"EnclosedPorch\"] == 0) * 1\n",
    "all_df[\"Has3SsnPorch\"] = (all_df[\"3SsnPorch\"] == 0) * 1\n",
    "all_df[\"HasScreenPorch\"] = (all_df[\"ScreenPorch\"] == 0) * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#加过这些值，但是得分低了些\n",
    "\n",
    "# all_df[\"HasBasement\"] = df[\"BsmtQual\"].isnull() * 1\n",
    "# all_df[\"HasGarage\"] = df[\"GarageQual\"].isnull() * 1\n",
    "# all_df[\"HasFireplace\"] = df[\"FireplaceQu\"].isnull() * 1\n",
    "# all_df[\"HasFence\"] = df[\"Fence\"].isnull() * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#将一些int进行离散化\n",
    "\n",
    "# Months with the largest number of deals may be significant.\n",
    "all_df[\"HighSeason\"] = df[\"MoSold\"].replace( \n",
    "{1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 1, 8: 0, 9: 0, 10: 0, 11: 0, 12: 0})\n",
    "\n",
    "all_df[\"NewerDwelling\"] = df[\"MSSubClass\"].replace(\n",
    "{20: 1, 30: 0, 40: 0, 45: 0,50: 0, 60: 1, 70: 0, 75: 0, 80: 0, 85: 0,\n",
    " 90: 0, 120: 1, 150: 0, 160: 0, 180: 0, 190: 0})   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#新建一些特征\n",
    "all_df.loc[df.Neighborhood == 'NridgHt', \"Neighborhood_Good\"] = 1\n",
    "all_df.loc[df.Neighborhood == 'Crawfor', \"Neighborhood_Good\"] = 1\n",
    "all_df.loc[df.Neighborhood == 'StoneBr', \"Neighborhood_Good\"] = 1\n",
    "all_df.loc[df.Neighborhood == 'Somerst', \"Neighborhood_Good\"] = 1\n",
    "all_df.loc[df.Neighborhood == 'NoRidge', \"Neighborhood_Good\"] = 1\n",
    "all_df[\"Neighborhood_Good\"].fillna(0, inplace=True)\n",
    "\n",
    "all_df[\"SaleCondition_PriceDown\"] = df.SaleCondition.replace(\n",
    "{'Abnorml': 1, 'Alloca': 1, 'AdjLand': 1, 'Family': 1, 'Normal': 0, 'Partial': 0})\n",
    "\n",
    "# House completed before sale or not\n",
    "all_df[\"BoughtOffPlan\"] = df.SaleCondition.replace(\n",
    "{\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})\n",
    "\n",
    "all_df[\"BadHeating\"] = df.HeatingQC.replace(\n",
    "{'Ex': 0, 'Gd': 0, 'TA': 0, 'Fa': 1, 'Po': 1})\n",
    "\n",
    "area_cols = ['LotFrontage', 'LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n",
    " 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', \n",
    " 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'LowQualFinSF', 'PoolArea' ]\n",
    "all_df[\"TotalArea\"] = all_df[area_cols].sum(axis=1)\n",
    "\n",
    "all_df[\"TotalArea1st2nd\"] = all_df[\"1stFlrSF\"] + all_df[\"2ndFlrSF\"]\n",
    "\n",
    "all_df[\"Age\"] = 2010 - all_df[\"YearBuilt\"]\n",
    "all_df[\"TimeSinceSold\"] = 2010 - all_df[\"YrSold\"]\n",
    "\n",
    "all_df[\"SeasonSold\"] = all_df[\"MoSold\"].map({12:0, 1:0, 2:0, 3:1, 4:1, 5:1, \n",
    "  6:2, 7:2, 8:2, 9:3, 10:3, 11:3}).astype(int)\n",
    "\n",
    "all_df[\"YearsSinceRemodel\"] = all_df[\"YrSold\"] - all_df[\"YearRemodAdd\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#简化一些关于质量的特征\n",
    "\n",
    "# Simplifications of existing features into bad/average/good.\n",
    "all_df[\"SimplOverallQual\"] = all_df.OverallQual.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\n",
    "all_df[\"SimplOverallCond\"] = all_df.OverallCond.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2, 7 : 3, 8 : 3, 9 : 3, 10 : 3})\n",
    "all_df[\"SimplPoolQC\"] = all_df.PoolQC.replace(\n",
    "{1 : 1, 2 : 1, 3 : 2, 4 : 2})\n",
    "all_df[\"SimplGarageCond\"] = all_df.GarageCond.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplGarageQual\"] = all_df.GarageQual.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplFireplaceQu\"] = all_df.FireplaceQu.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplFireplaceQu\"] = all_df.FireplaceQu.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplFunctional\"] = all_df.Functional.replace(\n",
    "{1 : 1, 2 : 1, 3 : 2, 4 : 2, 5 : 3, 6 : 3, 7 : 3, 8 : 4})\n",
    "all_df[\"SimplKitchenQual\"] = all_df.KitchenQual.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplHeatingQC\"] = all_df.HeatingQC.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplBsmtFinType1\"] = all_df.BsmtFinType1.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\n",
    "all_df[\"SimplBsmtFinType2\"] = all_df.BsmtFinType2.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2, 6 : 2})\n",
    "all_df[\"SimplBsmtCond\"] = all_df.BsmtCond.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplBsmtQual\"] = all_df.BsmtQual.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplExterCond\"] = all_df.ExterCond.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})\n",
    "all_df[\"SimplExterQual\"] = all_df.ExterQual.replace(\n",
    "{1 : 1, 2 : 1, 3 : 1, 4 : 2, 5 : 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#处理‘neighborhood’特征（有点乱），根据groupby以后的均值来离散化\n",
    "# Bin by neighborhood (a little arbitrarily). Values were computed by: \n",
    "# train_df[\"SalePrice\"].groupby(train_df[\"Neighborhood\"]).median().sort_values()\n",
    "neighborhood_map = {\n",
    "        \"MeadowV\" : 0,  #  88000\n",
    "        \"IDOTRR\" : 1,   # 103000\n",
    "        \"BrDale\" : 1,   # 106000\n",
    "        \"OldTown\" : 1,  # 119000\n",
    "        \"Edwards\" : 1,  # 119500\n",
    "        \"BrkSide\" : 1,  # 124300\n",
    "        \"Sawyer\" : 1,   # 135000\n",
    "        \"Blueste\" : 1,  # 137500\n",
    "        \"SWISU\" : 2,    # 139500\n",
    "        \"NAmes\" : 2,    # 140000\n",
    "        \"NPkVill\" : 2,  # 146000\n",
    "        \"Mitchel\" : 2,  # 153500\n",
    "        \"SawyerW\" : 2,  # 179900\n",
    "        \"Gilbert\" : 2,  # 181000\n",
    "        \"NWAmes\" : 2,   # 182900\n",
    "        \"Blmngtn\" : 2,  # 191000\n",
    "        \"CollgCr\" : 2,  # 197200\n",
    "        \"ClearCr\" : 3,  # 200250\n",
    "        \"Crawfor\" : 3,  # 200624\n",
    "        \"Veenker\" : 3,  # 218000\n",
    "        \"Somerst\" : 3,  # 225500\n",
    "        \"Timber\" : 3,   # 228475\n",
    "        \"StoneBr\" : 4,  # 278000\n",
    "        \"NoRidge\" : 4,  # 290000\n",
    "        \"NridgHt\" : 4,  # 315000\n",
    "    }\n",
    "all_df[\"NeighborhoodBin\"] = df[\"Neighborhood\"].map(neighborhood_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Copy NeighborhoodBin into a temporary DataFrame because we want to use the\n",
    "# unscaled version later on (to one-hot encode it). \n",
    "neighborhood_bin_train = pd.DataFrame(index = df.index)\n",
    "neighborhood_bin_train[\"NeighborhoodBin\"] = all_df[\"NeighborhoodBin\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#讲所有int和float，skew一下，使得整齐\n",
    "\n",
    "numeric_features = all_df.dtypes[all_df.dtypes != \"object\"].index\n",
    "\n",
    "# Transform the skewed numeric features by taking log(feature + 1).\n",
    "# This will make the features more normal.\n",
    "from scipy.stats import skew\n",
    "\n",
    "skewed = all_df[numeric_features].apply(lambda x: skew(x.dropna().astype(float)))\n",
    "skewed = skewed[skewed > 0.75]\n",
    "skewed = skewed.index\n",
    "\n",
    "\n",
    "all_df[skewed] = np.log1p(all_df[skewed])\n",
    "\n",
    "\n",
    "# Additional processing: scale the data.   \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(all_df[numeric_features])\n",
    "\n",
    "scaled = scaler.transform(all_df[numeric_features])\n",
    "for i, col in enumerate(numeric_features):\n",
    "    all_df[col] = scaled[:, i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共0个特征有缺失值\n"
     ]
    }
   ],
   "source": [
    "#查看缺省值\n",
    "data_miss = all_df.isnull().apply(lambda x: x.sum(), axis=0)\n",
    "\n",
    "data_miss = pd.DataFrame({'Index':data_miss.index,'Values':data_miss.values})\n",
    "var=list()\n",
    "for i,j in data_miss.iterrows():\n",
    "    if data_miss.iloc[i,1] > 0:\n",
    "        var.append(data_miss.iloc[i,0])\n",
    "\n",
    "print '共%s个特征有缺失值' % len(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#讲所有字符型的进行OneHot（是否太繁琐，可以直接onehot一下）\n",
    "\n",
    "# Convert categorical features using one-hot encoding.\n",
    "def onehot(onehot_df, df, column_name, fill_na, drop_name):\n",
    "    onehot_df[column_name] = df[column_name]\n",
    "    if fill_na is not None:\n",
    "        onehot_df[column_name].fillna(fill_na, inplace=True)\n",
    "\n",
    "    dummies = pd.get_dummies(onehot_df[column_name], prefix=\"_\" + column_name)\n",
    "    \n",
    "    # Dropping one of the columns actually made the results slightly worse.\n",
    "    # if drop_name is not None:\n",
    "    #     dummies.drop([\"_\" + column_name + \"_\" + drop_name], axis=1, inplace=True)\n",
    "\n",
    "    onehot_df = onehot_df.join(dummies)\n",
    "    onehot_df = onehot_df.drop([column_name], axis=1)\n",
    "    return onehot_df\n",
    "\n",
    "def munge_onehot(df):\n",
    "    onehot_df = pd.DataFrame(index = df.index)\n",
    "\n",
    "    onehot_df = onehot(onehot_df, df, \"MSSubClass\", None, \"40\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MSZoning\", \"RL\", \"RH\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LotConfig\", None, \"FR3\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Neighborhood\", None, \"OldTown\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Condition1\", None, \"RRNe\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BldgType\", None, \"2fmCon\")\n",
    "    onehot_df = onehot(onehot_df, df, \"HouseStyle\", None, \"1.5Unf\")\n",
    "    onehot_df = onehot(onehot_df, df, \"RoofStyle\", None, \"Shed\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Exterior1st\", \"VinylSd\", \"CBlock\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Exterior2nd\", \"VinylSd\", \"CBlock\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Foundation\", None, \"Wood\")\n",
    "    onehot_df = onehot(onehot_df, df, \"SaleType\", \"WD\", \"Oth\")\n",
    "    onehot_df = onehot(onehot_df, df, \"SaleCondition\", \"Normal\", \"AdjLand\")\n",
    "\n",
    "    # Fill in missing MasVnrType for rows that do have a MasVnrArea.\n",
    "    temp_df = df[[\"MasVnrType\", \"MasVnrArea\"]].copy()\n",
    "    idx = (df[\"MasVnrArea\"] != 0) & ((df[\"MasVnrType\"] == \"None\") | (df[\"MasVnrType\"].isnull()))\n",
    "    temp_df.loc[idx, \"MasVnrType\"] = \"BrkFace\"\n",
    "    onehot_df = onehot(onehot_df, temp_df, \"MasVnrType\", \"None\", \"BrkCmn\")\n",
    "\n",
    "    # Also add the booleans from calc_df as dummy variables.\n",
    "    onehot_df = onehot(onehot_df, df, \"LotShape\", None, \"IR3\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LandContour\", None, \"Low\")\n",
    "    onehot_df = onehot(onehot_df, df, \"LandSlope\", None, \"Sev\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Electrical\", \"SBrkr\", \"FuseP\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageType\", \"None\", \"CarPort\")\n",
    "    onehot_df = onehot(onehot_df, df, \"PavedDrive\", None, \"P\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MiscFeature\", \"None\", \"Othr\")\n",
    "\n",
    "    # Features we can probably ignore (but want to include anyway to see\n",
    "    # if they make any positive difference).\n",
    "    # Definitely ignoring Utilities: all records are \"AllPub\", except for\n",
    "    # one \"NoSeWa\" in the train set and 2 NA in the test set.\n",
    "    onehot_df = onehot(onehot_df, df, \"Street\", None, \"Grvl\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Alley\", \"None\", \"Grvl\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Condition2\", None, \"PosA\")\n",
    "    onehot_df = onehot(onehot_df, df, \"RoofMatl\", None, \"WdShake\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Heating\", None, \"Wall\")\n",
    "\n",
    "    # I have these as numerical variables too.\n",
    "    onehot_df = onehot(onehot_df, df, \"ExterQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"ExterCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"HeatingQC\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"KitchenQual\", \"TA\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"FireplaceQu\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageQual\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageCond\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"PoolQC\", \"None\", \"Ex\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtExposure\", \"None\", \"Gd\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtFinType1\", \"None\", \"GLQ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"BsmtFinType2\", \"None\", \"GLQ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Functional\", \"Typ\", \"Typ\")\n",
    "    onehot_df = onehot(onehot_df, df, \"GarageFinish\", \"None\", \"Fin\")\n",
    "    onehot_df = onehot(onehot_df, df, \"Fence\", \"None\", \"MnPrv\")\n",
    "    onehot_df = onehot(onehot_df, df, \"MoSold\", None, None)\n",
    "    \n",
    "    # Divide up the years between 1871 and 2010 in slices of 20 years.\n",
    "    year_map = pd.concat(pd.Series(\"YearBin\" + str(i+1), index=range(1871+i*20,1891+i*20)) for i in range(0, 7))\n",
    "\n",
    "    yearbin_df = pd.DataFrame(index = df.index)\n",
    "    yearbin_df[\"GarageYrBltBin\"] = df.GarageYrBlt.map(year_map)\n",
    "    yearbin_df[\"GarageYrBltBin\"].fillna(\"NoGarage\", inplace=True)\n",
    "\n",
    "    yearbin_df[\"YearBuiltBin\"] = df.YearBuilt.map(year_map)\n",
    "    yearbin_df[\"YearRemodAddBin\"] = df.YearRemodAdd.map(year_map)\n",
    "    \n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"GarageYrBltBin\", None, None)\n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"YearBuiltBin\", None, None)\n",
    "    onehot_df = onehot(onehot_df, yearbin_df, \"YearRemodAddBin\", None, None)\n",
    "\n",
    "    return onehot_df\n",
    "\n",
    "\n",
    "# Add the one-hot encoded categorical features.\n",
    "onehot_df = munge_onehot(df)\n",
    "onehot_df = onehot(onehot_df, neighborhood_bin_train, \"NeighborhoodBin\", None, None)\n",
    "all_df_munged = all_df.join(onehot_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共0个特征有缺失值\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data_miss = all_df_munged.isnull().apply(lambda x: x.sum(), axis=0)\n",
    "\n",
    "data_miss = pd.DataFrame({'Index':data_miss.index,'Values':data_miss.values})\n",
    "var=list()\n",
    "for i,j in data_miss.iterrows():\n",
    "    if data_miss.iloc[i,1] > 0:\n",
    "        var.append(data_miss.iloc[i,0])\n",
    "\n",
    "print '共%s个特征有缺失值' % len(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       train\n",
       "1       train\n",
       "2       train\n",
       "3       train\n",
       "4       train\n",
       "5       train\n",
       "6       train\n",
       "7       train\n",
       "8       train\n",
       "9       train\n",
       "10      train\n",
       "11      train\n",
       "12      train\n",
       "13      train\n",
       "14      train\n",
       "15      train\n",
       "16      train\n",
       "17      train\n",
       "18      train\n",
       "19      train\n",
       "20      train\n",
       "21      train\n",
       "22      train\n",
       "23      train\n",
       "24      train\n",
       "25      train\n",
       "26      train\n",
       "27      train\n",
       "28      train\n",
       "29      train\n",
       "        ...  \n",
       "2889     test\n",
       "2890     test\n",
       "2891     test\n",
       "2892     test\n",
       "2893     test\n",
       "2894     test\n",
       "2895     test\n",
       "2896     test\n",
       "2897     test\n",
       "2898     test\n",
       "2899     test\n",
       "2900     test\n",
       "2901     test\n",
       "2902     test\n",
       "2903     test\n",
       "2904     test\n",
       "2905     test\n",
       "2906     test\n",
       "2907     test\n",
       "2908     test\n",
       "2909     test\n",
       "2910     test\n",
       "2911     test\n",
       "2912     test\n",
       "2913     test\n",
       "2914     test\n",
       "2915     test\n",
       "2916     test\n",
       "2917     test\n",
       "2918     test\n",
       "Name: source, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_df_munged['source'] = df['source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 五、清洗完的数据重新分开"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set size:', (1460, 424))\n",
      "('Test set size:', (1459, 424))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "#区分训练集和测试集\n",
    "all_df_munged['source'] = df['source']\n",
    "\n",
    "\n",
    "train_df_munged = all_df_munged.loc[all_df_munged['source']=='train']\n",
    "test_df_munged = all_df_munged.loc[all_df_munged['source']=='test']\n",
    "\n",
    "train_df_munged.drop(['source'],axis=1,inplace=True)\n",
    "test_df_munged.drop(['source'],axis=1,inplace=True)\n",
    "\n",
    "\n",
    "label_df = pd.DataFrame(index = train_df_munged.index, columns=[\"SalePrice\"])\n",
    "label_df[\"SalePrice\"] = np.log(df[\"SalePrice\"])\n",
    "\n",
    "print(\"Training set size:\", train_df_munged.shape)\n",
    "print(\"Test set size:\", test_df_munged.shape)\n",
    "\n",
    "\n",
    "train.to_csv('train_modified_12-10.csv',index=False)\n",
    "test.to_csv('test_modified_12-10.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "regr = xgb.XGBRegressor(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=4,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=7200,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 seed=42,\n",
    "                 silent=1)\n",
    "\n",
    "regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_pred = regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"XGBoost score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_xgb = regr.predict(test_df_munged)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# I found this best alpha through cross-validation.\n",
    "best_alpha = 0.00099\n",
    "\n",
    "regr = Lasso(alpha=best_alpha, max_iter=50000)\n",
    "regr.fit(train_df_munged, label_df)\n",
    "\n",
    "# Run prediction on training set to get a rough idea of how well it does.\n",
    "y_pred = regr.predict(train_df_munged)\n",
    "y_test = label_df\n",
    "print(\"Lasso score on training set: \", rmse(y_test, y_pred))\n",
    "\n",
    "# Run prediction on the Kaggle test set.\n",
    "y_pred_lasso = regr.predict(test_df_munged)\n",
    "\n",
    "################################################################################\n",
    "\n",
    "# Blend the results of the two regressors and save the prediction to a CSV file.\n",
    "\n",
    "y_pred = (y_pred_xgb + y_pred_lasso) / 2\n",
    "y_pred = np.exp(y_pred)\n",
    "\n",
    "pred_df = pd.DataFrame(y_pred, index=test_df[\"Id\"], columns=[\"SalePrice\"])\n",
    "pred_df.to_csv('output.csv', header=True, index_label='Id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
