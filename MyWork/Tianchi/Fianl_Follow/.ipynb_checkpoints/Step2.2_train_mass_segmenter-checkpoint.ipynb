{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *\n",
    "MEAN_FRAME_COUNT = 1\n",
    "CHANNEL_COUNT = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get train/holdout files.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid file path or buffer object type: <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-90ae462be820>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodel_type_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"masses\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinue_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    463\u001b[0m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinue_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m             \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mholdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_type_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontinue_from\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontinue_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-90ae462be820>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(holdout, model_type, continue_from)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    370\u001b[0m     \u001b[0mtrain_percentage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m80\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmodel_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"masses\"\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m90\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 371\u001b[0;31m     \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_holdout_files\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_percentage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCHANNEL_COUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    372\u001b[0m     \u001b[0;31m# train_files = train_files[:100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    373\u001b[0m     \u001b[0;31m# holdout_files = train_files[:10]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-90ae462be820>\u001b[0m in \u001b[0;36mget_train_holdout_files\u001b[0;34m(model_type, csv_path, holdout, train_percentage, frame_count)\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mpatient_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_patient_fold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpatient_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mholdout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m                 \u001b[0mtrain_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/CloudStation/Python Code/Tianchi/Fianl_Follow/utils/helpers.py\u001b[0m in \u001b[0;36mget_patient_fold\u001b[0;34m(csv_path, patient_id, submission_set_neg)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mPATIENT_LIST\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mPATIENT_LIST\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsv_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0mPATIENT_LIST\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seriesuid\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/envs/python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    653\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/envs/python36/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0mcompression\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_infer_compression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[0;32m--> 392\u001b[0;31m         filepath_or_buffer, encoding, compression)\n\u001b[0m\u001b[1;32m    393\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'compression'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/envs/python36/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Invalid file path or buffer object type: {_type}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid file path or buffer object type: <class 'int'>"
     ]
    }
   ],
   "source": [
    "def random_scale_img(img, xy_range, lock_xy=False):\n",
    "    if random.random() > xy_range.chance:\n",
    "        return img\n",
    "\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "\n",
    "    import cv2\n",
    "    scale_x = random.uniform(xy_range.x_min, xy_range.x_max)\n",
    "    scale_y = random.uniform(xy_range.y_min, xy_range.y_max)\n",
    "    if lock_xy:\n",
    "        scale_y = scale_x\n",
    "\n",
    "    org_height, org_width = img[0].shape[:2]\n",
    "    xy_range.last_x = scale_x\n",
    "    xy_range.last_y = scale_y\n",
    "\n",
    "    res = []\n",
    "    for img_inst in img:\n",
    "        scaled_width = int(org_width * scale_x)\n",
    "        scaled_height = int(org_height * scale_y)\n",
    "        scaled_img = cv2.resize(img_inst, (scaled_width, scaled_height), interpolation=cv2.INTER_CUBIC)\n",
    "        if scaled_width < org_width:\n",
    "            extend_left = (org_width - scaled_width) / 2\n",
    "            extend_right = org_width - extend_left - scaled_width\n",
    "            scaled_img = cv2.copyMakeBorder(scaled_img, 0, 0, extend_left, extend_right, borderType=cv2.BORDER_CONSTANT)\n",
    "            scaled_width = org_width\n",
    "\n",
    "        if scaled_height < org_height:\n",
    "            extend_top = (org_height - scaled_height) / 2\n",
    "            extend_bottom = org_height - extend_top - scaled_height\n",
    "            scaled_img = cv2.copyMakeBorder(scaled_img, extend_top, extend_bottom, 0, 0,  borderType=cv2.BORDER_CONSTANT)\n",
    "            scaled_height = org_height\n",
    "\n",
    "        start_x = (scaled_width - org_width) / 2\n",
    "        start_y = (scaled_height - org_height) / 2\n",
    "        tmp = scaled_img[start_y: start_y + org_height, start_x: start_x + org_width]\n",
    "        res.append(tmp)\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "class XYRange:\n",
    "    def __init__(self, x_min, x_max, y_min, y_max, chance=1.0):\n",
    "        self.chance = chance\n",
    "        self.x_min = x_min\n",
    "        self.x_max = x_max\n",
    "        self.y_min = y_min\n",
    "        self.y_max = y_max\n",
    "        self.last_x = 0\n",
    "        self.last_y = 0\n",
    "\n",
    "    def get_last_xy_txt(self):\n",
    "        res = \"x_\" + str(int(self.last_x * 100)).replace(\"-\", \"m\") + \"-\" + \"y_\" + str(int(self.last_y * 100)).replace(\"-\", \"m\")\n",
    "        return res\n",
    "\n",
    "\n",
    "def random_translate_img(img, xy_range, border_mode=\"constant\"):\n",
    "    if random.random() > xy_range.chance:\n",
    "        return img\n",
    "    import cv2\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "\n",
    "    org_height, org_width = img[0].shape[:2]\n",
    "    translate_x = random.randint(xy_range.x_min, xy_range.x_max)\n",
    "    translate_y = random.randint(xy_range.y_min, xy_range.y_max)\n",
    "    trans_matrix = numpy.float32([[1, 0, translate_x], [0, 1, translate_y]])\n",
    "\n",
    "    border_const = cv2.BORDER_CONSTANT\n",
    "    if border_mode == \"reflect\":\n",
    "        border_const = cv2.BORDER_REFLECT\n",
    "\n",
    "    res = []\n",
    "    for img_inst in img:\n",
    "        img_inst = cv2.warpAffine(img_inst, trans_matrix, (org_width, org_height), borderMode=border_const)\n",
    "        res.append(img_inst)\n",
    "    if len(res) == 1:\n",
    "        res = res[0]\n",
    "    xy_range.last_x = translate_x\n",
    "    xy_range.last_y = translate_y\n",
    "    return res\n",
    "\n",
    "\n",
    "def random_rotate_img(img, chance, min_angle, max_angle):\n",
    "    import cv2\n",
    "    if random.random() > chance:\n",
    "        return img\n",
    "    if not isinstance(img, list):\n",
    "        img = [img]\n",
    "\n",
    "    angle = random.randint(min_angle, max_angle)\n",
    "    center = (img[0].shape[0] / 2, img[0].shape[1] / 2)\n",
    "    rot_matrix = cv2.getRotationMatrix2D(center, angle, scale=1.0)\n",
    "\n",
    "    res = []\n",
    "    for img_inst in img:\n",
    "        img_inst = cv2.warpAffine(img_inst, rot_matrix, dsize=img_inst.shape[:2], borderMode=cv2.BORDER_CONSTANT)\n",
    "        res.append(img_inst)\n",
    "    if len(res) == 0:\n",
    "        res = res[0]\n",
    "    return res\n",
    "\n",
    "\n",
    "def random_flip_img(img, horizontal_chance=0, vertical_chance=0):\n",
    "    import cv2\n",
    "    flip_horizontal = False\n",
    "    if random.random() < horizontal_chance:\n",
    "        flip_horizontal = True\n",
    "\n",
    "    flip_vertical = False\n",
    "    if random.random() < vertical_chance:\n",
    "        flip_vertical = True\n",
    "\n",
    "    if not flip_horizontal and not flip_vertical:\n",
    "        return img\n",
    "\n",
    "    flip_val = 1\n",
    "    if flip_vertical:\n",
    "        flip_val = -1 if flip_horizontal else 0\n",
    "\n",
    "    if not isinstance(img, list):\n",
    "        res = cv2.flip(img, flip_val) # 0 = X axis, 1 = Y axis,  -1 = both\n",
    "    else:\n",
    "        res = []\n",
    "        for img_item in img:\n",
    "            img_flip = cv2.flip(img_item, flip_val)\n",
    "            res.append(img_flip)\n",
    "    return res\n",
    "\n",
    "\n",
    "ELASTIC_INDICES = None  # needed to make it faster to fix elastic deformation per epoch.\n",
    "def elastic_transform(image, alpha, sigma, random_state=None):\n",
    "    global ELASTIC_INDICES\n",
    "    shape = image.shape\n",
    "\n",
    "    if ELASTIC_INDICES == None:\n",
    "        if random_state is None:\n",
    "            random_state = numpy.random.RandomState(1301)\n",
    "\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        x, y = numpy.meshgrid(numpy.arange(shape[0]), numpy.arange(shape[1]))\n",
    "        ELASTIC_INDICES = numpy.reshape(y + dy, (-1, 1)), numpy.reshape(x + dx, (-1, 1))\n",
    "    return map_coordinates(image, ELASTIC_INDICES, order=1).reshape(shape)\n",
    "\n",
    "\n",
    "def prepare_image_for_net(img):\n",
    "    img = img.astype(numpy.float)\n",
    "    img /= 255.\n",
    "    if len(img.shape) == 3:\n",
    "        img = img.reshape(img.shape[-3], img.shape[-2], img.shape[-1])\n",
    "    else:\n",
    "        img = img.reshape(1, img.shape[-2], img.shape[-1], 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_train_holdout_files(model_type, csv_path,holdout, train_percentage=80, frame_count=8):\n",
    "    print(\"Get train/holdout files.\")\n",
    "    file_paths = glob.glob(\"/Volumes/solo/ali/pic/luna16_train_cubes_pos/\" + \"*_1_pos.png\")\n",
    "    file_paths.sort()\n",
    "    train_res = []\n",
    "    holdout_res = []\n",
    "    for index, file_path in enumerate(file_paths):\n",
    "        file_name = ntpath.basename(file_path)\n",
    "        overlay_path = file_path.replace(\"_1.png\", \"_o.png\")\n",
    "        train_set = False\n",
    "        if \"1.3.6.1.4\" in file_name or \"spie\" in file_name or \"TIME\" in file_name:\n",
    "            train_set = True\n",
    "        else:\n",
    "            patient_id = file_name.split(\"_\")[0]\n",
    "            if helpers.get_patient_fold(csv_path,patient_id) % 3 != holdout:\n",
    "                train_set = True\n",
    "\n",
    "        if train_set:\n",
    "            train_res.append((file_path, overlay_path))\n",
    "        else:\n",
    "            holdout_res.append((file_path, overlay_path))\n",
    "    print(\"Train count: \", len(train_res), \", holdout count: \", len(holdout_res))\n",
    "    return train_res, holdout_res\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 100) / (K.sum(y_true_f) + K.sum(y_pred_f) + 100)\n",
    "\n",
    "\n",
    "def dice_coef_np(y_true, y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = numpy.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 100) / (numpy.sum(y_true_f) + numpy.sum(y_pred_f) + 100)\n",
    "\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "class DumpPredictions(Callback):\n",
    "\n",
    "    def __init__(self, dump_filelist : List[Tuple[str, str]], model_type):\n",
    "        super(DumpPredictions, self).__init__()\n",
    "        self.dump_filelist = dump_filelist\n",
    "        self.batch_count = 0\n",
    "        if not os.path.exists(\"workdir/segmenter/\"):\n",
    "            os.mkdir(\"workdir/segmenter/\")\n",
    "        for file_path in glob.glob(\"workdir/segmenter/*.*\"):\n",
    "            os.remove(file_path)\n",
    "        self.model_type = model_type\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        model = self.model  # type: Model\n",
    "        generator = image_generator(self.dump_filelist, 1, train_set=False, model_type=self.model_type)\n",
    "        for i in range(0, 10):\n",
    "            x, y = next(generator)\n",
    "            y_pred = model.predict(x, batch_size=1)\n",
    "\n",
    "            x = x.swapaxes(0, 3)\n",
    "            x = x[0]\n",
    "            # print(x.shape, y.shape, y_pred.shape)\n",
    "            x *= 255.\n",
    "            x = x.reshape((x.shape[0], x.shape[0])).astype(numpy.uint8)\n",
    "            y *= 255.\n",
    "            y = y.reshape((y.shape[1], y.shape[2])).astype(numpy.uint8)\n",
    "            y_pred *= 255.\n",
    "            y_pred = y_pred.reshape((y_pred.shape[1], y_pred.shape[2])).astype(numpy.uint8)\n",
    "            # cv2.imwrite(\"workdir/segmenter/img_{0:03d}_{1:02d}_i.png\".format(epoch, i), x)\n",
    "            # cv2.imwrite(\"workdit/segmenter/img_{0:03d}_{1:02d}_o.png\".format(epoch, i), y)\n",
    "            # cv2.imwrite(\"workdit/segmenter/img_{0:03d}_{1:02d}_p.png\".format(epoch, i), y_pred)\n",
    "\n",
    "\n",
    "def image_generator(batch_files, batch_size, train_set, model_type):\n",
    "    global ELASTIC_INDICES\n",
    "    while True:\n",
    "        if train_set:\n",
    "            random.shuffle(batch_files)\n",
    "\n",
    "        img_list = []\n",
    "        overlay_list = []\n",
    "        ELASTIC_INDICES = None\n",
    "        for batch_file_idx, batch_file in enumerate(batch_files):\n",
    "            images = []\n",
    "            img = cv2.imread(batch_file[0], cv2.IMREAD_GRAYSCALE)\n",
    "            images.append(img)\n",
    "            overlay = cv2.imread(batch_file[1], cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "            if train_set:\n",
    "                if random.randint(0, 100) > 50:\n",
    "                    for img_index, img in enumerate(images):\n",
    "                        images[img_index] = elastic_transform(img, 128, 15)\n",
    "                    overlay = elastic_transform(overlay, 128, 15)\n",
    "\n",
    "                if True:\n",
    "                    augmented = images + [overlay]\n",
    "                    augmented = random_rotate_img(augmented, 0.8, -20, 20)\n",
    "                    augmented = random_flip_img(augmented, 0.5, 0.5)\n",
    "\n",
    "                    # processed = helpers_augmentation.random_flip_img(processed, horizontal_chance=0.5, vertical_chance=0)\n",
    "                    # processed = helpers_augmentation.random_scale_img(processed, xy_range=helpers_augmentation.XYRange(x_min=0.8, x_max=1.2, y_min=0.8, y_max=1.2, chance=1.0))\n",
    "                    augmented = random_translate_img(augmented, XYRange(-30, 30, -30, 30, 0.8))\n",
    "                    images = augmented[:-1]\n",
    "                    overlay = augmented[-1]\n",
    "\n",
    "            for index, img in enumerate(images):\n",
    "                # img = img[crop_y: crop_y + settings.TRAIN_IMG_HEIGHT3D, crop_x: crop_x + settings.TRAIN_IMG_WIDTH3D]\n",
    "                img = prepare_image_for_net(img)\n",
    "                images[index] = img\n",
    "\n",
    "            # helpers_augmentation.dump_augmented_image(img, mean_img=None, target_path=\"c:\\\\tmp\\\\\" + batch_file[0])\n",
    "            # overlay = overlay[crop_y: crop_y + settings.TRAIN_IMG_HEIGHT3D, crop_x: crop_x + settings.TRAIN_IMG_WIDTH3D]\n",
    "            overlay = prepare_image_for_net(overlay)\n",
    "            # overlay = overlay.reshape(1, overlay.shape[-3] * overlay.shape[-2])\n",
    "            # overlay *= settings.OVERLAY_MULTIPLIER\n",
    "            images3d = numpy.vstack(images)\n",
    "            images3d = images3d.swapaxes(0, 3)\n",
    "\n",
    "            img_list.append(images3d)\n",
    "            overlay_list.append(overlay)\n",
    "            if len(img_list) >= batch_size:\n",
    "                x = numpy.vstack(img_list)\n",
    "                y = numpy.vstack(overlay_list)\n",
    "                # if len(img_list) >= batch_size:\n",
    "                yield x, y\n",
    "                img_list = []\n",
    "                overlay_list = []\n",
    "\n",
    "\n",
    "def get_unet(learn_rate, load_weights_path=None) -> Model:\n",
    "    inputs = Input((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE, CHANNEL_COUNT))\n",
    "    filter_size = 32\n",
    "    growth_step = 32\n",
    "    x = BatchNormalization()(inputs)\n",
    "    conv1 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(x)\n",
    "    conv1 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    pool1 = BatchNormalization()(pool1)\n",
    "    filter_size += growth_step\n",
    "    conv2 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(pool1)\n",
    "    conv2 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    pool2 = BatchNormalization()(pool2)\n",
    "\n",
    "    filter_size += growth_step\n",
    "    conv3 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(pool2)\n",
    "    conv3 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    pool3 = BatchNormalization()(pool3)\n",
    "\n",
    "    filter_size += growth_step\n",
    "    conv4 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(pool3)\n",
    "    conv4 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "    pool4 = BatchNormalization()(pool4)\n",
    "\n",
    "    conv5 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(pool4)\n",
    "    conv5 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same', name=\"conv5b\")(conv5)\n",
    "    pool5 = MaxPooling2D(pool_size=(2, 2), name=\"pool5\")(conv5)\n",
    "    pool5 = BatchNormalization()(pool5)\n",
    "\n",
    "    conv6 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(pool5)\n",
    "    conv6 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same', name=\"conv6b\")(conv6)\n",
    "\n",
    "    up6 = UpSampling2D(size=(2, 2), name=\"up6\")(conv6)\n",
    "    up6 = merge([up6, conv5], mode='concat', concat_axis=3)\n",
    "    up6 = BatchNormalization()(up6)\n",
    "\n",
    "    # up6 = SpatialDropout2D(0.1)(up6)\n",
    "    filter_size -= growth_step\n",
    "    conv66 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(up6)\n",
    "    conv66 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv66)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv66), conv4], mode='concat', concat_axis=3)\n",
    "    up7 = BatchNormalization()(up7)\n",
    "    # up7 = SpatialDropout2D(0.1)(up7)\n",
    "\n",
    "    filter_size -= growth_step\n",
    "    conv7 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(up7)\n",
    "    conv7 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv3], mode='concat', concat_axis=3)\n",
    "    up8 = BatchNormalization()(up8)\n",
    "    filter_size -= growth_step\n",
    "    conv8 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(up8)\n",
    "    conv8 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv8)\n",
    "\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv2], mode='concat', concat_axis=3)\n",
    "    up9 = BatchNormalization()(up9)\n",
    "    conv9 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(up9)\n",
    "    conv9 = Convolution2D(filter_size, 3, 3, activation='relu', border_mode='same')(conv9)\n",
    "    # conv9 = BatchNormalization()(conv9)\n",
    "\n",
    "    up10 = UpSampling2D(size=(2, 2))(conv9)\n",
    "    conv10 = Convolution2D(1, 1, 1, activation='sigmoid')(up10)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    # model.load_weights(load_weights_path)\n",
    "    # model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    model.compile(optimizer=SGD(lr=learn_rate, momentum=0.9, nesterov=True), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_model(holdout, model_type, continue_from=None):\n",
    "    batch_size = 4\n",
    "    train_percentage = 80 if model_type == \"masses\" else 90\n",
    "    train_files, holdout_files = get_train_holdout_files( model_type, holdout, train_percentage, frame_count=CHANNEL_COUNT)\n",
    "    # train_files = train_files[:100]\n",
    "    # holdout_files = train_files[:10]\n",
    "\n",
    "    tmp_gen = image_generator(train_files[:2], 2, True, model_type)\n",
    "    for i in range(10):\n",
    "        x = next(tmp_gen)\n",
    "        img = x[0][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n",
    "        img *= 255\n",
    "        # cv2.imwrite(\"c:/tmp/img_\" + str(i).rjust(3, '0') + \"i.png\", img)\n",
    "        img = x[1][0].reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n",
    "        img *= 255\n",
    "        # cv2.imwrite(\"c:/tmp/img_\" + str(i).rjust(3, '0') + \"o.png\", img)\n",
    "        # print(x.shape)\n",
    "\n",
    "    train_gen = image_generator(train_files, batch_size, True, model_type)\n",
    "    holdout_gen = image_generator(holdout_files, batch_size, False, model_type)\n",
    "\n",
    "    if continue_from is None:\n",
    "        model = get_unet(0.001)\n",
    "    else:\n",
    "        model = get_unet(0.0001)\n",
    "        model.load_weights(continue_from)\n",
    "\n",
    "    checkpoint1 = ModelCheckpoint(\"workdir/\" + model_type +\"_model_h\" + str(holdout) + \"_{epoch:02d}-{val_loss:.2f}.hd5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    checkpoint2 = ModelCheckpoint(\"workdir/\" + model_type +\"_model_h\" + str(holdout) + \"_best.hd5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    files = []\n",
    "    idx = 0\n",
    "    while (idx < (len(holdout_files))):\n",
    "        files.append(holdout_files[idx])\n",
    "        idx += 5\n",
    "    dumper = DumpPredictions(holdout_files[::10], model_type)\n",
    "    epoch_div = 1\n",
    "    epoch_count = 200 if model_type == \"masses\" else 50\n",
    "    model.fit_generator(train_gen, len(train_files) / epoch_div, epoch_count, validation_data=holdout_gen, nb_val_samples=len(holdout_files) / epoch_div, callbacks=[checkpoint1, checkpoint2, dumper])\n",
    "    shutil.copy(\"workdir/\" + model_type +\"_model_h\" + str(holdout) + \"_best.hd5\", \"models/\" + model_type +\"_model_h\" + str(holdout) + \"_best.hd5\")\n",
    "\n",
    "def predict_patients(patients_dir, csv_path, model_path, holdout, patient_predictions, model_type):\n",
    "    model = get_unet(0.001)\n",
    "    model.load_weights(model_path)\n",
    "    for item_name in os.listdir(patients_dir):\n",
    "        if not os.path.isdir(patients_dir + item_name):\n",
    "            continue\n",
    "        patient_id = item_name\n",
    "\n",
    "        if holdout >= 0:\n",
    "            patient_fold = helpers.get_patient_fold(csv_path,patient_id, submission_set_neg=True)\n",
    "            if patient_fold < 0:\n",
    "                if holdout != 0:\n",
    "                    continue\n",
    "            else:\n",
    "                patient_fold %= 3\n",
    "                if patient_fold != holdout:\n",
    "                    continue\n",
    "\n",
    "        # if \"100953483028192176989979435275\" not in patient_id:\n",
    "        #     continue\n",
    "        print(patient_id)\n",
    "        patient_dir = patients_dir + patient_id + \"/\"\n",
    "        mass = 0\n",
    "        img_type = \"_i\" if model_type == \"masses\" else \"_c\"\n",
    "        slices = glob.glob(patient_dir + \"*\" + img_type + \".png\")\n",
    "        if model_type == \"emphysema\":\n",
    "            slices = slices[int(len(slices) / 2):]\n",
    "        for img_path in slices:\n",
    "            src_img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            src_img = cv2.resize(src_img, dsize=(settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n",
    "            src_img = prepare_image_for_net(src_img)\n",
    "            p = model.predict(src_img, batch_size=1)\n",
    "            p[p < 0.5] = 0\n",
    "            mass += p.sum()\n",
    "            p = p[0, :, :, 0] * 255\n",
    "            # cv2.imwrite(img_path.replace(\"_i.png\", \"_mass.png\"), p)\n",
    "            src_img = src_img.reshape((settings.SEGMENTER_IMG_SIZE, settings.SEGMENTER_IMG_SIZE))\n",
    "            src_img *= 255\n",
    "            # src_img = cv2.cvtColor(src_img.astype(numpy.uint8), cv2.COLOR_GRAY2BGR)\n",
    "            # p = cv2.cvtColor(p.astype(numpy.uint8), cv2.COLOR_GRAY2BGRA)\n",
    "            src_img = cv2.addWeighted(p.astype(numpy.uint8), 0.2, src_img.astype(numpy.uint8), 1 - 0.2, 0)\n",
    "            cv2.imwrite(img_path.replace(img_type + \".png\", \"_\" + model_type + \"o.png\"), src_img)\n",
    "\n",
    "        if mass > 1:\n",
    "            print(model_type + \": \", mass)\n",
    "        patient_predictions.append((patient_id, mass))\n",
    "        df = pandas.DataFrame(patient_predictions, columns=[\"patient_id\", \"prediction\"])\n",
    "        df.to_csv(settings.BASE_DIR + model_type + \"_predictions.csv\", index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = PATH['annotations_train']\n",
    "model_paths = PATH['model_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "continue_from = None\n",
    "if True:\n",
    "    for model_type_name in [\"masses\"]:\n",
    "        train_model(holdout=0, model_type=model_type_name, continue_from=continue_from)\n",
    "        train_model(holdout=1, model_type=model_type_name, continue_from=continue_from)\n",
    "        train_model(holdout=2, model_type=model_type_name, continue_from=continue_from)\n",
    "\n",
    "if True:\n",
    "    for model_type_name in [\"masses\"]:\n",
    "        patient_predictions_global = []\n",
    "        for holdout_no in [0, 1, 2]:\n",
    "            patient_base_dir = settings.NDSB3_EXTRACTED_IMAGE_DIR\n",
    "            predict_patients(patients_dir=patient_base_dir, model_path=model_paths + model_type_name + \"_model_h\" + str(holdout_no) + \"_best.hd5\", holdout=holdout_no, patient_predictions=patient_predictions_global, model_type=model_type_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
