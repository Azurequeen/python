{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *\n",
    "K.set_image_dim_ordering(\"tf\")\n",
    "CUBE_SIZE = 32\n",
    "MEAN_PIXEL_VALUE = MEAN_PIXEL_VALUE_NODULE\n",
    "POS_WEIGHT = 2\n",
    "NEGS_PER_POS = 20\n",
    "P_TH = 0.6\n",
    "# POS_IMG_DIR = \"luna16_train_cubes_pos\"\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "USE_DROPOUT = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# limit memory usage..\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "# zonder aug, 10:1 99 train, 97 test, 0.27 cross entropy, before commit 573\n",
    "# 3 pools istead of 4 gives (bigger end layer) gives much worse validation accuray + logloss .. strange ?\n",
    "# 32 x 32 x 32 lijkt het beter te doen dan 48 x 48 x 48..\n",
    "\n",
    "\n",
    "\n",
    "def prepare_image_for_net3D(img):\n",
    "    img = img.astype(numpy.float32)\n",
    "    img -= MEAN_PIXEL_VALUE\n",
    "    img /= 255.\n",
    "    img = img.reshape(1, img.shape[0], img.shape[1], img.shape[2], 1)\n",
    "    return img\n",
    "\n",
    "\n",
    "def get_train_holdout_files(fold_count, train_percentage=80, logreg=True, ndsb3_holdout=0, manual_labels=True, full_luna_set=False):\n",
    "    print(\"Get train/holdout files.\")\n",
    "    # pos_samples = glob.glob(settings.BASE_DIR_SSD + \"luna16_train_cubes_pos/*.png\")\n",
    "    pos_samples = glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/luna16_train_cubes_lidc/*.png\")\n",
    "    print(\"Pos samples: \", len(pos_samples))\n",
    "\n",
    "    pos_samples_manual = glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/luna16_train_cubes_manual/*_pos.png\")\n",
    "    print(\"Pos samples manual: \", len(pos_samples_manual))\n",
    "    pos_samples += pos_samples_manual\n",
    "\n",
    "    random.shuffle(pos_samples)\n",
    "    train_pos_count = int((len(pos_samples) * train_percentage) / 100)\n",
    "    pos_samples_train = pos_samples[:train_pos_count]\n",
    "    pos_samples_holdout = pos_samples[train_pos_count:]\n",
    "    if full_luna_set:\n",
    "        pos_samples_train += pos_samples_holdout\n",
    "        if manual_labels:\n",
    "            pos_samples_holdout = []\n",
    "\n",
    "\n",
    "    ndsb3_list = glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/ndsb3_train_cubes_manual/*.png\")\n",
    "    print(\"Ndsb3 samples: \", len(ndsb3_list))\n",
    "\n",
    "    pos_samples_ndsb3_fold = []\n",
    "    pos_samples_ndsb3_holdout = []\n",
    "    ndsb3_pos = 0\n",
    "    ndsb3_neg = 0\n",
    "    ndsb3_pos_holdout = 0\n",
    "    ndsb3_neg_holdout = 0\n",
    "    if manual_labels:\n",
    "        for file_path in ndsb3_list:\n",
    "            file_name = ntpath.basename(file_path)\n",
    "\n",
    "            parts = file_name.split(\"_\")\n",
    "            if int(parts[4]) == 0 and parts[3] != \"neg\":  # skip positive non-cancer-cases\n",
    "                continue\n",
    "\n",
    "            if fold_count == 3:\n",
    "                if parts[3] == \"neg\":  # skip negative cases\n",
    "                    continue\n",
    "\n",
    "\n",
    "            patient_id = parts[1]\n",
    "            patient_fold = helpers.get_patient_fold(patient_id) % fold_count\n",
    "            if patient_fold == ndsb3_holdout:\n",
    "                pos_samples_ndsb3_holdout.append(file_path)\n",
    "                if parts[3] == \"neg\":\n",
    "                    ndsb3_neg_holdout += 1\n",
    "                else:\n",
    "                    ndsb3_pos_holdout += 1\n",
    "            else:\n",
    "                pos_samples_ndsb3_fold.append(file_path)\n",
    "                print(\"In fold: \", patient_id)\n",
    "                if parts[3] == \"neg\":\n",
    "                    ndsb3_neg += 1\n",
    "                else:\n",
    "                    ndsb3_pos += 1\n",
    "\n",
    "    print(ndsb3_pos, \" ndsb3 pos labels train\")\n",
    "    print(ndsb3_neg, \" ndsb3 neg labels train\")\n",
    "    print(ndsb3_pos_holdout, \" ndsb3 pos labels holdout\")\n",
    "    print(ndsb3_neg_holdout, \" ndsb3 neg labels holdout\")\n",
    "\n",
    "\n",
    "    if manual_labels:\n",
    "        for times_ndsb3 in range(4):  # make ndsb labels count 4 times just like in LIDC when 4 doctors annotated a nodule\n",
    "            pos_samples_train += pos_samples_ndsb3_fold\n",
    "            pos_samples_holdout += pos_samples_ndsb3_holdout\n",
    "\n",
    "    neg_samples_edge = glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/luna16_train_cubes_auto/*_edge.png\")\n",
    "    print(\"Edge samples: \", len(neg_samples_edge))\n",
    "\n",
    "    # neg_samples_white = glob.glob(settings.BASE_DIR_SSD + \"luna16_train_cubes_auto/*_white.png\")\n",
    "    neg_samples_luna = glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/luna16_train_cubes_auto/*_luna.png\")\n",
    "    print(\"Luna samples: \", len(neg_samples_luna))\n",
    "\n",
    "    # neg_samples = neg_samples_edge + neg_samples_white\n",
    "    neg_samples = neg_samples_edge + neg_samples_luna\n",
    "    random.shuffle(neg_samples)\n",
    "\n",
    "    train_neg_count = int((len(neg_samples) * train_percentage) / 100)\n",
    "\n",
    "    neg_samples_falsepos = []\n",
    "    for file_path in glob.glob(settings.BASE_DIR_SSD + \"generated_traindata/luna16_train_cubes_auto/*_falsepos.png\"):\n",
    "        neg_samples_falsepos.append(file_path)\n",
    "    print(\"Falsepos LUNA count: \", len(neg_samples_falsepos))\n",
    "\n",
    "    neg_samples_train = neg_samples[:train_neg_count]\n",
    "    neg_samples_train += neg_samples_falsepos + neg_samples_falsepos + neg_samples_falsepos\n",
    "    neg_samples_holdout = neg_samples[train_neg_count:]\n",
    "    if full_luna_set:\n",
    "        neg_samples_train += neg_samples_holdout\n",
    "\n",
    "    train_res = []\n",
    "    holdout_res = []\n",
    "    sets = [(train_res, pos_samples_train, neg_samples_train), (holdout_res, pos_samples_holdout, neg_samples_holdout)]\n",
    "    for set_item in sets:\n",
    "        pos_idx = 0\n",
    "        negs_per_pos = NEGS_PER_POS\n",
    "        res = set_item[0]\n",
    "        neg_samples = set_item[2]\n",
    "        pos_samples = set_item[1]\n",
    "        print(\"Pos\", len(pos_samples))\n",
    "        ndsb3_pos = 0\n",
    "        ndsb3_neg = 0\n",
    "        for index, neg_sample_path in enumerate(neg_samples):\n",
    "            # res.append(sample_path + \"/\")\n",
    "            res.append((neg_sample_path, 0, 0))\n",
    "            if index % negs_per_pos == 0:\n",
    "                pos_sample_path = pos_samples[pos_idx]\n",
    "                file_name = ntpath.basename(pos_sample_path)\n",
    "                parts = file_name.split(\"_\")\n",
    "                if parts[0].startswith(\"ndsb3manual\"):\n",
    "                    if parts[3] == \"pos\":\n",
    "                        class_label = 1  # only take positive examples where we know there was a cancer..\n",
    "                        cancer_label = int(parts[4])\n",
    "                        assert cancer_label == 1\n",
    "                        size_label = int(parts[5])\n",
    "                        # print(parts[1], size_label)\n",
    "                        assert class_label == 1\n",
    "                        if size_label < 1:\n",
    "                            print(\"huh ?\")\n",
    "                        assert size_label >= 1\n",
    "                        ndsb3_pos += 1\n",
    "                    else:\n",
    "                        class_label = 0\n",
    "                        size_label = 0\n",
    "                        ndsb3_neg += 1\n",
    "                else:\n",
    "                    class_label = int(parts[-2])\n",
    "                    size_label = int(parts[-3])\n",
    "                    assert class_label == 1\n",
    "                    assert parts[-1] == \"pos.png\"\n",
    "                    assert size_label >= 1\n",
    "\n",
    "                res.append((pos_sample_path, class_label, size_label))\n",
    "                pos_idx += 1\n",
    "                pos_idx %= len(pos_samples)\n",
    "\n",
    "        print(\"ndsb2 pos: \", ndsb3_pos)\n",
    "        print(\"ndsb2 neg: \", ndsb3_neg)\n",
    "\n",
    "    print(\"Train count: \", len(train_res), \", holdout count: \", len(holdout_res))\n",
    "    return train_res, holdout_res\n",
    "\n",
    "\n",
    "def data_generator(batch_size, record_list, train_set):\n",
    "    batch_idx = 0\n",
    "    means = []\n",
    "    random_state = numpy.random.RandomState(1301)\n",
    "    while True:\n",
    "        img_list = []\n",
    "        class_list = []\n",
    "        size_list = []\n",
    "        if train_set:\n",
    "            random.shuffle(record_list)\n",
    "        CROP_SIZE = CUBE_SIZE\n",
    "        # CROP_SIZE = 48\n",
    "        for record_idx, record_item in enumerate(record_list):\n",
    "            #rint patient_dir\n",
    "            class_label = record_item[1]\n",
    "            size_label = record_item[2]\n",
    "            if class_label == 0:\n",
    "                cube_image = helpers.load_cube_img(record_item[0], 6, 8, 48)\n",
    "                # if train_set:\n",
    "                #     # helpers.save_cube_img(\"c:/tmp/pre.png\", cube_image, 8, 8)\n",
    "                #     cube_image = random_rotate_cube_img(cube_image, 0.99, -180, 180)\n",
    "                #\n",
    "                # if train_set:\n",
    "                #     if random.randint(0, 100) > 0.1:\n",
    "                #         # cube_image = numpy.flipud(cube_image)\n",
    "                #         cube_image = elastic_transform48(cube_image, 64, 8, random_state)\n",
    "                wiggle = 48 - CROP_SIZE - 1\n",
    "                indent_x = 0\n",
    "                indent_y = 0\n",
    "                indent_z = 0\n",
    "                if wiggle > 0:\n",
    "                    indent_x = random.randint(0, wiggle)\n",
    "                    indent_y = random.randint(0, wiggle)\n",
    "                    indent_z = random.randint(0, wiggle)\n",
    "                cube_image = cube_image[indent_z:indent_z + CROP_SIZE, indent_y:indent_y + CROP_SIZE, indent_x:indent_x + CROP_SIZE]\n",
    "\n",
    "                if train_set:\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = numpy.fliplr(cube_image)\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = numpy.flipud(cube_image)\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = cube_image[:, :, ::-1]\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = cube_image[:, ::-1, :]\n",
    "\n",
    "                if CROP_SIZE != CUBE_SIZE:\n",
    "                    cube_image = helpers.rescale_patient_images2(cube_image, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\n",
    "                assert cube_image.shape == (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE)\n",
    "            else:\n",
    "                cube_image = helpers.load_cube_img(record_item[0], 8, 8, 64)\n",
    "\n",
    "                if train_set:\n",
    "                    pass\n",
    "\n",
    "                current_cube_size = cube_image.shape[0]\n",
    "                indent_x = (current_cube_size - CROP_SIZE) / 2\n",
    "                indent_y = (current_cube_size - CROP_SIZE) / 2\n",
    "                indent_z = (current_cube_size - CROP_SIZE) / 2\n",
    "                wiggle_indent = 0\n",
    "                wiggle = current_cube_size - CROP_SIZE - 1\n",
    "                if wiggle > (CROP_SIZE / 2):\n",
    "                    wiggle_indent = CROP_SIZE / 4\n",
    "                    wiggle = current_cube_size - CROP_SIZE - CROP_SIZE / 2 - 1\n",
    "                if train_set:\n",
    "                    indent_x = wiggle_indent + random.randint(0, wiggle)\n",
    "                    indent_y = wiggle_indent + random.randint(0, wiggle)\n",
    "                    indent_z = wiggle_indent + random.randint(0, wiggle)\n",
    "\n",
    "                indent_x = int(indent_x)\n",
    "                indent_y = int(indent_y)\n",
    "                indent_z = int(indent_z)\n",
    "                cube_image = cube_image[indent_z:indent_z + CROP_SIZE, indent_y:indent_y + CROP_SIZE, indent_x:indent_x + CROP_SIZE]\n",
    "                if CROP_SIZE != CUBE_SIZE:\n",
    "                    cube_image = helpers.rescale_patient_images2(cube_image, (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE))\n",
    "                assert cube_image.shape == (CUBE_SIZE, CUBE_SIZE, CUBE_SIZE)\n",
    "\n",
    "                if train_set:\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = numpy.fliplr(cube_image)\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = numpy.flipud(cube_image)\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = cube_image[:, :, ::-1]\n",
    "                    if random.randint(0, 100) > 50:\n",
    "                        cube_image = cube_image[:, ::-1, :]\n",
    "\n",
    "\n",
    "            means.append(cube_image.mean())\n",
    "            img3d = prepare_image_for_net3D(cube_image)\n",
    "            if train_set:\n",
    "                if len(means) % 1000000 == 0:\n",
    "                    print(\"Mean: \", sum(means) / len(means))\n",
    "            img_list.append(img3d)\n",
    "            class_list.append(class_label)\n",
    "            size_list.append(size_label)\n",
    "\n",
    "            batch_idx += 1\n",
    "            if batch_idx >= batch_size:\n",
    "                x = numpy.vstack(img_list)\n",
    "                y_class = numpy.vstack(class_list)\n",
    "                y_size = numpy.vstack(size_list)\n",
    "                yield x, {\"out_class\": y_class, \"out_malignancy\": y_size}\n",
    "                img_list = []\n",
    "                class_list = []\n",
    "                size_list = []\n",
    "                batch_idx = 0\n",
    "\n",
    "\n",
    "def get_net(input_shape=(CUBE_SIZE, CUBE_SIZE, CUBE_SIZE, 1), load_weight_path=None, features=False, mal=False) -> Model:\n",
    "    inputs = Input(shape=input_shape, name=\"input_1\")\n",
    "    x = inputs\n",
    "    x = AveragePooling3D(pool_size=(2, 1, 1), strides=(2, 1, 1), border_mode=\"same\")(x)\n",
    "    x = Convolution3D(64, 3, 3, 3, activation='relu', border_mode='same', name='conv1', subsample=(1, 1, 1))(x)\n",
    "    x = MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), border_mode='valid', name='pool1')(x)\n",
    "\n",
    "    # 2nd layer group\n",
    "    x = Convolution3D(128, 3, 3, 3, activation='relu', border_mode='same', name='conv2', subsample=(1, 1, 1))(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool2')(x)\n",
    "    if USE_DROPOUT:\n",
    "        x = Dropout(p=0.3)(x)\n",
    "\n",
    "    # 3rd layer group\n",
    "    x = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3a', subsample=(1, 1, 1))(x)\n",
    "    x = Convolution3D(256, 3, 3, 3, activation='relu', border_mode='same', name='conv3b', subsample=(1, 1, 1))(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool3')(x)\n",
    "    if USE_DROPOUT:\n",
    "        x = Dropout(p=0.4)(x)\n",
    "\n",
    "    # 4th layer group\n",
    "    x = Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv4a', subsample=(1, 1, 1))(x)\n",
    "    x = Convolution3D(512, 3, 3, 3, activation='relu', border_mode='same', name='conv4b', subsample=(1, 1, 1),)(x)\n",
    "    x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2), border_mode='valid', name='pool4')(x)\n",
    "    if USE_DROPOUT:\n",
    "        x = Dropout(p=0.5)(x)\n",
    "\n",
    "    last64 = Convolution3D(64, 2, 2, 2, activation=\"relu\", name=\"last_64\")(x)\n",
    "    out_class = Convolution3D(1, 1, 1, 1, activation=\"sigmoid\", name=\"out_class_last\")(last64)\n",
    "    out_class = Flatten(name=\"out_class\")(out_class)\n",
    "\n",
    "    out_malignancy = Convolution3D(1, 1, 1, 1, activation=None, name=\"out_malignancy_last\")(last64)\n",
    "    out_malignancy = Flatten(name=\"out_malignancy\")(out_malignancy)\n",
    "\n",
    "    model = Model(input=inputs, output=[out_class, out_malignancy])\n",
    "    if load_weight_path is not None:\n",
    "        model.load_weights(load_weight_path, by_name=False)\n",
    "    model.compile(optimizer=SGD(lr=LEARN_RATE, momentum=0.9, nesterov=True), loss={\"out_class\": \"binary_crossentropy\", \"out_malignancy\": mean_absolute_error}, metrics={\"out_class\": [binary_accuracy, binary_crossentropy], \"out_malignancy\": mean_absolute_error})\n",
    "\n",
    "    if features:\n",
    "        model = Model(input=inputs, output=[last64])\n",
    "    model.summary(line_length=140)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    res = 0.001\n",
    "    if epoch > 5:\n",
    "        res = 0.0001\n",
    "    print(\"learnrate: \", res, \" epoch: \", epoch)\n",
    "    return res\n",
    "\n",
    "\n",
    "def train(model_name, fold_count, train_full_set=False, load_weights_path=None, ndsb3_holdout=0, manual_labels=True):\n",
    "    batch_size = 16\n",
    "    train_files, holdout_files = get_train_holdout_files(train_percentage=80, ndsb3_holdout=ndsb3_holdout, manual_labels=manual_labels, full_luna_set=train_full_set, fold_count=fold_count)\n",
    "\n",
    "    # train_files = train_files[:100]\n",
    "    # holdout_files = train_files[:10]\n",
    "    train_gen = data_generator(batch_size, train_files, True)\n",
    "    holdout_gen = data_generator(batch_size, holdout_files, False)\n",
    "    for i in range(0, 10):\n",
    "        tmp = next(holdout_gen)\n",
    "        cube_img = tmp[0][0].reshape(CUBE_SIZE, CUBE_SIZE, CUBE_SIZE, 1)\n",
    "        cube_img = cube_img[:, :, :, 0]\n",
    "        cube_img *= 255.\n",
    "        cube_img += MEAN_PIXEL_VALUE\n",
    "        # helpers.save_cube_img(\"c:/tmp/img_\" + str(i) + \".png\", cube_img, 4, 8)\n",
    "        # print(tmp)\n",
    "\n",
    "    learnrate_scheduler = LearningRateScheduler(step_decay)\n",
    "    model = get_net(load_weight_path=load_weights_path)\n",
    "    holdout_txt = \"_h\" + str(ndsb3_holdout) if manual_labels else \"\"\n",
    "    if train_full_set:\n",
    "        holdout_txt = \"_fs\" + holdout_txt\n",
    "    checkpoint = ModelCheckpoint(\"workdir/model_\" + model_name + \"_\" + holdout_txt + \"_e\" + \"{epoch:02d}-{val_loss:.4f}.hd5\", monitor='val_loss', verbose=1, save_best_only=not train_full_set, save_weights_only=False, mode='auto', period=1)\n",
    "    checkpoint_fixed_name = ModelCheckpoint(\"workdir/model_\" + model_name + \"_\" + holdout_txt + \"_best.hd5\", monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "    model.fit_generator(train_gen, len(train_files) / 1, 12, validation_data=holdout_gen, nb_val_samples=len(holdout_files) / 1, callbacks=[checkpoint, checkpoint_fixed_name, learnrate_scheduler])\n",
    "    model.save(\"workdir/model_\" + model_name + \"_\" + holdout_txt + \"_end.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get train/holdout files.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'settings' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-dad254441d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0;31m# model 1 on luna16 annotations. full set 1 versions for blending\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_full_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_weights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"luna16_full\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-65e1904e6e37>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model_name, fold_count, train_full_set, load_weights_path, ndsb3_holdout, manual_labels)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_full_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_weights_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndsb3_holdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m16\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0mtrain_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mholdout_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_train_holdout_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m80\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndsb3_holdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndsb3_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanual_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmanual_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_luna_set\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_full_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfold_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;31m# train_files = train_files[:100]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-65e1904e6e37>\u001b[0m in \u001b[0;36mget_train_holdout_files\u001b[0;34m(fold_count, train_percentage, logreg, ndsb3_holdout, manual_labels, full_luna_set)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Get train/holdout files.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# pos_samples = glob.glob(settings.BASE_DIR_SSD + \"luna16_train_cubes_pos/*.png\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mpos_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBASE_DIR_SSD\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"generated_traindata/luna16_train_cubes_lidc/*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pos samples: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'settings' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "if True:\n",
    "        # model 1 on luna16 annotations. full set 1 versions for blending\n",
    "    train(train_full_set=True, load_weights_path=None, model_name=\"luna16_full\", fold_count=-1, manual_labels=False)\n",
    "    if not os.path.exists(\"models/\"):\n",
    "        os.mkdir(\"models\")\n",
    "    shutil.copy(\"workdir/model_luna16_full__fs_best.hd5\", \"models/model_luna16_full__fs_best.hd5\")\n",
    "\n",
    "    # model 2 on luna16 annotations + ndsb pos annotations. 3 folds (1st half, 2nd half of ndsb patients) 2 versions for blending\n",
    "if True:\n",
    "    train(train_full_set=True, load_weights_path=None, ndsb3_holdout=0, manual_labels=True, model_name=\"luna_posnegndsb_v1\", fold_count=2)\n",
    "    train(train_full_set=True, load_weights_path=None, ndsb3_holdout=1, manual_labels=True, model_name=\"luna_posnegndsb_v1\", fold_count=2)\n",
    "    shutil.copy(\"workdir/model_luna_posnegndsb_v1__fs_h0_end.hd5\", \"models/model_luna_posnegndsb_v1__fs_h0_end.hd5\")\n",
    "    shutil.copy(\"workdir/model_luna_posnegndsb_v1__fs_h1_end.hd5\", \"models/model_luna_posnegndsb_v1__fs_h1_end.hd5\")\n",
    "\n",
    "if True:\n",
    "    train(train_full_set=True, load_weights_path=None, ndsb3_holdout=0, manual_labels=True, model_name=\"luna_posnegndsb_v2\", fold_count=2)\n",
    "    train(train_full_set=True, load_weights_path=None, ndsb3_holdout=1, manual_labels=True, model_name=\"luna_posnegndsb_v2\", fold_count=2)\n",
    "    shutil.copy(\"workdir/model_luna_posnegndsb_v2__fs_h0_end.hd5\", \"models/model_luna_posnegndsb_v2__fs_h0_end.hd5\")\n",
    "    shutil.copy(\"workdir/model_luna_posnegndsb_v2__fs_h1_end.hd5\", \"models/model_luna_posnegndsb_v2__fs_h1_end.hd5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = PATH['annotations_train']\n",
    "generated_path = PATH['generated_train']\n",
    "model_paths = PATH['model_paths']\n",
    "model_paths_temp = PATH['model_paths_temp']\n",
    "pred_path = PATH['pic_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
