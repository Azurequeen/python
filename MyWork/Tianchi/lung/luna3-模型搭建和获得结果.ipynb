{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 一、加载常用库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from skimage import morphology\n",
    "from skimage import measure\n",
    "from sklearn.cluster import KMeans\n",
    "from skimage.transform import resize\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings #不显示乱七八糟的warning\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二、设置环境"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "luna_path = './'\n",
    "luna_subset_path = luna_path + 'sample_patients/'\n",
    "file_list = glob(luna_subset_path + \"*.mhd\")\n",
    "df_node = pd.read_csv(luna_path + 'csv_files/' + 'annotations.csv')\n",
    "npy_list=glob(luna_path + 'npy/' + \"images_*.npy\")\n",
    "\n",
    "\n",
    "output_path = luna_path + 'npy/'\n",
    "working_path = luna_path + 'output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三、处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 归一化后使用kmeans分割前景和背景"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    train_and_predict(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_file in npy_list:\n",
    "    imgs_to_process = np.load(img_file).astype(np.float64) \n",
    "    print u'图像文件：', img_file\n",
    "    for i in range(len(imgs_to_process)):\n",
    "        img = imgs_to_process[i]\n",
    "        mean = np.mean(img)\n",
    "        std = np.std(img)\n",
    "        img = img-mean\n",
    "        img = img/std        \n",
    "        middle = img[100:400,100:400] \n",
    "        mean = np.mean(middle)  \n",
    "        max = np.max(img)\n",
    "        min = np.min(img)\n",
    "        img[img==max]=mean\n",
    "        img[img==min]=mean\n",
    "        kmeans = KMeans(n_clusters=2).fit(np.reshape(middle,[np.prod(middle.shape),1]))\n",
    "        centers = sorted(kmeans.cluster_centers_.flatten())\n",
    "        threshold = np.mean(centers)\n",
    "        thresh_img = np.where(img<threshold,1.0,0.0)  # threshold the image\n",
    "        eroded = morphology.erosion(thresh_img,np.ones([4,4]))\n",
    "        dilation = morphology.dilation(eroded,np.ones([10,10]))\n",
    "        labels = measure.label(dilation)\n",
    "        label_vals = np.unique(labels)\n",
    "        regions = measure.regionprops(labels)\n",
    "        good_labels = []\n",
    "        for prop in regions:\n",
    "            B = prop.bbox\n",
    "            if B[2]-B[0]<475 and B[3]-B[1]<475 and B[0]>40 and B[2]<472:\n",
    "                good_labels.append(prop.label)\n",
    "        mask = np.ndarray([512,512],dtype=np.int8)\n",
    "        mask[:] = 0\n",
    "        for N in good_labels:\n",
    "            mask = mask + np.where(labels==N,1,0)\n",
    "        mask = morphology.dilation(mask,np.ones([10,10])) # one last dilation\n",
    "        imgs_to_process[i] = mask\n",
    "    np.save(img_file.replace(\"images\",\"lungmask\"),imgs_to_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图像文件： ./npy/images_0000_0025.npy\n",
      "图像文件： ./npy/images_0000_0026.npy\n",
      "图像文件： ./npy/images_0001_1002.npy\n",
      "图像文件： ./npy/images_0001_1003.npy\n",
      "图像文件： ./npy/images_0002_1145.npy\n",
      "图像文件： ./npy/images_0002_1146.npy\n",
      "图像文件： ./npy/images_0002_1147.npy\n",
      "图像文件： ./npy/images_0002_1148.npy\n",
      "图像文件： ./npy/images_0002_1149.npy\n",
      "图像文件： ./npy/images_0002_1150.npy\n",
      "图像文件： ./npy/images_0002_1151.npy\n",
      "正在处理： ./npy/lungmask_0000_0025.npy\n",
      "正在处理： ./npy/lungmask_0000_0026.npy\n",
      "正在处理： ./npy/lungmask_0001_1002.npy\n",
      "正在处理： ./npy/lungmask_0001_1003.npy\n",
      "正在处理： ./npy/lungmask_0002_1145.npy\n",
      "正在处理： ./npy/lungmask_0002_1146.npy\n",
      "正在处理： ./npy/lungmask_0002_1147.npy\n",
      "正在处理： ./npy/lungmask_0002_1148.npy\n",
      "正在处理： ./npy/lungmask_0002_1149.npy\n",
      "正在处理： ./npy/lungmask_0002_1150.npy\n",
      "正在处理： ./npy/lungmask_0002_1151.npy\n"
     ]
    }
   ],
   "source": [
    "npy_list=glob(luna_path + 'npy/' + \"lungmask_*.npy\")\n",
    "#file_list=glob(working_path+\"lungmask_*.npy\")\n",
    "out_images = []      #final set of images\n",
    "out_nodemasks = []   #final set of nodemasks\n",
    "for fname in npy_list:\n",
    "    print u'正在处理：', fname\n",
    "    imgs_to_process = np.load(fname.replace(\"lungmask\",\"images\"))\n",
    "    masks = np.load(fname)\n",
    "    node_masks = np.load(fname.replace(\"lungmask\",\"masks\"))\n",
    "    for i in range(len(imgs_to_process)):\n",
    "        mask = masks[i]\n",
    "        node_mask = node_masks[i]\n",
    "        img = imgs_to_process[i]\n",
    "        new_size = [512,512]   \n",
    "        img= mask*img          \n",
    "        new_mean = np.mean(img[mask>0])  \n",
    "        new_std = np.std(img[mask>0])\n",
    "        old_min = np.min(img)       \n",
    "        img[img==old_min] = new_mean-1.2*new_std   \n",
    "        img = img-new_mean\n",
    "        img = img/new_std\n",
    "        labels = measure.label(mask)\n",
    "        regions = measure.regionprops(labels)\n",
    "        min_row = 512\n",
    "        max_row = 0\n",
    "        min_col = 512\n",
    "        max_col = 0\n",
    "        for prop in regions:\n",
    "            B = prop.bbox\n",
    "            if min_row > B[0]:\n",
    "                min_row = B[0]\n",
    "            if min_col > B[1]:\n",
    "                min_col = B[1]\n",
    "            if max_row < B[2]:\n",
    "                max_row = B[2]\n",
    "            if max_col < B[3]:\n",
    "                max_col = B[3]\n",
    "        width = max_col-min_col\n",
    "        height = max_row - min_row\n",
    "        if width > height:\n",
    "            max_row=min_row+width\n",
    "        else:\n",
    "            max_col = min_col+height\n",
    "        img = img[min_row:max_row,min_col:max_col]\n",
    "        mask =  mask[min_row:max_row,min_col:max_col]\n",
    "        if max_row-min_row <5 or max_col-min_col<5:  \n",
    "            pass\n",
    "        else:\n",
    "            mean = np.mean(img)\n",
    "            img = img - mean\n",
    "            min = np.min(img)\n",
    "            max = np.max(img)\n",
    "            img = img/(max-min)\n",
    "            new_img = resize(img,[512,512])\n",
    "            new_node_mask = resize(node_mask[min_row:max_row,min_col:max_col],[512,512])\n",
    "            out_images.append(new_img)\n",
    "            out_nodemasks.append(new_node_mask)\n",
    "num_images = len(out_images)\n",
    "final_images = np.ndarray([num_images,1,512,512],dtype=np.float32)\n",
    "final_masks = np.ndarray([num_images,1,512,512],dtype=np.float32)\n",
    "for i in range(num_images):\n",
    "    final_images[i,0] = out_images[i]\n",
    "    final_masks[i,0] = out_nodemasks[i]\n",
    "rand_i = np.random.choice(range(num_images),size=num_images,replace=False)\n",
    "test_i = int(0.2*num_images)\n",
    "np.save(working_path+\"trainImages.npy\",final_images[rand_i[test_i:]])\n",
    "np.save(working_path+\"trainMasks.npy\",final_masks[rand_i[test_i:]])\n",
    "np.save(working_path+\"testImages.npy\",final_images[rand_i[:test_i]])\n",
    "np.save(working_path+\"testMasks.npy\",final_masks[rand_i[:test_i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Loading and preprocessing train data...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Creating and compiling model...\n",
      "------------------------------\n",
      "------------------------------\n",
      "Fitting model...\n",
      "------------------------------\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[1,768,64,64]\n\t [[Node: merge_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](up_sampling2d_1/transpose_1, conv2d_8/Relu, merge_1/concat/axis)]]\n\t [[Node: Mean_3/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_615_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'merge_1/concat', defined at:\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-a03b732dd258>\", line 149, in <module>\n    train_and_predict(False)\n  File \"<ipython-input-4-a03b732dd258>\", line 102, in train_and_predict\n    model = get_unet()\n  File \"<ipython-input-4-a03b732dd258>\", line 58, in get_unet\n    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 456, in merge\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 117, in __init__\n    self(input_tensors, mask=input_masks)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/engine/topology.py\", line 554, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 210, in call\n    return K.concatenate(inputs, axis=self.concat_axis)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1525, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1034, in concat\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,768,64,64]\n\t [[Node: merge_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](up_sampling2d_1/transpose_1, conv2d_8/Relu, merge_1/concat/axis)]]\n\t [[Node: Mean_3/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_615_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a03b732dd258>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mtrain_and_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-a03b732dd258>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(use_existing)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     model.fit(imgs_train, imgs_mask_train, batch_size=1, nb_epoch=1, verbose=1, shuffle=True,\n\u001b[0;32m--> 126\u001b[0;31m               callbacks=[model_checkpoint])\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;31m# loading best weights from training session\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1483\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1484\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1485\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1487\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1138\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1140\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1141\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2071\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 2073\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1035\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1036\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[1,768,64,64]\n\t [[Node: merge_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](up_sampling2d_1/transpose_1, conv2d_8/Relu, merge_1/concat/axis)]]\n\t [[Node: Mean_3/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_615_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n\nCaused by op u'merge_1/concat', defined at:\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 174, in _run_module_as_main\n    \"__main__\", fname, loader, pkg_name)\n  File \"/Users/mahui/anaconda/lib/python2.7/runpy.py\", line 72, in _run_code\n    exec code in run_globals\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n    app.launch_new_instance()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelapp.py\", line 477, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-4-a03b732dd258>\", line 149, in <module>\n    train_and_predict(False)\n  File \"<ipython-input-4-a03b732dd258>\", line 102, in train_and_predict\n    model = get_unet()\n  File \"<ipython-input-4-a03b732dd258>\", line 58, in get_unet\n    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 456, in merge\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 117, in __init__\n    self(input_tensors, mask=input_masks)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/engine/topology.py\", line 554, in __call__\n    output = self.call(inputs, **kwargs)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/legacy/layers.py\", line 210, in call\n    return K.concatenate(inputs, axis=self.concat_axis)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.py\", line 1525, in concatenate\n    return tf.concat([to_dense(x) for x in tensors], axis)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/array_ops.py\", line 1034, in concat\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/ops/gen_array_ops.py\", line 519, in _concat_v2\n    name=name)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/op_def_library.py\", line 763, in apply_op\n    op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 2327, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/mahui/anaconda/lib/python2.7/site-packages/tensorflow/python/framework/ops.py\", line 1226, in __init__\n    self._traceback = _extract_stack()\n\nResourceExhaustedError (see above for traceback): OOM when allocating tensor with shape[1,768,64,64]\n\t [[Node: merge_1/concat = ConcatV2[N=2, T=DT_FLOAT, Tidx=DT_INT32, _device=\"/job:localhost/replica:0/task:0/gpu:0\"](up_sampling2d_1/transpose_1, conv2d_8/Relu, merge_1/concat/axis)]]\n\t [[Node: Mean_3/_27 = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/cpu:0\", send_device=\"/job:localhost/replica:0/task:0/gpu:0\", send_device_incarnation=1, tensor_name=\"edge_615_Mean_3\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/cpu:0\"]()]]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, merge, Convolution2D, MaxPooling2D, UpSampling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n",
    "K.set_image_dim_ordering('th')  # Theano dimension ordering in this code\n",
    "\n",
    "img_rows = 512\n",
    "img_cols = 512\n",
    "\n",
    "smooth = 1.\n",
    "\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (np.sum(y_true_f) + np.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "\n",
    "def get_unet():\n",
    "    inputs = Input((1,img_rows, img_cols))\n",
    "    conv1 = Convolution2D(32, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv1 = Convolution2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(64, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv2 = Convolution2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(128, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv3 = Convolution2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(256, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv4 = Convolution2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(512, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv5 = Convolution2D(512, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = Convolution2D(256, (3, 3), padding=\"same\", activation=\"relu\")(up6)\n",
    "    conv6 = Convolution2D(256, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = Convolution2D(128, (3, 3), padding=\"same\", activation=\"relu\")(up7)\n",
    "    conv7 = Convolution2D(128, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = Convolution2D(64, (3, 3), padding=\"same\", activation=\"relu\")(up8)\n",
    "    conv8 = Convolution2D(64, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = Convolution2D(32, (3, 3), padding=\"same\", activation=\"relu\")(up9)\n",
    "    conv9 = Convolution2D(32, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "\n",
    "    conv10 = Convolution2D(1, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=1.0e-5), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_and_predict(use_existing):\n",
    "    print('-'*30)\n",
    "    print('Loading and preprocessing train data...')\n",
    "    print('-'*30)\n",
    "    imgs_train = np.load(working_path+\"trainImages.npy\").astype(np.float32)\n",
    "    imgs_mask_train = np.load(working_path+\"trainMasks.npy\").astype(np.float32)\n",
    "\n",
    "    imgs_test = np.load(working_path+\"testImages.npy\").astype(np.float32)\n",
    "    imgs_mask_test_true = np.load(working_path+\"testMasks.npy\").astype(np.float32)\n",
    "    \n",
    "    mean = np.mean(imgs_train)  # mean for data centering\n",
    "    std = np.std(imgs_train)  # std for data normalization\n",
    "\n",
    "    imgs_train -= mean  # images should already be standardized, but just in case\n",
    "    imgs_train /= std\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Creating and compiling model...')\n",
    "    print('-'*30)\n",
    "    model = get_unet()\n",
    "    # Saving weights to unet.hdf5 at checkpoints\n",
    "    model_checkpoint = ModelCheckpoint('unet.hdf5', monitor='loss', save_best_only=True)\n",
    "    #\n",
    "    # Should we load existing weights? \n",
    "    # Set argument for call to train_and_predict to true at end of script\n",
    "    if use_existing:\n",
    "        model.load_weights('./unet.hdf5')\n",
    "        \n",
    "    # \n",
    "    # The final results for this tutorial were produced using a multi-GPU\n",
    "    # machine using TitanX's.\n",
    "    # For a home GPU computation benchmark, on my home set up with a GTX970 \n",
    "    # I was able to run 20 epochs with a training set size of 320 and \n",
    "    # batch size of 2 in about an hour. I started getting reseasonable masks \n",
    "    # after about 3 hours of training. \n",
    "    #\n",
    "    print('-'*30)\n",
    "    print('Fitting model...')\n",
    "    print('-'*30)\n",
    "    \n",
    "    #原代码batch_size=2, nb_epoch=20，我显存不够，所以改cpu跑\n",
    "    \n",
    "    model.fit(imgs_train, imgs_mask_train, batch_size=1, nb_epoch=1, verbose=1, shuffle=True,\n",
    "              callbacks=[model_checkpoint])\n",
    "\n",
    "    # loading best weights from training session\n",
    "    print('-'*30)\n",
    "    print('Loading saved weights...')\n",
    "    print('-'*30)\n",
    "    model.load_weights('./unet.hdf5')\n",
    "\n",
    "    print('-'*30)\n",
    "    print('Predicting masks on test data...')\n",
    "    print('-'*30)\n",
    "    num_test = len(imgs_test)\n",
    "    imgs_mask_test = np.ndarray([num_test,1,512,512],dtype=np.float32)\n",
    "    for i in range(num_test):\n",
    "        imgs_mask_test[i] = model.predict([imgs_test[i:i+1]], verbose=0)[0]\n",
    "    np.save('masksTestPredicted.npy', imgs_mask_test)\n",
    "    mean = 0.0\n",
    "    for i in range(num_test):\n",
    "        mean+=dice_coef_np(imgs_mask_test_true[i,0], imgs_mask_test[i,0])\n",
    "    mean/=num_test\n",
    "    print(\"Mean Dice Coeff : \",mean)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_and_predict(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Predicted = np.load('masksTestPredicted.npy')\n",
    "#plt.imshow(aa[2,0,:,:])\n",
    "fig,ax = plt.subplots(len(Predicted),1,figsize=[30,30])\n",
    "for i in range(len(Predicted)):     \n",
    "    ax[i].imshow(Predicted[i,0,:,:])\n",
    "    ax[i].set_title(Predicted[i,0,0,0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 其他参考方案\n",
    "- https://www.kaggle.com/arnavkj95/data-science-bowl-2017/candidate-generation-and-luna16-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
