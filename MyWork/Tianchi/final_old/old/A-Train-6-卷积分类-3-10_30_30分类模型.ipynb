{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_dirfiles(dir):\n",
    "    file_list = []\n",
    "    subset_path = os.listdir(dir)\n",
    "    for _ in range(len(subset_path)):\n",
    "        if subset_path[_] != '.DS_Store':\n",
    "            file_list.append(dir + subset_path[_])\n",
    "    return file_list\n",
    "\n",
    "def train_generator(output_true,output_false):\n",
    "    file_list_true = get_dirfiles(output_true)[0:19]\n",
    "    file_list_false = get_dirfiles(output_false)[0:19]\n",
    "    \n",
    "    file_list_true = shuffle(file_list_true)\n",
    "    file_list_false = shuffle(file_list_false)\n",
    "    \n",
    "    nb_true = len(file_list_true) + len(file_list_false)\n",
    "    sample = np.zeros([nb_true,10,30,30])\n",
    "    labels = np.zeros([nb_true,2])\n",
    "    for i in tqdm(range(len(file_list_true))):       \n",
    "        cc= np.load(file_list_true[i])\n",
    "        sample[i] = cc\n",
    "        labels[i] = [0.,1.]\n",
    "    for j in tqdm(range(len(file_list_false))):\n",
    "        bb= np.load(file_list_false[j])\n",
    "        sample[j+len(file_list_true)] = bb \n",
    "        labels[j+len(file_list_true)] = [1.,0.]\n",
    "    sample = np.expand_dims(sample, axis=1)        \n",
    "    return sample,labels\n",
    "\n",
    "def valid_generator(output_true,output_false):\n",
    "    file_list_true = get_dirfiles(output_true)[-3:]\n",
    "    file_list_false = get_dirfiles(output_false)[-3:]\n",
    "    \n",
    "    file_list_true = shuffle(file_list_true)\n",
    "    file_list_false = shuffle(file_list_false)\n",
    "\n",
    "    nb_true = len(file_list_true) + len(file_list_false)\n",
    "    sample = np.zeros([nb_true,10,30,30])\n",
    "    labels = np.zeros([nb_true,2])\n",
    "  \n",
    "    for i in tqdm(range(len(file_list_true))):       \n",
    "        cc= np.load(file_list_true[i])\n",
    "        sample[i] = cc\n",
    "        labels[i] = [0.,1.]\n",
    "    for j in tqdm(range(len(file_list_false))):\n",
    "        bb= np.load(file_list_false[j])\n",
    "        sample[j+len(file_list_true)] = bb \n",
    "        labels[j+len(file_list_true)] = [1.,0.]\n",
    "    sample = np.expand_dims(sample, axis=1)        \n",
    "    return sample,labels\n",
    "\n",
    "def model_30():    \n",
    "\n",
    "    inputs = Input(shape=(1, 10, 30, 30))\n",
    "    \n",
    "    conv1 = Convolution3D(48, 3, 5, 5, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(48, 1, 2, 2, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(48, 3, 5, 5, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(48, 3, 5, 5, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)    \n",
    "    \n",
    "    output = Flatten(name='flatten')(conv1)\n",
    "    output = Dense(250)(output)\n",
    "    output = PReLU()(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dense(2, activation='softmax', name = 'predictions')(output)\n",
    "    model3d = Model(inputs, output)\n",
    "    model3d.compile(loss='categorical_crossentropy', optimizer = Adam(lr=1e-5), metrics = ['accuracy'])\n",
    "    return model3d\n",
    "\n",
    "\n",
    "def fenlei_fit(name, load_check = False,batch_size=2, epochs=100,check_name = None):\n",
    "\n",
    "    t = time.time()\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience = 20, verbose = 1),\n",
    "                 ModelCheckpoint((model_paths + '{}.h5').format(name),\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose = 0,\n",
    "                                 save_best_only = True)]\n",
    "    if load_check:\n",
    "        check_model = (model_paths + '{}.h5').format(check_name)\n",
    "        model = load_model(check_model)\n",
    "    else:\n",
    "        model = model_30()\n",
    "    x,y = train_generator(output_true,output_false)\n",
    "    model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=valid_generator(output_true,output_false),verbose=1, callbacks=callbacks, shuffle=True) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_path = PATH['cls_train_30']\n",
    "output_true = PATH['cls_train_30_true']\n",
    "output_false = PATH['cls_train_30_false']\n",
    "model_paths = PATH['model_paths']\n",
    "model_final = PATH['model_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19/19 [00:00<00:00, 60.61it/s]\n",
      "100%|██████████| 19/19 [00:00<00:00, 2163.48it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 165.02it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00, 208.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 38 samples, validate on 6 samples\n",
      "Epoch 1/100\n",
      "38/38 [==============================] - 24s - loss: 0.6932 - acc: 0.4737 - val_loss: 0.7311 - val_acc: 0.5000\n",
      "Epoch 2/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 0.8673 - val_acc: 0.5000\n",
      "Epoch 3/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 1.0795 - val_acc: 0.5000\n",
      "Epoch 4/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.3684 - val_loss: 1.3133 - val_acc: 0.5000\n",
      "Epoch 5/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 1.5529 - val_acc: 0.5000\n",
      "Epoch 6/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4737 - val_loss: 1.7569 - val_acc: 0.5000\n",
      "Epoch 7/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 1.9484 - val_acc: 0.5000\n",
      "Epoch 8/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4211 - val_loss: 2.0536 - val_acc: 0.5000\n",
      "Epoch 9/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 2.1265 - val_acc: 0.5000\n",
      "Epoch 10/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.3684 - val_loss: 2.0885 - val_acc: 0.6667\n",
      "Epoch 11/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 2.2563 - val_acc: 0.6667\n",
      "Epoch 12/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 2.4910 - val_acc: 0.6667\n",
      "Epoch 13/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 2.6975 - val_acc: 0.6667\n",
      "Epoch 14/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4211 - val_loss: 2.8917 - val_acc: 0.6667\n",
      "Epoch 15/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4737 - val_loss: 3.3674 - val_acc: 0.5000\n",
      "Epoch 16/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 4.0393 - val_acc: 0.5000\n",
      "Epoch 17/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4211 - val_loss: 4.6579 - val_acc: 0.3333\n",
      "Epoch 18/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.4474 - val_loss: 5.3435 - val_acc: 0.3333\n",
      "Epoch 19/100\n",
      "38/38 [==============================] - 5s - loss: 0.6932 - acc: 0.5000 - val_loss: 5.8384 - val_acc: 0.3333\n",
      "Epoch 20/100\n",
      "37/38 [============================>.] - ETA: 0s - loss: 0.6932 - acc: 0.3784"
     ]
    }
   ],
   "source": [
    "fenlei_fit('Fenge_10_30_30_0617_2', load_check = False, batch_size=1, epochs=100, check_name = 'Fenge_10_30_30_0617')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_true = get_dirfiles(output_true)\n",
    "file_list_false = get_dirfiles(output_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_pred = classifier((1, 36, 36, 36), (3, 3, 3), (2, 2, 2))\n",
    "model_pred = load_model(model_paths + 'Fenge_10_30_30_0617_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = []\n",
    "for i in file_list_false[0:200]:\n",
    "    a=np.load(i)\n",
    "    a=np.expand_dims(a,0)\n",
    "    a=np.expand_dims(a,0)\n",
    "    cc.append(model_pred.predict(a))\n",
    "count = 0\n",
    "for i in cc:\n",
    "    if i[0][0] > 0.9:\n",
    "        count += 1\n",
    "print count*1.0/len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = []\n",
    "for i in file_list_true[0:200]:\n",
    "    a=np.load(i)\n",
    "    a=np.expand_dims(a,0)\n",
    "    a=np.expand_dims(a,0)\n",
    "    cc.append(model_pred.predict(a))\n",
    "count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in cc:\n",
    "    if i[0][1] > 0.9:\n",
    "        count += 1\n",
    "print count*1.0/len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
