{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getRegionFromMap(slice_npy):\n",
    "    #thr = np.where(slice_npy > np.mean(slice_npy),0.,1.0)\n",
    "    label_image = measure.label(slice_npy)\n",
    "    labels = label_image.astype(int)\n",
    "    regions = measure.regionprops(labels)\n",
    "    return regions\n",
    "\n",
    "def getRegionMetricRow(fname):\n",
    "    # fname, numpy array of dimension [#slices, 1, 512, 512] containing the images\n",
    "    \n",
    "    seg = np.load(fname)\n",
    "    nslices = seg.shape[0]\n",
    "    \n",
    "    #metrics\n",
    "    totalArea = 0.\n",
    "    avgArea = 0.\n",
    "    maxArea = 0.\n",
    "    avgEcc = 0.\n",
    "    avgEquivlentDiameter = 0.\n",
    "    stdEquivlentDiameter = 0.\n",
    "    weightedX = 0.\n",
    "    weightedY = 0.\n",
    "    numNodes = 0.\n",
    "    numNodesperSlice = 0.\n",
    "    # crude hueristic to filter some bad segmentaitons\n",
    "    # do not allow any nodes to be larger than 10% of the pixels to eliminate background regions\n",
    "    maxAllowedArea = 0.10 * 512 * 512 \n",
    "    \n",
    "    areas = []\n",
    "    eqDiameters = []\n",
    "    seg = np.expand_dims(seg,1)\n",
    "    for slicen in range(nslices):\n",
    "        regions = getRegionFromMap(seg[slicen,0,:,:])\n",
    "        for region in regions:\n",
    "            if region.area > maxAllowedArea:\n",
    "                continue\n",
    "            totalArea += region.area\n",
    "            areas.append(region.area)\n",
    "            avgEcc += region.eccentricity\n",
    "            avgEquivlentDiameter += region.equivalent_diameter\n",
    "            eqDiameters.append(region.equivalent_diameter)\n",
    "            weightedX += region.centroid[0]*region.area\n",
    "            weightedY += region.centroid[1]*region.area\n",
    "            numNodes += 1\n",
    "    if totalArea == 0 or numNodes == 0:\n",
    "        os.remove(fname)\n",
    "    else:\n",
    "        weightedX = weightedX / totalArea \n",
    "        weightedY = weightedY / totalArea\n",
    "        avgArea = totalArea / numNodes\n",
    "        avgEcc = avgEcc / numNodes\n",
    "        avgEquivlentDiameter = avgEquivlentDiameter / numNodes\n",
    "        stdEquivlentDiameter = np.std(eqDiameters)\n",
    "\n",
    "        maxArea = max(areas)\n",
    "\n",
    "\n",
    "        numNodesperSlice = numNodes*1. / nslices\n",
    "\n",
    "\n",
    "        return np.array([avgArea,maxArea,avgEcc,avgEquivlentDiameter,\\\n",
    "                         stdEquivlentDiameter, weightedX, weightedY, numNodes, numNodesperSlice])\n",
    "\n",
    "def createFeatureDataset(nodfiles_true,nodfiles_false):\n",
    "    numfeatures = 9\n",
    "    feature_array = np.zeros((len(nodfiles_true)+len(nodfiles_false),numfeatures))\n",
    "    truth_metric = np.zeros((len(nodfiles_true)+len(nodfiles_false)))\n",
    "    \n",
    "    for i,nodfile in enumerate(tqdm(nodfiles_true)):\n",
    "        patID = nodfile.split(\"_\")[3][-10:]\n",
    "        truth_metric[i] = 1\n",
    "        feature_array[i] = getRegionMetricRow(nodfiles_true[i])\n",
    "    for i,nodfile in enumerate(tqdm(nodfiles_false)):\n",
    "        patID = nodfile.split(\"_\")[3][-10:]\n",
    "        truth_metric[len(nodfiles_true)+i] = 0\n",
    "        feature_array[len(nodfiles_true)+i] = getRegionMetricRow(nodfiles_false[i])   \n",
    "    np.save(\"dataY.npy\", truth_metric)\n",
    "    np.save(\"dataX.npy\", feature_array)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csv_path = PATH['annotations_test']\n",
    "src = PATH['model_test_pred']\n",
    "pred_csv_path = PATH['model_test_pred']\n",
    "data_path = PATH['src_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_paths = PATH['model_paths']\n",
    "model_final = PATH['model_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = joblib.load('classifymodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_0 = pd.read_csv(pred_csv_path + \"1final_test_result.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patients = [x for x in os.listdir(pred_csv_path) if 'orig' in x]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_0[\"file\"] = test_pred_0[\"seriesuid\"].map(lambda file_name: get_filename(patients, file_name))\n",
    "test_pred_0 = test_pred_0.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 800/800 [50:09<00:00,  3.76s/it]  \n"
     ]
    }
   ],
   "source": [
    "probability_30_30_30_cube = []\n",
    "\n",
    "average = []\n",
    "\n",
    "for img_file in tqdm(sorted(patients)):\n",
    "    mini_df_anno = test_pred_0[test_pred_0[\"file\"]==img_file] #get all nodules associate with file\n",
    "    if mini_df_anno.shape[0]>0: # some files may not have a nodule--skipping those \n",
    "        # load the data once        \n",
    "        patient_id = img_file[:-9]\n",
    "        img_array = np.load(src + img_file)\n",
    "        pos_annos = pd.read_csv(src + img_file[:-9] + '_annos_pos.csv')\n",
    "        origin = np.array([pos_annos.loc[0]['origin_x'],pos_annos.loc[0]['origin_y'],pos_annos.loc[0]['origin_z']]) \n",
    "        spacing = np.array([pos_annos.loc[0]['spacing_x'],pos_annos.loc[0]['spacing_y'],pos_annos.loc[0]['spacing_z']])\n",
    "        img_array = normalize(img_array)                \n",
    "        for node_idx1, cur_row1 in mini_df_anno.iterrows():       \n",
    "            node_x = cur_row1[\"coordX\"]\n",
    "            node_y = cur_row1[\"coordY\"]\n",
    "            node_z = cur_row1[\"coordZ\"]\n",
    "            diam = cur_row1[\"diameter_mm\"]\n",
    "            center = np.array([node_x, node_y, node_z])   # nodule center\n",
    "            v_center = np.rint(np.absolute(center-origin)/spacing)            \n",
    "            new_x = int(v_center[0])\n",
    "            new_y = int(v_center[1])\n",
    "            new_z = int(v_center[2])\n",
    "            \n",
    "            if new_z<18 or new_x<18 or new_y<18 or new_x+18>img_array.shape[2] or new_y+18>img_array.shape[1] or new_z+18>img_array.shape[0]:\n",
    "                cls_result_cube_30 = int(0)\n",
    "            else:\n",
    "                seg =  img_array[new_z - 18: new_z + 18,\n",
    "                                    new_y - 18 : new_y + 18,\n",
    "                                    new_x - 18 : new_x + 18]    \n",
    "\n",
    "                nslices = seg.shape[0]\n",
    "\n",
    "                #metrics\n",
    "                totalArea = 0.\n",
    "                avgArea = 0.\n",
    "                maxArea = 0.\n",
    "                avgEcc = 0.\n",
    "                avgEquivlentDiameter = 0.\n",
    "                stdEquivlentDiameter = 0.\n",
    "                weightedX = 0.\n",
    "                weightedY = 0.\n",
    "                numNodes = 0.\n",
    "                numNodesperSlice = 0.\n",
    "                # crude hueristic to filter some bad segmentaitons\n",
    "                # do not allow any nodes to be larger than 10% of the pixels to eliminate background regions\n",
    "                maxAllowedArea = 0.10 * 512 * 512 \n",
    "\n",
    "                areas = []\n",
    "                eqDiameters = []\n",
    "                seg = np.expand_dims(seg,1)\n",
    "                for slicen in range(nslices):\n",
    "                    regions = getRegionFromMap(seg[slicen,0,:,:])\n",
    "                    for region in regions:\n",
    "                        if region.area > maxAllowedArea:\n",
    "                            continue\n",
    "                        totalArea += region.area\n",
    "                        areas.append(region.area)\n",
    "                        avgEcc += region.eccentricity\n",
    "                        avgEquivlentDiameter += region.equivalent_diameter\n",
    "                        eqDiameters.append(region.equivalent_diameter)\n",
    "                        weightedX += region.centroid[0]*region.area\n",
    "                        weightedY += region.centroid[1]*region.area\n",
    "                        numNodes += 1\n",
    "                if totalArea == 0 or numNodes == 0:\n",
    "                    cls_result_cube_30 = int(0)\n",
    "                else:\n",
    "                    weightedX = weightedX / totalArea \n",
    "                    weightedY = weightedY / totalArea\n",
    "                    avgArea = totalArea / numNodes\n",
    "                    avgEcc = avgEcc / numNodes\n",
    "                    avgEquivlentDiameter = avgEquivlentDiameter / numNodes\n",
    "                    stdEquivlentDiameter = np.std(eqDiameters)\n",
    "\n",
    "                    maxArea = max(areas)\n",
    "\n",
    "\n",
    "                    numNodesperSlice = numNodes*1. / nslices\n",
    "\n",
    "\n",
    "                    feature =  np.array([avgArea,maxArea,avgEcc,avgEquivlentDiameter,\\\n",
    "                                     stdEquivlentDiameter, weightedX, weightedY, numNodes, numNodesperSlice])\n",
    "                    cls_result_cube_30 = clf.predict_proba(np.expand_dims(feature,0))[0][1]\n",
    "            probability_30_30_30_cube.append(cls_result_cube_30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probability_30_30_30_cube = np.array(probability_30_30_30_cube)\n",
    "probability_30_30_30_cube = probability_30_30_30_cube.clip(0.005,0.995)\n",
    "probability_30_30_30_cube = probability_30_30_30_cube.round(3)\n",
    "test_pred_0['probability'] = probability_30_30_30_cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_pred_0.to_csv(csv_path + \"0625final.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
