{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_20():    \n",
    "    learning_rate = 5e-5\n",
    "    #optimizer = SGD(lr=learning_rate, momentum = 0.9, decay = 1e-3, nesterov = True)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    inputs = Input(shape=(1, 20, 20, 6))\n",
    "    \n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 1, 1, 1, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 1, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)    \n",
    "    \n",
    "    output = Flatten(name='flatten')(conv1)\n",
    "    output = Dense(150)(output)\n",
    "    output = PReLU()(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dense(2, activation='softmax', name = 'predictions')(output)\n",
    "    model3d = Model(inputs, output)\n",
    "    model3d.compile(loss='binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model3d\n",
    "\n",
    "def model_30():    \n",
    "    learning_rate = 5e-5\n",
    "    #optimizer = SGD(lr=learning_rate, momentum = 0.9, decay = 1e-3, nesterov = True)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    inputs = Input(shape=(1, 20, 20, 6))\n",
    "    \n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 2, 2, 1, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)    \n",
    "    \n",
    "    output = Flatten(name='flatten')(conv1)\n",
    "    output = Dense(250)(output)\n",
    "    output = PReLU()(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dense(2, activation='softmax', name = 'predictions')(output)\n",
    "    model3d = Model(inputs, output)\n",
    "    model3d.compile(loss='binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model3d\n",
    "\n",
    "def model_40():    \n",
    "    learning_rate = 5e-5\n",
    "    #optimizer = SGD(lr=learning_rate, momentum = 0.9, decay = 1e-3, nesterov = True)\n",
    "    optimizer = Adam(lr=learning_rate)\n",
    "    \n",
    "    inputs = Input(shape=(1, 20, 20, 6))\n",
    "    \n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 2, 2, 2, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution3D(64, 5, 5, 3, activation = 'relu', border_mode='same')(conv1)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)    \n",
    "    \n",
    "    output = Flatten(name='flatten')(conv1)\n",
    "    output = Dense(250)(output)\n",
    "    output = PReLU()(output)\n",
    "    output = BatchNormalization()(output)\n",
    "    output = Dense(2, activation='softmax', name = 'predictions')(output)\n",
    "    model3d = Model(inputs, output)\n",
    "    model3d.compile(loss='binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])\n",
    "    return model3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing: \n",
    "- 1) Data augmentation - translated by 1 voxel along each axis and rotated 90, 180 and 270 degrees with the transverse plane. In total, 0.65 million samples generated for training. \n",
    "- 2) Normalization - clipped the intensities into the interval (-1000,400) HU and normalized them to the range of (0,1).\n",
    "\n",
    "### 3D CNN architecture details: \n",
    "- Learning from Scratch, lr=0.3 and decayed by 5% every 5000 iterations. batchsize=200, momentum=0.9, and the dropout rate=0.2 stragety is utilized in C and FC layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_dir = '/Volumes/solo/ali/pic/train/'\n",
    "nb_train_samples = 1800\n",
    "nb_validation_samples = 500\n",
    "epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(20, 20, 6),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(20, 20, 6),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_steps=nb_validation_samples // batch_size)\n",
    "\n",
    "model.save('/Volumes/solo/ali/Data/model/my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
