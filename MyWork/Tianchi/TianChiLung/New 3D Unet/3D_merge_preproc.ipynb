{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import csv\n",
    "import scipy\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "from scipy import ndimage\n",
    "from tqdm import tqdm \n",
    "import pandas as pd\n",
    "import skimage, os\n",
    "from skimage.morphology import ball, disk, dilation, binary_erosion, remove_small_objects, erosion, closing, reconstruction, binary_closing\n",
    "from skimage.measure import label,regionprops, perimeter\n",
    "from skimage.morphology import binary_dilation, binary_opening\n",
    "from skimage.filters import roberts, sobel\n",
    "from skimage import measure, feature\n",
    "from skimage.segmentation import clear_border\n",
    "from skimage import data\n",
    "from scipy import ndimage\n",
    "from mpl_toolkits.mplot3d.art3d import Poly3DCollection\n",
    "import dicom\n",
    "import scipy.misc\n",
    "import numpy as np\n",
    "from skimage import measure, morphology, segmentation\n",
    "import matplotlib.pyplot as plt\n",
    "from joblib import Parallel, delayed\n",
    "from numba import autojit\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from utils_3d import *\n",
    "from paths import *\n",
    "dir_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def load_train():\n",
    "    data_path = src\n",
    "    folders = [x for x in os.listdir(data_path) if 'subset' in x]\n",
    "    os.chdir(data_path)\n",
    "    patients = []\n",
    "    \n",
    "    for i in folders:\n",
    "        os.chdir(data_path + i)\n",
    "        #print('Changing folder to: {}'.format(data_path + i))\n",
    "        patient_ids = [x for x in os.listdir(data_path + i) if '.mhd' in x]\n",
    "        for id in patient_ids:\n",
    "            j = '{}/{}'.format(i, id)\n",
    "            patients.append(j)\n",
    "    return patients\n",
    "\n",
    "def get_filename(file_list, case):\n",
    "    for f in file_list:\n",
    "        if case in f:\n",
    "            return(f)\n",
    "        \n",
    "        \n",
    "def plot_ct_scan(scan):\n",
    "    f, plots = plt.subplots(int(scan.shape[0] / 20) + 1, 4, figsize=(25, 25))\n",
    "    for i in range(0, scan.shape[0], 5):\n",
    "        plots[int(i / 20), int((i % 20) / 5)].axis('off')\n",
    "        plots[int(i / 20), int((i % 20) / 5)].imshow(scan[i], cmap=plt.cm.bone) \n",
    "         \n",
    "def print_mask(lung_m, nodule_m):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(20,16))\n",
    "    ax[0].imshow(lung_m, cmap = plt.cm.bone)\n",
    "    ax[1].imshow(nodule_m, cmap = plt.cm.bone)\n",
    "    return\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "def seq(start, stop, step=1):\n",
    "    n = int(round((stop - start)/float(step)))\n",
    "    if n > 1:\n",
    "        return([start + step*i for i in range(n+1)])\n",
    "    else:\n",
    "        return([])\n",
    "    \n",
    "    \n",
    "def load_itk(filename):\n",
    "    itkimage = sitk.ReadImage(filename)\n",
    "    numpyImage = sitk.GetArrayFromImage(itkimage)\n",
    "    numpyOrigin = np.array(list(reversed(itkimage.GetOrigin())))\n",
    "    numpySpacing = np.array(list(reversed(itkimage.GetSpacing())))\n",
    "    return numpyImage, numpyOrigin, numpySpacing\n",
    "\n",
    "def world_2_voxel(world_coordinates, origin, spacing):\n",
    "    stretched_voxel_coordinates = np.absolute(world_coordinates - origin)\n",
    "    voxel_coordinates = stretched_voxel_coordinates / spacing\n",
    "    return voxel_coordinates\n",
    "\n",
    "def voxel_2_world(voxel_coordinates, origin, spacing):\n",
    "    stretched_voxel_coordinates = voxel_coordinates * spacing\n",
    "    world_coordinates = stretched_voxel_coordinates + origin\n",
    "    return world_coordinates\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "def generate_markers(image):\n",
    "    #Creation of the internal Marker\n",
    "    marker_internal = image < -400\n",
    "    marker_internal = segmentation.clear_border(marker_internal)\n",
    "    marker_internal_labels = measure.label(marker_internal)\n",
    "    areas = [r.area for r in measure.regionprops(marker_internal_labels)]\n",
    "    areas.sort()\n",
    "    if len(areas) > 2:\n",
    "        for region in measure.regionprops(marker_internal_labels):\n",
    "            if region.area < areas[-2]:\n",
    "                for coordinates in region.coords:                \n",
    "                       marker_internal_labels[coordinates[0], coordinates[1]] = 0\n",
    "    marker_internal = marker_internal_labels > 0\n",
    "    #Creation of the external Marker\n",
    "    external_a = ndimage.binary_dilation(marker_internal, iterations=10)\n",
    "    external_b = ndimage.binary_dilation(marker_internal, iterations=55)\n",
    "    marker_external = external_b ^ external_a\n",
    "    #Creation of the Watershed Marker matrix\n",
    "    marker_watershed = np.zeros(image.shape, dtype=np.int)\n",
    "    marker_watershed += marker_internal * 255\n",
    "    marker_watershed += marker_external * 128\n",
    "    return marker_internal, marker_external, marker_watershed\n",
    "\n",
    "\n",
    "def get_segmented_lungs(image):\n",
    "    #Creation of the markers as shown above:\n",
    "    marker_internal, marker_external, marker_watershed = generate_markers(image)\n",
    "    \n",
    "    #Creation of the Sobel-Gradient\n",
    "    sobel_filtered_dx = ndimage.sobel(image, 1)\n",
    "    sobel_filtered_dy = ndimage.sobel(image, 0)\n",
    "    sobel_gradient = np.hypot(sobel_filtered_dx, sobel_filtered_dy)\n",
    "    sobel_gradient *= 255.0 / np.max(sobel_gradient)\n",
    "    \n",
    "    #Watershed algorithm\n",
    "    watershed = morphology.watershed(sobel_gradient, marker_watershed)\n",
    "    \n",
    "    #Reducing the image created by the Watershed algorithm to its outline\n",
    "    outline = ndimage.morphological_gradient(watershed, size=(3,3))\n",
    "    outline = outline.astype(bool)\n",
    "    \n",
    "    #Performing Black-Tophat Morphology for reinclusion\n",
    "    #Creation of the disk-kernel and increasing its size a bit\n",
    "    blackhat_struct = [[0, 0, 1, 1, 1, 0, 0],\n",
    "                       [0, 1, 1, 1, 1, 1, 0],\n",
    "                       [1, 1, 1, 1, 1, 1, 1],\n",
    "                       [1, 1, 1, 1, 1, 1, 1],\n",
    "                       [1, 1, 1, 1, 1, 1, 1],\n",
    "                       [0, 1, 1, 1, 1, 1, 0],\n",
    "                       [0, 0, 1, 1, 1, 0, 0]]\n",
    "    #blackhat_struct = ndimage.iterate_structure(blackhat_struct, 8)\n",
    "    blackhat_struct = ndimage.iterate_structure(blackhat_struct, 14) # <- retains more of the area, 12 works well. Changed to 14, 12 still excluded some parts.\n",
    "    #Perform the Black-Hat\n",
    "    outline += ndimage.black_tophat(outline, structure=blackhat_struct)\n",
    "    \n",
    "    #Use the internal marker and the Outline that was just created to generate the lungfilter\n",
    "    lungfilter = np.bitwise_or(marker_internal, outline)\n",
    "    #Close holes in the lungfilter\n",
    "    #fill_holes is not used here, since in some slices the heart would be reincluded by accident\n",
    "    lungfilter = ndimage.morphology.binary_closing(lungfilter, structure=np.ones((5,5)), iterations=3)\n",
    "    \n",
    "    #Apply the lungfilter (note the filtered areas being assigned threshold_min HU)\n",
    "    segmented = np.where(lungfilter == 1, image, threshold_min*np.ones(image.shape))\n",
    "    \n",
    "    #return segmented, lungfilter, outline, watershed, sobel_gradient, marker_internal, marker_external, marker_watershed\n",
    "    return segmented\n",
    "\n",
    "\n",
    "def segment_lung_from_ct_scan(ct_scan):\n",
    "    return np.asarray([get_segmented_lungs(slice) for slice in ct_scan])\n",
    "\n",
    "def get_pixels_hu(image):\n",
    "    image = image.astype(np.int16)\n",
    "    image[image == threshold_min] = 0\n",
    "    return np.array(image, dtype=np.int16)\n",
    "\n",
    "\n",
    "def get_nodule_slices(lung_mask, nodule_mask, lung_raw):\n",
    "    indexes = []\n",
    "    for img_slice in range(nodule_mask.shape[0]):\n",
    "        if np.max(nodule_mask[img_slice]) == 1.0:\n",
    "            indexes.append(img_slice)\n",
    "\n",
    "    #print('Nodule_present on slices: {}'.format(indexes))\n",
    "    lung_mask_pres = lung_mask[indexes, :, :]\n",
    "    nod_mask_pres = nodule_mask[indexes, :, :]\n",
    "    lung_raw_pres = lung_raw[indexes, :, :]\n",
    "    return lung_mask_pres, nod_mask_pres, lung_raw_pres\n",
    "\n",
    "def reshape_3d(image_3d):\n",
    "    reshaped_img = image_3d.reshape([image_3d.shape[0], 1, height_mask, width_mask])\n",
    "    #print('Reshaped image shape:', reshaped_img.shape)\n",
    "    return reshaped_img\n",
    "\n",
    "def create_masks_for_patient_watershed(img_file, save = True):\n",
    "    \n",
    "    def draw_nodule_mask(node_idx, cur_row):\n",
    "\n",
    "        #print('Working on node: {}, row: {}'.format(node_idx, cur_row), '\\n')\n",
    "        coord_x = cur_row[\"coordX\"]\n",
    "        coord_y = cur_row[\"coordY\"]\n",
    "        coord_z = cur_row[\"coordZ\"]\n",
    "        diam = cur_row[\"diameter_mm\"]\n",
    "        radius = np.ceil(diam/2)\n",
    "\n",
    "        noduleRange = seq(-radius, radius, RESIZE_SPACING[0])\n",
    "        #print('Nodule range:', noduleRange)\n",
    "\n",
    "        #imgs = np.zeros([noduleRange, height, width],dtype=np.float32)\n",
    "        #print('imgs shape:', imgs.shape)\n",
    "        #masks = np.zeros([noduleRange, height, width],dtype=np.float32)\n",
    "        #print('masks shape:', masks.shape)\n",
    "\n",
    "        world_center = np.array((coord_z,coord_y,coord_x))   # nodule center\n",
    "        voxel_center = world_2_voxel(world_center, origin, new_spacing)\n",
    "\n",
    "        image_mask = np.zeros(lung_img.shape)\n",
    "\n",
    "        for x in noduleRange:\n",
    "            for y in noduleRange:\n",
    "                for z in noduleRange:\n",
    "                    coords = world_2_voxel(np.array((coord_z+z,coord_y+y,coord_x+x)),origin,new_spacing)\n",
    "                    if (np.linalg.norm(voxel_center - coords) * RESIZE_SPACING[0]) < radius:\n",
    "                        image_mask[int(np.round(coords[0])),int(np.round(coords[1])),int(np.round(coords[2]))] = int(1)\n",
    "        #print(np.max(image_mask))                \n",
    "\n",
    "        return image_mask\n",
    "\n",
    "\n",
    "    #print(\"Getting mask for image file {}\".format(img_file))\n",
    "    patient_id = img_file.split('/')[-1][:-4]\n",
    "    mini_df = df_node[df_node[\"file\"] == img_file]\n",
    "\n",
    "    if mini_df.shape[0] > 0: # some files may not have a nodule--skipping those \n",
    "\n",
    "        img, origin, spacing = load_itk(src + img_file)\n",
    "        height, width = img.shape[1], img.shape[2]\n",
    "\n",
    "        #calculate resize factor\n",
    "        #RESIZE_SPACING = [1, 1, 1]\n",
    "        resize_factor = spacing / RESIZE_SPACING\n",
    "        new_real_shape = img.shape * resize_factor\n",
    "        new_shape = np.round(new_real_shape)\n",
    "        real_resize = new_shape / img.shape\n",
    "        new_spacing = spacing / real_resize\n",
    "\n",
    "        lung_img = scipy.ndimage.interpolation.zoom(img, real_resize)\n",
    "        #print('Original image shape: {}'.format(img.shape))\n",
    "        #print('Resized image shape: {}'.format(lung_img.shape))\n",
    "\n",
    "        \n",
    "        lung_img = get_pixels_hu(lung_img)\n",
    "        #lung_mask = segment_lung_from_ct_scan(lung_img)\n",
    "        #lung_mask[lung_mask >= threshold_max] = threshold_max\n",
    "        #lung_img[lung_img >= threshold_max] = threshold_max\n",
    "        #lung_img[lung_img == 0] = threshold_min\n",
    "        \n",
    "        lung_mask = lung_img.copy()\n",
    "        #lung_mask[lung_mask == 0] = threshold_min\n",
    "        lung_mask[lung_mask >= threshold_max] = threshold_max\n",
    "        lung_img[lung_img >= threshold_max] = threshold_max\n",
    "        \n",
    "        \n",
    "        lung_masks_512 = np.zeros([lung_img.shape[0], height_mask, width_mask], dtype = np.float32)\n",
    "        nodule_masks_512 = np.zeros([lung_img.shape[0], height_mask, width_mask], dtype = np.float32)\n",
    "        lung_masks_512[lung_masks_512 == 0] = threshold_min\n",
    "\n",
    "        i = 0\n",
    "        for node_idx, cur_row in mini_df.iterrows(): \n",
    "\n",
    "            nodule_mask = draw_nodule_mask(node_idx, cur_row)\n",
    "            lung_img_512, lung_mask_512, nodule_mask_512 = np.zeros((lung_img.shape[0], height_mask, width_mask)), \\\n",
    "            np.zeros((lung_mask.shape[0], height_mask, width_mask)), \\\n",
    "            np.zeros((nodule_mask.shape[0], height_mask, width_mask))\n",
    "            lung_mask_512[lung_mask_512 == 0] = threshold_min\n",
    "            lung_img_512[lung_img_512 == 0] = threshold_min\n",
    "            \n",
    "            original_shape = lung_img.shape\n",
    "            for z in range(lung_img.shape[0]):\n",
    "\n",
    "                offset = (height_mask - original_shape[1])\n",
    "                upper_offset = int(np.round(offset/2))\n",
    "                lower_offset = int(offset - upper_offset)\n",
    "\n",
    "                new_origin = voxel_2_world([-upper_offset,-lower_offset,0],origin,new_spacing)\n",
    "                \n",
    "                lung_img_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = lung_img[z,:,:]\n",
    "                lung_mask_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = lung_mask[z,:,:]\n",
    "                nodule_mask_512[z, upper_offset:-lower_offset,upper_offset:-lower_offset] = nodule_mask[z,:,:]\n",
    "                nodule_masks_512 += nodule_mask_512\n",
    "                \n",
    "            #print('Offsets shape for node index {} - main: {}, upper: {}, lower: {}'.format(node_idx, offset, upper_offset, lower_offset), '\\n')\n",
    "\n",
    "        lung_mask_pres = reshape_3d(lung_mask_512)\n",
    "        nod_mask_pres = reshape_3d(nodule_masks_512)\n",
    "        if np.max(nod_mask_pres) == 0:\n",
    "            #print('Nodule lost')\n",
    "        lung_mask_pres[lung_mask_pres <= threshold_min] = threshold_min\n",
    "        lung_mask_pres[lung_mask_pres >= threshold_max] = threshold_max\n",
    "\n",
    "        lung_mask_preproc = my_PreProc(lung_mask_pres)\n",
    "        lung_mask_preproc = lung_mask_preproc.astype(np.float32)\n",
    "        nod_mask_pres = (nod_mask_pres > 0.0).astype(np.float32)\n",
    "        nod_mask_pres[nod_mask_pres == 1.0] = 255.\n",
    "\n",
    "        #np.save('{}/lung_mask/{}_{}'.format(dst_nodules, patient_id, node_idx), lung_mask_pres)\n",
    "        np.save('{}/lung_mask/{}'.format(dst_nodules, patient_id), lung_mask_preproc)\n",
    "        np.save('{}/nodule_mask/{}'.format(dst_nodules, patient_id), nod_mask_pres)\n",
    "\n",
    "        #return lung_mask_pres, nod_mask_pres\n",
    "        return\n",
    "    \n",
    "    else: \n",
    "        #print('\\n', 'No nodules found for patient: {}'.format(patient_id), '\\n')\n",
    "        return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations_path = annotations_train_path\n",
    "src = src_train\n",
    "dst_nodules = mask_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients = load_train()\n",
    "df_node = pd.read_csv(annotations_path+\"annotations.csv\")\n",
    "df_node[\"file\"] = df_node[\"seriesuid\"].map(lambda file_name: get_filename(patients, file_name))\n",
    "df_node = df_node.dropna()\n",
    "\n",
    "RESIZE_SPACING = [4, 4, 4]\n",
    "threshold_min = -1200\n",
    "threshold_max = 400\n",
    "\n",
    "height_mask = 128\n",
    "width_mask = 128\n",
    "\n",
    "Parallel(n_jobs=-1)(delayed(create_masks_for_patient_watershed)(patient) for patient in sorted(patients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "annotations_path = annotations_val_path\n",
    "src = src_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodule lost\n",
      "Nodule lost\n",
      "Nodule lost\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patients = load_train()\n",
    "df_node = pd.read_csv(annotations_path+\"annotations.csv\")\n",
    "df_node[\"file\"] = df_node[\"seriesuid\"].map(lambda file_name: get_filename(patients, file_name))\n",
    "df_node = df_node.dropna()\n",
    "RESIZE_SPACING = [4, 4, 4]\n",
    "threshold_min = -1200\n",
    "threshold_max = 400\n",
    "\n",
    "height_mask = 128\n",
    "width_mask = 128\n",
    "\n",
    "\n",
    "Parallel(n_jobs=-1)(delayed(create_masks_for_patient_watershed)(patient) for patient in sorted(patients))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
