{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from importlib import import_module\n",
    "import shutil\n",
    "import random\n",
    "import sys\n",
    "#sys.path.append('../')\n",
    "\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#from utils.config_training import config as config_training\n",
    "\n",
    "from detector.layers import acc\n",
    "from detector.split_combine import SplitComb\n",
    "from detector import data\n",
    "from detector.utils import *\n",
    "from utils.paths import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttributeDict(dict): \n",
    "    __getattr__ = dict.__getitem__\n",
    "    __setattr__ = dict.__setitem__\n",
    "\n",
    "def main():\n",
    "    global args\n",
    "    args = AttributeDict()\n",
    "    args.batch_size=32\n",
    "    args.epochs=100\n",
    "    args.gpu='all'\n",
    "    args.lr=0.01\n",
    "    args.model='detector.res18'\n",
    "    args.momentum=0.9\n",
    "    args.n_test=8\n",
    "    args.resume=''\n",
    "    args.save_dir='result'\n",
    "    args.save_freq=10\n",
    "    args.split=8\n",
    "    args.start_epoch=0\n",
    "    args.test=0\n",
    "    args.weight_decay=0.0001\n",
    "    args.workers=32\n",
    "    \n",
    "    torch.manual_seed(0)\n",
    "    #torch.cuda.set_device(0)\n",
    "\n",
    "    model = import_module(args.model)\n",
    "    config, net, loss, get_pbb = model.get_model()\n",
    "    start_epoch = args.start_epoch\n",
    "    save_dir = args.save_dir\n",
    "    \n",
    "    if args.resume:\n",
    "        checkpoint = torch.load(args.resume)\n",
    "        if start_epoch == 0:\n",
    "            start_epoch = checkpoint['epoch'] + 1\n",
    "        if not save_dir:\n",
    "            save_dir = checkpoint['save_dir']\n",
    "        else:\n",
    "            save_dir = os.path.join('results',save_dir)\n",
    "        net.load_state_dict(checkpoint['state_dict'])\n",
    "    else:\n",
    "        if start_epoch == 0:\n",
    "            start_epoch = 1\n",
    "        if not save_dir:\n",
    "            exp_id = time.strftime('%Y%m%d-%H%M%S', time.localtime())\n",
    "            save_dir = os.path.join('results', args.model + '-' + exp_id)\n",
    "        else:\n",
    "            save_dir = os.path.join('results',save_dir)\n",
    "    \n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    logfile = os.path.join(save_dir,'log')\n",
    "    if args.test!=1:\n",
    "        sys.stdout = Logger(logfile)\n",
    "        pyfiles = [f for f in os.listdir('./') if f.endswith('.py')]\n",
    "        for f in pyfiles:\n",
    "            shutil.copy(f,os.path.join(save_dir,f))\n",
    "    #n_gpu = setgpu(args.gpu)\n",
    "    #args.n_gpu = n_gpu\n",
    "    #net = net.cuda()\n",
    "    #loss = loss.cuda()\n",
    "    #cudnn.benchmark = True\n",
    "    #net = DataParallel(net)\n",
    "    datadir = PATH['pic_train']\n",
    "    \n",
    "    if args.test == 1:\n",
    "        margin = 32\n",
    "        sidelen = 144\n",
    "\n",
    "        split_comber = SplitComb(sidelen,config['max_stride'],config['stride'],margin,config['pad_value'])\n",
    "        dataset = data.DataBowl3Detector(\n",
    "            datadir,\n",
    "            'full.npy',\n",
    "            config,\n",
    "            phase='test',\n",
    "            split_comber=split_comber)\n",
    "        test_loader = DataLoader(\n",
    "            dataset,\n",
    "            batch_size = 1,\n",
    "            shuffle = False,\n",
    "            num_workers = args.workers,\n",
    "            collate_fn = data.collate,\n",
    "            pin_memory=False)\n",
    "        \n",
    "        test(test_loader, net, get_pbb, save_dir,config)\n",
    "        return\n",
    "\n",
    "    #net = DataParallel(net)\n",
    "    \n",
    "    dataset1 = data.DataBowl3Detector(\n",
    "        datadir,\n",
    "        'kaggleluna_full.npy',\n",
    "        config,\n",
    "        phase = 'train')\n",
    "    train_loader = DataLoader(\n",
    "        dataset=dataset1,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = True,\n",
    "        num_workers = args.workers,\n",
    "        pin_memory=True)\n",
    "    \n",
    "    dataset2 = data.DataBowl3Detector(\n",
    "        datadir,\n",
    "        'valsplit.npy',\n",
    "        config,\n",
    "        phase = 'val')\n",
    "    val_loader = DataLoader(\n",
    "        dataset=dataset2,\n",
    "        batch_size = args.batch_size,\n",
    "        shuffle = False,\n",
    "        num_workers = args.workers,\n",
    "        pin_memory=True)\n",
    "    ##\n",
    "    optimizer = torch.optim.SGD(\n",
    "        net.parameters(),\n",
    "        args.lr,\n",
    "        momentum = 0.9,\n",
    "        weight_decay = args.weight_decay)\n",
    "    \n",
    "    def get_lr(epoch):\n",
    "        if epoch <= args.epochs * 0.5:\n",
    "            lr = args.lr\n",
    "        elif epoch <= args.epochs * 0.8:\n",
    "            lr = 0.1 * args.lr\n",
    "        else:\n",
    "            lr = 0.01 * args.lr\n",
    "        return lr\n",
    "    \n",
    "\n",
    "    for epoch in range(start_epoch, args.epochs + 1):\n",
    "        train(train_loader, net, loss, epoch, optimizer, get_lr, args.save_freq, save_dir)\n",
    "        validate(val_loader, net, loss)\n",
    "\n",
    "def train(data_loader, net, loss, epoch, optimizer, get_lr, save_freq, save_dir):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.train()\n",
    "    lr = get_lr(epoch)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    metrics = []\n",
    "    for i, (data, target, coord) in enumerate(data_loader):\n",
    "        data = Variable(data.cuda(async = True))\n",
    "        target = Variable(target.cuda(async = True))\n",
    "        coord = Variable(coord.cuda(async = True))\n",
    "\n",
    "        output = net(data, coord)\n",
    "        loss_output = loss(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss_output[0].backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_output[0] = loss_output[0].data[0]\n",
    "        metrics.append(loss_output)\n",
    "\n",
    "    if epoch % args.save_freq == 0:            \n",
    "        state_dict = net.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].cpu()\n",
    "            \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'save_dir': save_dir,\n",
    "            'state_dict': state_dict,\n",
    "            'args': args},\n",
    "            os.path.join(save_dir, '%03d.ckpt' % epoch))\n",
    "\n",
    "    end_time = time.time()\n",
    "\n",
    "    metrics = np.asarray(metrics, np.float32)\n",
    "\n",
    "    print('Epoch %03d (lr %.5f)' % (epoch, lr))\n",
    "    #print('Train:      tpr %3.2f, tnr %3.2f, total pos %d, total neg %d, time %3.2f' % (\n",
    "    #    100.0 * np.sum(metrics[:, 6]) / np.sum(metrics[:, 7]),\n",
    "    #    100.0 * np.sum(metrics[:, 8]) / np.sum(metrics[:, 9]),\n",
    "    #    np.sum(metrics[:, 7]),\n",
    "    #    np.sum(metrics[:, 9]),\n",
    "    #    end_time - start_time))\n",
    "    #print('loss %2.4f, classify loss %2.4f, regress loss %2.4f, %2.4f, %2.4f, %2.4f' % (\n",
    "    #    np.mean(metrics[:, 0]),\n",
    "    #    np.mean(metrics[:, 1]),\n",
    "    #    np.mean(metrics[:, 2]),\n",
    "    #    np.mean(metrics[:, 3]),\n",
    "    #    np.mean(metrics[:, 4]),\n",
    "    #    np.mean(metrics[:, 5])))\n",
    "    print\n",
    "\n",
    "def validate(data_loader, net, loss):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    net.eval()\n",
    "\n",
    "    metrics = []\n",
    "    for i, (data, target, coord) in enumerate(data_loader):\n",
    "        data = Variable(data.cuda(async = True), volatile = True)\n",
    "        target = Variable(target.cuda(async = True), volatile = True)\n",
    "        coord = Variable(coord.cuda(async = True), volatile = True)\n",
    "\n",
    "        output = net(data, coord)\n",
    "        loss_output = loss(output, target, train = False)\n",
    "\n",
    "        loss_output[0] = loss_output[0].data[0]\n",
    "        metrics.append(loss_output)    \n",
    "    end_time = time.time()\n",
    "\n",
    "    metrics = np.asarray(metrics, np.float32)\n",
    "    #print('Validation: tpr %3.2f, tnr %3.8f, total pos %d, total neg %d, time %3.2f' % (\n",
    "    #    100.0 * np.sum(metrics[:, 6]) / np.sum(metrics[:, 7]),\n",
    "    #    100.0 * np.sum(metrics[:, 8]) / np.sum(metrics[:, 9]),\n",
    "    #    np.sum(metrics[:, 7]),\n",
    "    #    np.sum(metrics[:, 9]),\n",
    "    #    end_time - start_time))\n",
    "    #print('loss %2.4f, classify loss %2.4f, regress loss %2.4f, %2.4f, %2.4f, %2.4f' % (\n",
    "    #    np.mean(metrics[:, 0]),\n",
    "    #    np.mean(metrics[:, 1]),\n",
    "    #    np.mean(metrics[:, 2]),\n",
    "    #    np.mean(metrics[:, 3]),\n",
    "    #    np.mean(metrics[:, 4]),\n",
    "    #    np.mean(metrics[:, 5])))\n",
    "    print\n",
    "    print\n",
    "\n",
    "def test(data_loader, net, get_pbb, save_dir, config):\n",
    "    start_time = time.time()\n",
    "    save_dir = os.path.join(save_dir,'bbox')\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "    print(save_dir)\n",
    "    net.eval()\n",
    "    namelist = []\n",
    "    split_comber = data_loader.dataset.split_comber\n",
    "    for i_name, (data, target, coord, nzhw) in enumerate(data_loader):\n",
    "        s = time.time()\n",
    "        target = [np.asarray(t, np.float32) for t in target]\n",
    "        lbb = target[0]\n",
    "        nzhw = nzhw[0]\n",
    "        name = data_loader.dataset.filenames[i_name].split('-')[0].split('/')[-1].split('_clean')[0]\n",
    "        data = data[0][0]\n",
    "        coord = coord[0][0]\n",
    "        isfeat = False\n",
    "        if 'output_feature' in config:\n",
    "            if config['output_feature']:\n",
    "                isfeat = True\n",
    "        n_per_run = args.n_test\n",
    "        print(data.size())\n",
    "        splitlist = range(0,len(data)+1,n_per_run)\n",
    "        if splitlist[-1]!=len(data):\n",
    "            splitlist.append(len(data))\n",
    "        outputlist = []\n",
    "        featurelist = []\n",
    "\n",
    "        for i in range(len(splitlist)-1):\n",
    "            input = Variable(data[splitlist[i]:splitlist[i+1]], volatile = True).cuda()\n",
    "            inputcoord = Variable(coord[splitlist[i]:splitlist[i+1]], volatile = True).cuda()\n",
    "            if isfeat:\n",
    "                output,feature = net(input,inputcoord)\n",
    "                featurelist.append(feature.data.cpu().numpy())\n",
    "            else:\n",
    "                output = net(input,inputcoord)\n",
    "            outputlist.append(output.data.cpu().numpy())\n",
    "        output = np.concatenate(outputlist,0)\n",
    "        output = split_comber.combine(output,nzhw=nzhw)\n",
    "        if isfeat:\n",
    "            feature = np.concatenate(featurelist,0).transpose([0,2,3,4,1])[:,:,:,:,:,np.newaxis]\n",
    "            feature = split_comber.combine(feature,sidelen)[...,0]\n",
    "\n",
    "        thresh = -3\n",
    "        pbb,mask = get_pbb(output,thresh,ismask=True)\n",
    "        if isfeat:\n",
    "            feature_selected = feature[mask[0],mask[1],mask[2]]\n",
    "            np.save(os.path.join(save_dir, name+'_feature.npy'), feature_selected)\n",
    "        #tp,fp,fn,_ = acc(pbb,lbb,0,0.1,0.1)\n",
    "        #print([len(tp),len(fp),len(fn)])\n",
    "        print([i_name,name])\n",
    "        e = time.time()\n",
    "        np.save(os.path.join(save_dir, name+'_pbb.npy'), pbb)\n",
    "        np.save(os.path.join(save_dir, name+'_lbb.npy'), lbb)\n",
    "    np.save(os.path.join(save_dir, 'namelist.npy'), namelist)\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    print('elapsed time is %3.2f seconds' % (end_time - start_time))\n",
    "    print\n",
    "    print\n",
    "\n",
    "def singletest(data,net,config,splitfun,combinefun,n_per_run,margin = 64,isfeat=False):\n",
    "    z, h, w = data.size(2), data.size(3), data.size(4)\n",
    "    print(data.size())\n",
    "    data = splitfun(data,config['max_stride'],margin)\n",
    "    data = Variable(data.cuda(async = True), volatile = True,requires_grad=False)\n",
    "    splitlist = range(0,args.split+1,n_per_run)\n",
    "    outputlist = []\n",
    "    featurelist = []\n",
    "    for i in range(len(splitlist)-1):\n",
    "        if isfeat:\n",
    "            output,feature = net(data[splitlist[i]:splitlist[i+1]])\n",
    "            featurelist.append(feature)\n",
    "        else:\n",
    "            output = net(data[splitlist[i]:splitlist[i+1]])\n",
    "        output = output.data.cpu().numpy()\n",
    "        outputlist.append(output)\n",
    "        \n",
    "    output = np.concatenate(outputlist,0)\n",
    "    output = combinefun(output, z / config['stride'], h / config['stride'], w / config['stride'])\n",
    "    if isfeat:\n",
    "        feature = np.concatenate(featurelist,0).transpose([0,2,3,4,1])\n",
    "        feature = combinefun(feature, z / config['stride'], h / config['stride'], w / config['stride'])\n",
    "        return output,feature\n",
    "    else:\n",
    "        return output\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Traceback (most recent call last):\n  File \"D:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 41, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"D:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 41, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\zhu\\solo\\training\\detector\\data.py\", line 96, in __getitem__\n    randimid = np.random.randint(len(self.kagglenames))                                     #在kaggle文件名表中获取随机的index\n  File \"mtrand.pyx\", line 988, in mtrand.RandomState.randint (numpy\\random\\mtrand\\mtrand.c:16157)\nValueError: low >= high\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-58ca95c5b364>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-2-23e6ddd230ed>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_lr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m         \u001b[0mvalidate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-23e6ddd230ed>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(data_loader, net, loss, epoch, optimizer, get_lr, save_freq, save_dir)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoord\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masync\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0masync\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    210\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                 \u001b[1;32mcontinue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[0mnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[1;31m# Python 2 compatibility\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    237\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 239\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    240\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Traceback (most recent call last):\n  File \"D:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 41, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"D:\\MyPrograms\\Anaconda\\pathfile\\envs\\py3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 41, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"C:\\Users\\zhu\\solo\\training\\detector\\data.py\", line 96, in __getitem__\n    randimid = np.random.randint(len(self.kagglenames))                                     #在kaggle文件名表中获取随机的index\n  File \"mtrand.pyx\", line 988, in mtrand.RandomState.randint (numpy\\random\\mtrand\\mtrand.c:16157)\nValueError: low >= high\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.model='detector.res18'    \n",
    "model = import_module(args.model)\n",
    "config, net, loss, get_pbb = model.get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycaffe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f194e78708dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpycaffe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycaffe'"
     ]
    }
   ],
   "source": [
    "import pycaffe"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
