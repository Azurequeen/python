{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import utils.helpers as helpers\n",
    "from utils.paths import *\n",
    "\n",
    "\n",
    "import SimpleITK  # conda install -c https://conda.anaconda.org/simpleitk SimpleITK\n",
    "import numpy\n",
    "import pandas\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ntpath\n",
    "import cv2  # conda install -c https://conda.anaconda.org/menpo opencv3\n",
    "import shutil\n",
    "import random\n",
    "import math\n",
    "import multiprocessing\n",
    "import os\n",
    "import glob\n",
    "import h5py\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import sklearn\n",
    "import time\n",
    "\n",
    "from typing import List, Tuple\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from skimage.transform import resize\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.layers import Input, Convolution2D, MaxPooling2D, UpSampling2D, merge, Dropout, BatchNormalization,SpatialDropout2D,Convolution3D,MaxPooling3D, UpSampling3D, Flatten, Dense\n",
    "from keras.models import Model, load_model, model_from_json\n",
    "from keras.metrics import binary_accuracy, binary_crossentropy, mean_squared_error, mean_absolute_error\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint, Callback, LearningRateScheduler,EarlyStopping\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.initializers import glorot_normal, he_uniform \n",
    "\n",
    "random.seed(1321)\n",
    "numpy.random.seed(1321)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "K.set_image_dim_ordering('th') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = PATH['model_train']\n",
    "model_paths = PATH['model_paths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + 1) / (K.sum(y_true_f) + K.sum(y_pred_f) + 1)\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return -dice_coef(y_true, y_pred)\n",
    "\n",
    "def dice_coef_np(y_true,y_pred):\n",
    "    y_true_f = y_true.flatten()\n",
    "    y_pred_f = y_pred.flatten()\n",
    "    intersection = np.sum(y_true_f * y_pred_f)\n",
    "    return ((2. * intersection + 1) / (np.sum(y_true_f) + np.sum(y_pred_f) + 1))\n",
    "\n",
    "\n",
    "def unet_model(dropout_rate,learn_rate, width):\n",
    "    inputs = Input((1, 512, 512))\n",
    "    conv1 = Convolution2D(width, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n",
    "    conv1 = BatchNormalization(axis = 1)(conv1)\n",
    "    conv1 = Convolution2D(width, (3, 3), padding=\"same\", activation=\"relu\")(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "\n",
    "    conv2 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"relu\")(pool1)\n",
    "    conv2 = BatchNormalization(axis = 1)(conv2)\n",
    "    conv2 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"relu\")(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "\n",
    "    conv3 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"relu\")(pool2)\n",
    "    conv3 = BatchNormalization(axis = 1)(conv3)\n",
    "    conv3 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"relu\")(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "\n",
    "    conv4 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"relu\")(pool3)\n",
    "    conv4 = BatchNormalization(axis = 1)(conv4)\n",
    "    conv4 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"relu\")(conv4)\n",
    "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
    "\n",
    "    conv5 = Convolution2D(width*16, (3, 3), padding=\"same\", activation=\"relu\")(pool4)\n",
    "    conv5 = BatchNormalization(axis = 1)(conv5)\n",
    "    conv5 = Convolution2D(width*16, (3, 3), padding=\"same\", activation=\"relu\")(conv5)\n",
    "\n",
    "    up6 = merge([UpSampling2D(size=(2, 2))(conv5), conv4], mode='concat', concat_axis=1)\n",
    "    conv6 = SpatialDropout2D(dropout_rate)(up6)\n",
    "    conv6 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "    conv6 = Convolution2D(width*8, (3, 3), padding=\"same\", activation=\"relu\")(conv6)\n",
    "\n",
    "    up7 = merge([UpSampling2D(size=(2, 2))(conv6), conv3], mode='concat', concat_axis=1)\n",
    "    conv7 = SpatialDropout2D(dropout_rate)(up7)\n",
    "    conv7 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "    conv7 = Convolution2D(width*4, (3, 3), padding=\"same\", activation=\"relu\")(conv7)\n",
    "\n",
    "    up8 = merge([UpSampling2D(size=(2, 2))(conv7), conv2], mode='concat', concat_axis=1)\n",
    "    conv8 = SpatialDropout2D(dropout_rate)(up8)\n",
    "    conv8 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "    conv8 = Convolution2D(width*2, (3, 3), padding=\"same\", activation=\"relu\")(conv8)\n",
    "\n",
    "    up9 = merge([UpSampling2D(size=(2, 2))(conv8), conv1], mode='concat', concat_axis=1)\n",
    "    conv9 = SpatialDropout2D(dropout_rate)(up9)\n",
    "    conv9 = Convolution2D(width, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    conv9 = Convolution2D(width, (3, 3), padding=\"same\", activation=\"relu\")(conv9)\n",
    "    conv10 = Convolution2D(1, (1, 1), activation=\"sigmoid\")(conv9)\n",
    "\n",
    "    model = Model(input=inputs, output=conv10)\n",
    "    #model.summary()\n",
    "    #model.compile(optimizer=Adam(lr=learn_rate), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    model.compile(optimizer=SGD(lr=learn_rate, momentum=0.9, nesterov=True), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "    \n",
    "    #plot_model(model, to_file='model1.png',show_shapes=True)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def unet_fit(name, check_name = None):\n",
    "    data_gen_args = dict(rotation_range=25.,   \n",
    "                     width_shift_range=0.3,  \n",
    "                     height_shift_range=0.3,   \n",
    "                     horizontal_flip=True,   \n",
    "                     vertical_flip=True,  \n",
    "                     )\n",
    "    from keras.preprocessing.image import ImageDataGenerator\n",
    "    image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "    mask_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "    # Provide the same seed and keyword arguments to the fit and flow methods\n",
    "    seed = 1\n",
    "    \n",
    "    image_generator = image_datagen.flow_from_directory(\n",
    "        src,\n",
    "        class_mode=None,\n",
    "        classes=['lung'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1)\n",
    "\n",
    "    mask_generator = mask_datagen.flow_from_directory(\n",
    "        src,\n",
    "        class_mode=None,\n",
    "        classes=['nodule'],\n",
    "        seed=seed,\n",
    "        target_size=(512,512),\n",
    "        color_mode=\"grayscale\",\n",
    "        batch_size=1)\n",
    "\n",
    "    # combine generators into one which yields image and masks\n",
    "    train_generator = zip(image_generator, mask_generator)\n",
    "        \n",
    "    t = time.time()\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience = 200, \n",
    "                                   verbose = 1),\n",
    "    ModelCheckpoint(model_paths + '{}.h5'.format(name), \n",
    "                        monitor='val_loss', \n",
    "                        verbose = 0, save_best_only = True)]\n",
    "    if check_name is not None:\n",
    "        check_model = model_paths + '{}.h5'.format(check_name)\n",
    "        model = load_model(check_model, \n",
    "                           custom_objects={'dice_coef_loss': dice_coef_loss, 'dice_coef': dice_coef})\n",
    "    else:\n",
    "        model = unet_model(dropout_rate = 0.2, learn_rate = 1e-4, width = 32)   \n",
    "        #model = get_unet(1e-5)        \n",
    "        \n",
    "    model.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=1000,\n",
    "        verbose =2, \n",
    "        callbacks = callbacks,\n",
    "        steps_per_epoch=256,\n",
    "        validation_data = train_generator,\n",
    "        nb_val_samples = 48)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3711 images belonging to 1 classes.\n",
      "Found 3711 images belonging to 1 classes.\n",
      "Epoch 1/1000\n",
      "49s - loss: -1.5961e+00 - dice_coef: 1.5961 - val_loss: -1.5905e+00 - val_dice_coef: 1.5905\n",
      "Epoch 2/1000\n",
      "48s - loss: -1.5827e+00 - dice_coef: 1.5827 - val_loss: -1.6658e+00 - val_dice_coef: 1.6658\n",
      "Epoch 3/1000\n",
      "48s - loss: -1.6334e+00 - dice_coef: 1.6334 - val_loss: -1.6081e+00 - val_dice_coef: 1.6081\n",
      "Epoch 4/1000\n",
      "48s - loss: -1.6433e+00 - dice_coef: 1.6433 - val_loss: -1.4068e+00 - val_dice_coef: 1.4068\n",
      "Epoch 5/1000\n",
      "49s - loss: -1.6236e+00 - dice_coef: 1.6236 - val_loss: -1.6710e+00 - val_dice_coef: 1.6710\n",
      "Epoch 6/1000\n",
      "48s - loss: -1.6374e+00 - dice_coef: 1.6374 - val_loss: -1.4537e+00 - val_dice_coef: 1.4537\n",
      "Epoch 7/1000\n",
      "48s - loss: -1.6582e+00 - dice_coef: 1.6582 - val_loss: -1.4442e+00 - val_dice_coef: 1.4442\n",
      "Epoch 8/1000\n",
      "48s - loss: -1.6757e+00 - dice_coef: 1.6757 - val_loss: -1.5094e+00 - val_dice_coef: 1.5094\n",
      "Epoch 9/1000\n",
      "49s - loss: -1.6740e+00 - dice_coef: 1.6740 - val_loss: -1.7506e+00 - val_dice_coef: 1.7506\n",
      "Epoch 10/1000\n",
      "48s - loss: -1.5814e+00 - dice_coef: 1.5814 - val_loss: -1.5877e+00 - val_dice_coef: 1.5877\n",
      "Epoch 11/1000\n",
      "48s - loss: -1.6491e+00 - dice_coef: 1.6491 - val_loss: -1.5360e+00 - val_dice_coef: 1.5360\n",
      "Epoch 12/1000\n",
      "48s - loss: -1.5902e+00 - dice_coef: 1.5902 - val_loss: -1.4753e+00 - val_dice_coef: 1.4753\n",
      "Epoch 13/1000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-5d62925f08a0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#unet_fit('final_fenge_170629')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0munet_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'final_fenge_170629_2'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'final_fenge_170629'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-3-7845163e5ebc>\u001b[0m in \u001b[0;36munet_fit\u001b[1;34m(name, check_name)\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 129\u001b[1;33m         nb_val_samples = 48)\n\u001b[0m\u001b[0;32m    130\u001b[0m     \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 88\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_q_size, workers, pickle_safe, initial_epoch)\u001b[0m\n\u001b[0;32m   1900\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[0;32m   1901\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1902\u001b[1;33m                                                class_weight=class_weight)\n\u001b[0m\u001b[0;32m   1903\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1904\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1640\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1641\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1642\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1643\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1644\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#unet_fit('final_fenge_170629')\n",
    "unet_fit('final_fenge_170629_2','final_fenge_170629')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
