{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from utils.imports import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier(input_shape, width, kernel_size, pool_size):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Convolution3D(width*1, kernel_size[0], kernel_size[1], kernel_size[2],\n",
    "                            border_mode='valid',\n",
    "                            input_shape=input_shape))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling3D(pool_size=pool_size))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Convolution3D(width*2, kernel_size[0], kernel_size[1], kernel_size[2]))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling3D(pool_size=pool_size))\n",
    "    #model.add(Dropout(0.2))\n",
    "    model.add(Convolution3D(width*4, kernel_size[0], kernel_size[1], kernel_size[2]))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(MaxPooling3D(pool_size=pool_size))\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('elu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(2))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "          #optimizer='adadelta',\n",
    "          optimizer=Adam(lr=1e-4),                  \n",
    "          metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output_path = PATH['cls_train_cube_30']\n",
    "output_true = PATH['cls_train_cube_30_true']\n",
    "output_false = PATH['cls_train_cube_30_false']\n",
    "model_paths = PATH['model_paths']\n",
    "model_final = PATH['model_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def get_dirfiles(dir):\n",
    "    file_list = []\n",
    "    subset_path = os.listdir(dir)\n",
    "    for _ in range(len(subset_path)):\n",
    "        if subset_path[_] != '.DS_Store':\n",
    "            file_list.append(dir + subset_path[_])\n",
    "    return file_list\n",
    "\n",
    "def train_generator(output_true,output_false):\n",
    "    file_list_true = get_dirfiles(output_true)[0:300]\n",
    "    file_list_false = get_dirfiles(output_false)[0:300]\n",
    "    \n",
    "    file_list_true = shuffle(file_list_true)\n",
    "    file_list_false = shuffle(file_list_false)\n",
    "    \n",
    "    nb_true = len(file_list_true) + len(file_list_false)\n",
    "    sample = np.zeros([nb_true,36,36,36])\n",
    "    labels = np.zeros([nb_true,2])\n",
    "    for i in tqdm(range(len(file_list_true))):       \n",
    "        cc= np.load(file_list_true[i])\n",
    "        sample[i] = cc\n",
    "        labels[i] = [0.,1.]\n",
    "    for j in tqdm(range(len(file_list_false))):\n",
    "        bb= np.load(file_list_false[j])\n",
    "        sample[j+len(file_list_true)] = bb \n",
    "        labels[j+len(file_list_true)] = [1.,0.]\n",
    "    sample = np.expand_dims(sample, axis=1)        \n",
    "    return sample,labels\n",
    "\n",
    "def valid_generator(output_true,output_false):\n",
    "    file_list_true = get_dirfiles(output_true)[-60:]\n",
    "    file_list_false = get_dirfiles(output_false)[-60:]\n",
    "    \n",
    "    file_list_true = shuffle(file_list_true)\n",
    "    file_list_false = shuffle(file_list_false)\n",
    "\n",
    "    nb_true = len(file_list_true) + len(file_list_false)\n",
    "    sample = np.zeros([nb_true,36,36,36])\n",
    "    labels = np.zeros([nb_true,2])\n",
    "  \n",
    "    for i in tqdm(range(len(file_list_true))):       \n",
    "        cc= np.load(file_list_true[i])\n",
    "        sample[i] = cc\n",
    "        labels[i] = [0.,1.]\n",
    "    for j in tqdm(range(len(file_list_false))):\n",
    "        bb= np.load(file_list_false[j])\n",
    "        sample[j+len(file_list_true)] = bb \n",
    "        labels[j+len(file_list_true)] = [1.,0.]\n",
    "    sample = np.expand_dims(sample, axis=1)        \n",
    "    return sample,labels\n",
    "\n",
    "\n",
    "def fenlei_fit(name, load_check = False,batch_size=2, epochs=100,check_name = None):\n",
    "\n",
    "    t = time.time()\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience = 50, verbose = 1),\n",
    "                 ModelCheckpoint((model_paths + '{}.h5').format(name),\n",
    "                                 monitor='val_loss',\n",
    "                                 verbose = 0,\n",
    "                                 save_best_only = True)]\n",
    "    if load_check:\n",
    "        check_model = (model_paths + '{}.h5').format(check_name)\n",
    "        model = load_model(check_model)\n",
    "    else:\n",
    "        #model = model_20()\n",
    "        #model = preds3d_dense(48)\n",
    "        model = classifier((1, 36, 36, 36),64,(3, 3, 3), (2, 2, 2))\n",
    "    x,y = train_generator(output_true,output_false)\n",
    "    model.fit(x=x, y=y, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=valid_generator(output_true,output_false),verbose=1, callbacks=callbacks, shuffle=True)\n",
    "    \n",
    " \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:54<00:00,  1.86it/s]\n",
      "100%|██████████| 100/100 [00:55<00:00,  1.88it/s]\n",
      "100%|██████████| 20/20 [00:00<00:00, 42.49it/s]\n",
      "100%|██████████| 20/20 [00:11<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 200 samples, validate on 40 samples\n",
      "Epoch 1/500\n",
      "200/200 [==============================] - 9s - loss: 0.7126 - acc: 0.7150 - val_loss: 0.5868 - val_acc: 0.8000\n",
      "Epoch 2/500\n",
      "200/200 [==============================] - 6s - loss: 0.4039 - acc: 0.8450 - val_loss: 0.4638 - val_acc: 0.7750\n",
      "Epoch 3/500\n",
      "200/200 [==============================] - 4s - loss: 0.3393 - acc: 0.8850 - val_loss: 0.5346 - val_acc: 0.7500\n",
      "Epoch 4/500\n",
      "200/200 [==============================] - 4s - loss: 0.2595 - acc: 0.8900 - val_loss: 0.5033 - val_acc: 0.7750\n",
      "Epoch 5/500\n",
      "200/200 [==============================] - 4s - loss: 0.2089 - acc: 0.9150 - val_loss: 0.5130 - val_acc: 0.8250\n",
      "Epoch 6/500\n",
      "200/200 [==============================] - 4s - loss: 0.1626 - acc: 0.9500 - val_loss: 0.8251 - val_acc: 0.7250\n",
      "Epoch 7/500\n",
      "200/200 [==============================] - 4s - loss: 0.0473 - acc: 0.9800 - val_loss: 0.6024 - val_acc: 0.8000\n",
      "Epoch 8/500\n",
      "200/200 [==============================] - 4s - loss: 0.0669 - acc: 0.9800 - val_loss: 0.5830 - val_acc: 0.7750\n",
      "Epoch 9/500\n",
      "200/200 [==============================] - 4s - loss: 0.0540 - acc: 0.9850 - val_loss: 0.6940 - val_acc: 0.8000\n",
      "Epoch 10/500\n",
      "200/200 [==============================] - 4s - loss: 0.0906 - acc: 0.9650 - val_loss: 0.5762 - val_acc: 0.8000\n",
      "Epoch 11/500\n",
      "200/200 [==============================] - 4s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.6634 - val_acc: 0.8250\n",
      "Epoch 12/500\n",
      "200/200 [==============================] - 4s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.7072 - val_acc: 0.8250\n",
      "Epoch 13/500\n",
      "200/200 [==============================] - 4s - loss: 0.0023 - acc: 1.0000 - val_loss: 0.7934 - val_acc: 0.8250\n",
      "Epoch 14/500\n",
      "200/200 [==============================] - 4s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.8068 - val_acc: 0.8500\n",
      "Epoch 15/500\n",
      "200/200 [==============================] - 4s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.9148 - val_acc: 0.8500\n",
      "Epoch 16/500\n",
      "200/200 [==============================] - 4s - loss: 8.4538e-04 - acc: 1.0000 - val_loss: 0.8536 - val_acc: 0.8250\n",
      "Epoch 17/500\n",
      "200/200 [==============================] - 4s - loss: 6.1841e-04 - acc: 1.0000 - val_loss: 0.9216 - val_acc: 0.8250\n",
      "Epoch 18/500\n",
      "200/200 [==============================] - 4s - loss: 4.9361e-04 - acc: 1.0000 - val_loss: 0.9250 - val_acc: 0.8500\n",
      "Epoch 19/500\n",
      "200/200 [==============================] - 4s - loss: 4.2046e-04 - acc: 1.0000 - val_loss: 0.9229 - val_acc: 0.8250\n",
      "Epoch 20/500\n",
      "200/200 [==============================] - 4s - loss: 5.1810e-04 - acc: 1.0000 - val_loss: 0.8890 - val_acc: 0.8500\n",
      "Epoch 21/500\n",
      "200/200 [==============================] - 4s - loss: 2.9805e-04 - acc: 1.0000 - val_loss: 0.9401 - val_acc: 0.8500\n",
      "Epoch 22/500\n",
      "200/200 [==============================] - 4s - loss: 3.4449e-04 - acc: 1.0000 - val_loss: 0.9317 - val_acc: 0.8250\n",
      "Epoch 23/500\n",
      "200/200 [==============================] - 4s - loss: 2.4975e-04 - acc: 1.0000 - val_loss: 0.9927 - val_acc: 0.8500\n",
      "Epoch 00022: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.models.Sequential at 0x10e5fe5d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fenlei_fit('Fenge_36_36_36_0624', load_check = False, batch_size=1, epochs=500, check_name = 'Fenge_36_36_36_0615')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_list_true = get_dirfiles(output_true)\n",
    "file_list_false = get_dirfiles(output_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_pred = classifier((1, 36, 36, 36), (3, 3, 3), (2, 2, 2))\n",
    "model_pred = load_model(model_paths + 'Fenge_36_36_36_0624.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model_pred.load_weights(model_paths + 'Fenge_0613_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = []\n",
    "for i in file_list_false[0:200]:\n",
    "    a=np.load(i)\n",
    "    a=np.expand_dims(a,0)\n",
    "    a=np.expand_dims(a,0)\n",
    "    cc.append(model_pred.predict(a))\n",
    "count = 0\n",
    "for i in cc:\n",
    "    if i[0][0] > 0.9:\n",
    "        count += 1\n",
    "print count*1.0/len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cc = []\n",
    "for i in file_list_true[0:200]:\n",
    "    a=np.load(i)\n",
    "    a=np.expand_dims(a,0)\n",
    "    a=np.expand_dims(a,0)\n",
    "    cc.append(model_pred.predict(a))\n",
    "count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in cc:\n",
    "    if i[0][1] > 0.9:\n",
    "        count += 1\n",
    "print count*1.0/len(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(file_list_false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
